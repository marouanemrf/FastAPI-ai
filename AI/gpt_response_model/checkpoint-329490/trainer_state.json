{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 329490,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004552490212146044,
      "grad_norm": 6.711992263793945,
      "learning_rate": 4.992427691280464e-05,
      "loss": 1.7673,
      "step": 500
    },
    {
      "epoch": 0.009104980424292088,
      "grad_norm": 3.201833963394165,
      "learning_rate": 4.984840207593554e-05,
      "loss": 1.604,
      "step": 1000
    },
    {
      "epoch": 0.013657470636438131,
      "grad_norm": 4.119305610656738,
      "learning_rate": 4.977252723906644e-05,
      "loss": 1.6249,
      "step": 1500
    },
    {
      "epoch": 0.018209960848584176,
      "grad_norm": 3.2692856788635254,
      "learning_rate": 4.9696652402197335e-05,
      "loss": 1.6417,
      "step": 2000
    },
    {
      "epoch": 0.02276245106073022,
      "grad_norm": 2.832024097442627,
      "learning_rate": 4.9620777565328235e-05,
      "loss": 1.6127,
      "step": 2500
    },
    {
      "epoch": 0.027314941272876262,
      "grad_norm": 2.507462501525879,
      "learning_rate": 4.9544902728459135e-05,
      "loss": 1.6134,
      "step": 3000
    },
    {
      "epoch": 0.03186743148502231,
      "grad_norm": 2.6528735160827637,
      "learning_rate": 4.9469027891590035e-05,
      "loss": 1.6213,
      "step": 3500
    },
    {
      "epoch": 0.03641992169716835,
      "grad_norm": 2.410271644592285,
      "learning_rate": 4.9393153054720935e-05,
      "loss": 1.5815,
      "step": 4000
    },
    {
      "epoch": 0.040972411909314395,
      "grad_norm": 2.4228503704071045,
      "learning_rate": 4.9317278217851835e-05,
      "loss": 1.6303,
      "step": 4500
    },
    {
      "epoch": 0.04552490212146044,
      "grad_norm": 2.2473554611206055,
      "learning_rate": 4.924140338098273e-05,
      "loss": 1.5858,
      "step": 5000
    },
    {
      "epoch": 0.05007739233360648,
      "grad_norm": 4.8948822021484375,
      "learning_rate": 4.916552854411363e-05,
      "loss": 1.6002,
      "step": 5500
    },
    {
      "epoch": 0.054629882545752524,
      "grad_norm": 1.545534372329712,
      "learning_rate": 4.908965370724453e-05,
      "loss": 1.5593,
      "step": 6000
    },
    {
      "epoch": 0.05918237275789857,
      "grad_norm": 2.9527928829193115,
      "learning_rate": 4.9013778870375435e-05,
      "loss": 1.5789,
      "step": 6500
    },
    {
      "epoch": 0.06373486297004462,
      "grad_norm": 2.5952768325805664,
      "learning_rate": 4.8937904033506335e-05,
      "loss": 1.6503,
      "step": 7000
    },
    {
      "epoch": 0.06828735318219066,
      "grad_norm": 2.5124309062957764,
      "learning_rate": 4.8862029196637235e-05,
      "loss": 1.5719,
      "step": 7500
    },
    {
      "epoch": 0.0728398433943367,
      "grad_norm": 2.3284144401550293,
      "learning_rate": 4.878615435976813e-05,
      "loss": 1.5241,
      "step": 8000
    },
    {
      "epoch": 0.07739233360648275,
      "grad_norm": 2.2942845821380615,
      "learning_rate": 4.871027952289903e-05,
      "loss": 1.6238,
      "step": 8500
    },
    {
      "epoch": 0.08194482381862879,
      "grad_norm": 1.7935348749160767,
      "learning_rate": 4.863440468602993e-05,
      "loss": 1.614,
      "step": 9000
    },
    {
      "epoch": 0.08649731403077483,
      "grad_norm": 2.6648783683776855,
      "learning_rate": 4.855852984916083e-05,
      "loss": 1.5704,
      "step": 9500
    },
    {
      "epoch": 0.09104980424292088,
      "grad_norm": 1.6302645206451416,
      "learning_rate": 4.848265501229173e-05,
      "loss": 1.5435,
      "step": 10000
    },
    {
      "epoch": 0.09560229445506692,
      "grad_norm": 1.8239576816558838,
      "learning_rate": 4.840678017542262e-05,
      "loss": 1.5374,
      "step": 10500
    },
    {
      "epoch": 0.10015478466721296,
      "grad_norm": 2.4171929359436035,
      "learning_rate": 4.833090533855352e-05,
      "loss": 1.5453,
      "step": 11000
    },
    {
      "epoch": 0.104707274879359,
      "grad_norm": 1.8650712966918945,
      "learning_rate": 4.825503050168442e-05,
      "loss": 1.5283,
      "step": 11500
    },
    {
      "epoch": 0.10925976509150505,
      "grad_norm": 2.2542166709899902,
      "learning_rate": 4.817915566481532e-05,
      "loss": 1.5883,
      "step": 12000
    },
    {
      "epoch": 0.11381225530365109,
      "grad_norm": 1.9423879384994507,
      "learning_rate": 4.810328082794622e-05,
      "loss": 1.5237,
      "step": 12500
    },
    {
      "epoch": 0.11836474551579713,
      "grad_norm": 4.801772117614746,
      "learning_rate": 4.802740599107712e-05,
      "loss": 1.5464,
      "step": 13000
    },
    {
      "epoch": 0.12291723572794319,
      "grad_norm": 1.9088784456253052,
      "learning_rate": 4.795153115420802e-05,
      "loss": 1.4789,
      "step": 13500
    },
    {
      "epoch": 0.12746972594008923,
      "grad_norm": 1.828966736793518,
      "learning_rate": 4.787565631733892e-05,
      "loss": 1.5253,
      "step": 14000
    },
    {
      "epoch": 0.13202221615223528,
      "grad_norm": 1.498440146446228,
      "learning_rate": 4.779978148046982e-05,
      "loss": 1.5235,
      "step": 14500
    },
    {
      "epoch": 0.13657470636438132,
      "grad_norm": 1.2931429147720337,
      "learning_rate": 4.772390664360072e-05,
      "loss": 1.5408,
      "step": 15000
    },
    {
      "epoch": 0.14112719657652736,
      "grad_norm": 1.5772604942321777,
      "learning_rate": 4.764803180673162e-05,
      "loss": 1.5701,
      "step": 15500
    },
    {
      "epoch": 0.1456796867886734,
      "grad_norm": 1.7862439155578613,
      "learning_rate": 4.757215696986252e-05,
      "loss": 1.5597,
      "step": 16000
    },
    {
      "epoch": 0.15023217700081945,
      "grad_norm": 1.632371187210083,
      "learning_rate": 4.7496282132993414e-05,
      "loss": 1.4911,
      "step": 16500
    },
    {
      "epoch": 0.1547846672129655,
      "grad_norm": 2.9482781887054443,
      "learning_rate": 4.7420407296124314e-05,
      "loss": 1.5559,
      "step": 17000
    },
    {
      "epoch": 0.15933715742511154,
      "grad_norm": 1.5640898942947388,
      "learning_rate": 4.7344532459255214e-05,
      "loss": 1.4804,
      "step": 17500
    },
    {
      "epoch": 0.16388964763725758,
      "grad_norm": 2.2412517070770264,
      "learning_rate": 4.7268657622386114e-05,
      "loss": 1.5526,
      "step": 18000
    },
    {
      "epoch": 0.16844213784940362,
      "grad_norm": 2.271923065185547,
      "learning_rate": 4.7192782785517014e-05,
      "loss": 1.5594,
      "step": 18500
    },
    {
      "epoch": 0.17299462806154967,
      "grad_norm": 1.2921818494796753,
      "learning_rate": 4.7116907948647914e-05,
      "loss": 1.5205,
      "step": 19000
    },
    {
      "epoch": 0.1775471182736957,
      "grad_norm": 1.9481315612792969,
      "learning_rate": 4.704103311177881e-05,
      "loss": 1.5361,
      "step": 19500
    },
    {
      "epoch": 0.18209960848584175,
      "grad_norm": 1.6352636814117432,
      "learning_rate": 4.696515827490971e-05,
      "loss": 1.5127,
      "step": 20000
    },
    {
      "epoch": 0.1866520986979878,
      "grad_norm": 1.2566120624542236,
      "learning_rate": 4.688928343804061e-05,
      "loss": 1.4809,
      "step": 20500
    },
    {
      "epoch": 0.19120458891013384,
      "grad_norm": 2.1223177909851074,
      "learning_rate": 4.681340860117151e-05,
      "loss": 1.5116,
      "step": 21000
    },
    {
      "epoch": 0.19575707912227988,
      "grad_norm": 1.8257746696472168,
      "learning_rate": 4.673753376430241e-05,
      "loss": 1.521,
      "step": 21500
    },
    {
      "epoch": 0.20030956933442592,
      "grad_norm": 1.5269874334335327,
      "learning_rate": 4.666165892743331e-05,
      "loss": 1.5207,
      "step": 22000
    },
    {
      "epoch": 0.20486205954657197,
      "grad_norm": 2.1110775470733643,
      "learning_rate": 4.658578409056421e-05,
      "loss": 1.5362,
      "step": 22500
    },
    {
      "epoch": 0.209414549758718,
      "grad_norm": 1.9805322885513306,
      "learning_rate": 4.650990925369511e-05,
      "loss": 1.5154,
      "step": 23000
    },
    {
      "epoch": 0.21396703997086405,
      "grad_norm": 2.283064126968384,
      "learning_rate": 4.643403441682601e-05,
      "loss": 1.4828,
      "step": 23500
    },
    {
      "epoch": 0.2185195301830101,
      "grad_norm": 1.9240810871124268,
      "learning_rate": 4.635815957995691e-05,
      "loss": 1.5956,
      "step": 24000
    },
    {
      "epoch": 0.22307202039515614,
      "grad_norm": 2.7188901901245117,
      "learning_rate": 4.628228474308781e-05,
      "loss": 1.4907,
      "step": 24500
    },
    {
      "epoch": 0.22762451060730218,
      "grad_norm": 1.6730681657791138,
      "learning_rate": 4.620640990621871e-05,
      "loss": 1.5159,
      "step": 25000
    },
    {
      "epoch": 0.23217700081944823,
      "grad_norm": 1.0829359292984009,
      "learning_rate": 4.61305350693496e-05,
      "loss": 1.5416,
      "step": 25500
    },
    {
      "epoch": 0.23672949103159427,
      "grad_norm": 1.9690616130828857,
      "learning_rate": 4.60546602324805e-05,
      "loss": 1.4973,
      "step": 26000
    },
    {
      "epoch": 0.2412819812437403,
      "grad_norm": 1.896836757659912,
      "learning_rate": 4.59787853956114e-05,
      "loss": 1.4919,
      "step": 26500
    },
    {
      "epoch": 0.24583447145588638,
      "grad_norm": 1.737241506576538,
      "learning_rate": 4.59029105587423e-05,
      "loss": 1.5036,
      "step": 27000
    },
    {
      "epoch": 0.2503869616680324,
      "grad_norm": 1.638809084892273,
      "learning_rate": 4.58270357218732e-05,
      "loss": 1.49,
      "step": 27500
    },
    {
      "epoch": 0.25493945188017847,
      "grad_norm": 2.3964622020721436,
      "learning_rate": 4.57511608850041e-05,
      "loss": 1.5257,
      "step": 28000
    },
    {
      "epoch": 0.2594919420923245,
      "grad_norm": 1.6429383754730225,
      "learning_rate": 4.567528604813499e-05,
      "loss": 1.5303,
      "step": 28500
    },
    {
      "epoch": 0.26404443230447056,
      "grad_norm": 1.478065848350525,
      "learning_rate": 4.55994112112659e-05,
      "loss": 1.53,
      "step": 29000
    },
    {
      "epoch": 0.2685969225166166,
      "grad_norm": 2.250810384750366,
      "learning_rate": 4.55235363743968e-05,
      "loss": 1.4999,
      "step": 29500
    },
    {
      "epoch": 0.27314941272876264,
      "grad_norm": 2.3155601024627686,
      "learning_rate": 4.54476615375277e-05,
      "loss": 1.5045,
      "step": 30000
    },
    {
      "epoch": 0.2777019029409087,
      "grad_norm": 1.9406174421310425,
      "learning_rate": 4.53717867006586e-05,
      "loss": 1.4882,
      "step": 30500
    },
    {
      "epoch": 0.28225439315305473,
      "grad_norm": 2.0826804637908936,
      "learning_rate": 4.52959118637895e-05,
      "loss": 1.5268,
      "step": 31000
    },
    {
      "epoch": 0.28680688336520077,
      "grad_norm": 1.9962064027786255,
      "learning_rate": 4.522003702692039e-05,
      "loss": 1.493,
      "step": 31500
    },
    {
      "epoch": 0.2913593735773468,
      "grad_norm": 1.8223729133605957,
      "learning_rate": 4.514416219005129e-05,
      "loss": 1.5031,
      "step": 32000
    },
    {
      "epoch": 0.29591186378949286,
      "grad_norm": 1.6665691137313843,
      "learning_rate": 4.506828735318219e-05,
      "loss": 1.4847,
      "step": 32500
    },
    {
      "epoch": 0.3004643540016389,
      "grad_norm": 1.9174461364746094,
      "learning_rate": 4.499241251631309e-05,
      "loss": 1.5001,
      "step": 33000
    },
    {
      "epoch": 0.30501684421378494,
      "grad_norm": 2.1745951175689697,
      "learning_rate": 4.491653767944399e-05,
      "loss": 1.4959,
      "step": 33500
    },
    {
      "epoch": 0.309569334425931,
      "grad_norm": 1.8195202350616455,
      "learning_rate": 4.484066284257489e-05,
      "loss": 1.4538,
      "step": 34000
    },
    {
      "epoch": 0.31412182463807703,
      "grad_norm": 3.1092007160186768,
      "learning_rate": 4.4764788005705786e-05,
      "loss": 1.4918,
      "step": 34500
    },
    {
      "epoch": 0.3186743148502231,
      "grad_norm": 1.930917739868164,
      "learning_rate": 4.4688913168836686e-05,
      "loss": 1.4934,
      "step": 35000
    },
    {
      "epoch": 0.3232268050623691,
      "grad_norm": 2.1174633502960205,
      "learning_rate": 4.4613038331967586e-05,
      "loss": 1.4888,
      "step": 35500
    },
    {
      "epoch": 0.32777929527451516,
      "grad_norm": 1.5831621885299683,
      "learning_rate": 4.4537163495098486e-05,
      "loss": 1.4966,
      "step": 36000
    },
    {
      "epoch": 0.3323317854866612,
      "grad_norm": 1.9858957529067993,
      "learning_rate": 4.4461288658229386e-05,
      "loss": 1.4325,
      "step": 36500
    },
    {
      "epoch": 0.33688427569880725,
      "grad_norm": 2.271111249923706,
      "learning_rate": 4.4385413821360286e-05,
      "loss": 1.4972,
      "step": 37000
    },
    {
      "epoch": 0.3414367659109533,
      "grad_norm": 1.579291820526123,
      "learning_rate": 4.4309538984491186e-05,
      "loss": 1.4988,
      "step": 37500
    },
    {
      "epoch": 0.34598925612309933,
      "grad_norm": 1.7411953210830688,
      "learning_rate": 4.4233664147622086e-05,
      "loss": 1.4576,
      "step": 38000
    },
    {
      "epoch": 0.3505417463352454,
      "grad_norm": 1.0848557949066162,
      "learning_rate": 4.4157789310752986e-05,
      "loss": 1.5001,
      "step": 38500
    },
    {
      "epoch": 0.3550942365473914,
      "grad_norm": 1.7193750143051147,
      "learning_rate": 4.4081914473883886e-05,
      "loss": 1.5135,
      "step": 39000
    },
    {
      "epoch": 0.35964672675953746,
      "grad_norm": 2.076620578765869,
      "learning_rate": 4.4006039637014786e-05,
      "loss": 1.4608,
      "step": 39500
    },
    {
      "epoch": 0.3641992169716835,
      "grad_norm": 1.7793735265731812,
      "learning_rate": 4.3930164800145686e-05,
      "loss": 1.4793,
      "step": 40000
    },
    {
      "epoch": 0.36875170718382955,
      "grad_norm": 1.4366289377212524,
      "learning_rate": 4.385428996327658e-05,
      "loss": 1.4905,
      "step": 40500
    },
    {
      "epoch": 0.3733041973959756,
      "grad_norm": 1.3538599014282227,
      "learning_rate": 4.377841512640748e-05,
      "loss": 1.4519,
      "step": 41000
    },
    {
      "epoch": 0.37785668760812163,
      "grad_norm": 2.698352575302124,
      "learning_rate": 4.370254028953838e-05,
      "loss": 1.5012,
      "step": 41500
    },
    {
      "epoch": 0.3824091778202677,
      "grad_norm": 1.4739645719528198,
      "learning_rate": 4.362666545266928e-05,
      "loss": 1.4796,
      "step": 42000
    },
    {
      "epoch": 0.3869616680324137,
      "grad_norm": 1.7406970262527466,
      "learning_rate": 4.355079061580018e-05,
      "loss": 1.4594,
      "step": 42500
    },
    {
      "epoch": 0.39151415824455976,
      "grad_norm": 1.3127927780151367,
      "learning_rate": 4.347491577893108e-05,
      "loss": 1.4767,
      "step": 43000
    },
    {
      "epoch": 0.3960666484567058,
      "grad_norm": 1.3746943473815918,
      "learning_rate": 4.339904094206197e-05,
      "loss": 1.4544,
      "step": 43500
    },
    {
      "epoch": 0.40061913866885185,
      "grad_norm": 2.2249526977539062,
      "learning_rate": 4.332316610519287e-05,
      "loss": 1.4708,
      "step": 44000
    },
    {
      "epoch": 0.4051716288809979,
      "grad_norm": 2.327796459197998,
      "learning_rate": 4.324729126832378e-05,
      "loss": 1.4801,
      "step": 44500
    },
    {
      "epoch": 0.40972411909314393,
      "grad_norm": 1.5917024612426758,
      "learning_rate": 4.317141643145468e-05,
      "loss": 1.443,
      "step": 45000
    },
    {
      "epoch": 0.41427660930529,
      "grad_norm": 1.9180450439453125,
      "learning_rate": 4.309554159458558e-05,
      "loss": 1.4992,
      "step": 45500
    },
    {
      "epoch": 0.418829099517436,
      "grad_norm": 1.876892328262329,
      "learning_rate": 4.301966675771647e-05,
      "loss": 1.4763,
      "step": 46000
    },
    {
      "epoch": 0.42338158972958206,
      "grad_norm": 2.57507586479187,
      "learning_rate": 4.294379192084737e-05,
      "loss": 1.4715,
      "step": 46500
    },
    {
      "epoch": 0.4279340799417281,
      "grad_norm": 1.6160082817077637,
      "learning_rate": 4.286791708397827e-05,
      "loss": 1.4571,
      "step": 47000
    },
    {
      "epoch": 0.43248657015387415,
      "grad_norm": 2.3625729084014893,
      "learning_rate": 4.279204224710917e-05,
      "loss": 1.472,
      "step": 47500
    },
    {
      "epoch": 0.4370390603660202,
      "grad_norm": 1.4745906591415405,
      "learning_rate": 4.271616741024007e-05,
      "loss": 1.4787,
      "step": 48000
    },
    {
      "epoch": 0.44159155057816624,
      "grad_norm": 1.8253474235534668,
      "learning_rate": 4.264029257337097e-05,
      "loss": 1.4408,
      "step": 48500
    },
    {
      "epoch": 0.4461440407903123,
      "grad_norm": 1.981594204902649,
      "learning_rate": 4.2564417736501865e-05,
      "loss": 1.4827,
      "step": 49000
    },
    {
      "epoch": 0.4506965310024583,
      "grad_norm": 2.4945731163024902,
      "learning_rate": 4.2488542899632765e-05,
      "loss": 1.4422,
      "step": 49500
    },
    {
      "epoch": 0.45524902121460437,
      "grad_norm": 1.399931788444519,
      "learning_rate": 4.2412668062763665e-05,
      "loss": 1.4517,
      "step": 50000
    },
    {
      "epoch": 0.4598015114267504,
      "grad_norm": 1.7381045818328857,
      "learning_rate": 4.2336793225894565e-05,
      "loss": 1.4792,
      "step": 50500
    },
    {
      "epoch": 0.46435400163889645,
      "grad_norm": 1.992401123046875,
      "learning_rate": 4.2260918389025465e-05,
      "loss": 1.4845,
      "step": 51000
    },
    {
      "epoch": 0.4689064918510425,
      "grad_norm": 2.4219253063201904,
      "learning_rate": 4.2185043552156365e-05,
      "loss": 1.4687,
      "step": 51500
    },
    {
      "epoch": 0.47345898206318854,
      "grad_norm": 1.3863270282745361,
      "learning_rate": 4.2109168715287265e-05,
      "loss": 1.4278,
      "step": 52000
    },
    {
      "epoch": 0.4780114722753346,
      "grad_norm": 2.184089422225952,
      "learning_rate": 4.2033293878418164e-05,
      "loss": 1.4607,
      "step": 52500
    },
    {
      "epoch": 0.4825639624874806,
      "grad_norm": 1.573323369026184,
      "learning_rate": 4.1957419041549064e-05,
      "loss": 1.4307,
      "step": 53000
    },
    {
      "epoch": 0.4871164526996267,
      "grad_norm": 1.3278053998947144,
      "learning_rate": 4.1881544204679964e-05,
      "loss": 1.4801,
      "step": 53500
    },
    {
      "epoch": 0.49166894291177277,
      "grad_norm": 1.918517827987671,
      "learning_rate": 4.1805669367810864e-05,
      "loss": 1.4652,
      "step": 54000
    },
    {
      "epoch": 0.4962214331239188,
      "grad_norm": 1.6713398694992065,
      "learning_rate": 4.1729794530941764e-05,
      "loss": 1.457,
      "step": 54500
    },
    {
      "epoch": 0.5007739233360649,
      "grad_norm": 1.9636454582214355,
      "learning_rate": 4.165391969407266e-05,
      "loss": 1.4802,
      "step": 55000
    },
    {
      "epoch": 0.5053264135482108,
      "grad_norm": 2.2944159507751465,
      "learning_rate": 4.157804485720356e-05,
      "loss": 1.4471,
      "step": 55500
    },
    {
      "epoch": 0.5098789037603569,
      "grad_norm": 1.6703966856002808,
      "learning_rate": 4.150217002033446e-05,
      "loss": 1.4511,
      "step": 56000
    },
    {
      "epoch": 0.5144313939725029,
      "grad_norm": 1.5112354755401611,
      "learning_rate": 4.142629518346536e-05,
      "loss": 1.4584,
      "step": 56500
    },
    {
      "epoch": 0.518983884184649,
      "grad_norm": 3.5930309295654297,
      "learning_rate": 4.135042034659626e-05,
      "loss": 1.4827,
      "step": 57000
    },
    {
      "epoch": 0.523536374396795,
      "grad_norm": 1.7980707883834839,
      "learning_rate": 4.127454550972716e-05,
      "loss": 1.4357,
      "step": 57500
    },
    {
      "epoch": 0.5280888646089411,
      "grad_norm": 1.4820793867111206,
      "learning_rate": 4.119867067285805e-05,
      "loss": 1.4636,
      "step": 58000
    },
    {
      "epoch": 0.5326413548210871,
      "grad_norm": 1.6004084348678589,
      "learning_rate": 4.112279583598895e-05,
      "loss": 1.4616,
      "step": 58500
    },
    {
      "epoch": 0.5371938450332332,
      "grad_norm": 1.527901530265808,
      "learning_rate": 4.104692099911985e-05,
      "loss": 1.4212,
      "step": 59000
    },
    {
      "epoch": 0.5417463352453792,
      "grad_norm": 2.45041561126709,
      "learning_rate": 4.097104616225075e-05,
      "loss": 1.4282,
      "step": 59500
    },
    {
      "epoch": 0.5462988254575253,
      "grad_norm": 1.7481502294540405,
      "learning_rate": 4.089517132538165e-05,
      "loss": 1.4231,
      "step": 60000
    },
    {
      "epoch": 0.5508513156696713,
      "grad_norm": 1.3719326257705688,
      "learning_rate": 4.081929648851256e-05,
      "loss": 1.4335,
      "step": 60500
    },
    {
      "epoch": 0.5554038058818174,
      "grad_norm": 1.3403795957565308,
      "learning_rate": 4.074342165164345e-05,
      "loss": 1.4605,
      "step": 61000
    },
    {
      "epoch": 0.5599562960939634,
      "grad_norm": 1.7062550783157349,
      "learning_rate": 4.066754681477435e-05,
      "loss": 1.4654,
      "step": 61500
    },
    {
      "epoch": 0.5645087863061095,
      "grad_norm": 3.0322699546813965,
      "learning_rate": 4.059167197790525e-05,
      "loss": 1.4563,
      "step": 62000
    },
    {
      "epoch": 0.5690612765182554,
      "grad_norm": 1.335919976234436,
      "learning_rate": 4.051579714103615e-05,
      "loss": 1.4979,
      "step": 62500
    },
    {
      "epoch": 0.5736137667304015,
      "grad_norm": 1.6601481437683105,
      "learning_rate": 4.043992230416705e-05,
      "loss": 1.4213,
      "step": 63000
    },
    {
      "epoch": 0.5781662569425475,
      "grad_norm": 1.551808476448059,
      "learning_rate": 4.036404746729795e-05,
      "loss": 1.4586,
      "step": 63500
    },
    {
      "epoch": 0.5827187471546936,
      "grad_norm": 0.9790830016136169,
      "learning_rate": 4.0288172630428843e-05,
      "loss": 1.4297,
      "step": 64000
    },
    {
      "epoch": 0.5872712373668396,
      "grad_norm": 1.507020115852356,
      "learning_rate": 4.0212297793559743e-05,
      "loss": 1.408,
      "step": 64500
    },
    {
      "epoch": 0.5918237275789857,
      "grad_norm": 0.5328876972198486,
      "learning_rate": 4.013642295669064e-05,
      "loss": 1.3959,
      "step": 65000
    },
    {
      "epoch": 0.5963762177911317,
      "grad_norm": 1.2166988849639893,
      "learning_rate": 4.006054811982154e-05,
      "loss": 1.4699,
      "step": 65500
    },
    {
      "epoch": 0.6009287080032778,
      "grad_norm": 2.0083260536193848,
      "learning_rate": 3.998467328295244e-05,
      "loss": 1.4231,
      "step": 66000
    },
    {
      "epoch": 0.6054811982154238,
      "grad_norm": 1.8845000267028809,
      "learning_rate": 3.990879844608334e-05,
      "loss": 1.4701,
      "step": 66500
    },
    {
      "epoch": 0.6100336884275699,
      "grad_norm": 1.0522596836090088,
      "learning_rate": 3.983292360921424e-05,
      "loss": 1.4649,
      "step": 67000
    },
    {
      "epoch": 0.6145861786397159,
      "grad_norm": 1.7644633054733276,
      "learning_rate": 3.975704877234514e-05,
      "loss": 1.4227,
      "step": 67500
    },
    {
      "epoch": 0.619138668851862,
      "grad_norm": 1.6478123664855957,
      "learning_rate": 3.968117393547604e-05,
      "loss": 1.4473,
      "step": 68000
    },
    {
      "epoch": 0.623691159064008,
      "grad_norm": 1.859995722770691,
      "learning_rate": 3.960529909860694e-05,
      "loss": 1.4432,
      "step": 68500
    },
    {
      "epoch": 0.6282436492761541,
      "grad_norm": 2.6457579135894775,
      "learning_rate": 3.952942426173784e-05,
      "loss": 1.4415,
      "step": 69000
    },
    {
      "epoch": 0.6327961394883,
      "grad_norm": 1.523418664932251,
      "learning_rate": 3.945354942486874e-05,
      "loss": 1.444,
      "step": 69500
    },
    {
      "epoch": 0.6373486297004461,
      "grad_norm": 1.3639545440673828,
      "learning_rate": 3.9377674587999636e-05,
      "loss": 1.461,
      "step": 70000
    },
    {
      "epoch": 0.6419011199125921,
      "grad_norm": 1.5062317848205566,
      "learning_rate": 3.9301799751130536e-05,
      "loss": 1.4501,
      "step": 70500
    },
    {
      "epoch": 0.6464536101247382,
      "grad_norm": 1.4950380325317383,
      "learning_rate": 3.9225924914261436e-05,
      "loss": 1.4345,
      "step": 71000
    },
    {
      "epoch": 0.6510061003368843,
      "grad_norm": 1.5224130153656006,
      "learning_rate": 3.9150050077392336e-05,
      "loss": 1.4422,
      "step": 71500
    },
    {
      "epoch": 0.6555585905490303,
      "grad_norm": 1.925525426864624,
      "learning_rate": 3.9074175240523236e-05,
      "loss": 1.427,
      "step": 72000
    },
    {
      "epoch": 0.6601110807611764,
      "grad_norm": 2.675509214401245,
      "learning_rate": 3.8998300403654136e-05,
      "loss": 1.4316,
      "step": 72500
    },
    {
      "epoch": 0.6646635709733224,
      "grad_norm": 1.532267451286316,
      "learning_rate": 3.892242556678503e-05,
      "loss": 1.4292,
      "step": 73000
    },
    {
      "epoch": 0.6692160611854685,
      "grad_norm": 7.720393180847168,
      "learning_rate": 3.884655072991593e-05,
      "loss": 1.4589,
      "step": 73500
    },
    {
      "epoch": 0.6737685513976145,
      "grad_norm": 1.5860896110534668,
      "learning_rate": 3.877067589304683e-05,
      "loss": 1.4481,
      "step": 74000
    },
    {
      "epoch": 0.6783210416097606,
      "grad_norm": 1.8452571630477905,
      "learning_rate": 3.869480105617773e-05,
      "loss": 1.4009,
      "step": 74500
    },
    {
      "epoch": 0.6828735318219066,
      "grad_norm": 1.3081570863723755,
      "learning_rate": 3.861892621930863e-05,
      "loss": 1.4423,
      "step": 75000
    },
    {
      "epoch": 0.6874260220340527,
      "grad_norm": 1.3529257774353027,
      "learning_rate": 3.854305138243953e-05,
      "loss": 1.4351,
      "step": 75500
    },
    {
      "epoch": 0.6919785122461987,
      "grad_norm": 2.912944793701172,
      "learning_rate": 3.846717654557043e-05,
      "loss": 1.4157,
      "step": 76000
    },
    {
      "epoch": 0.6965310024583448,
      "grad_norm": 1.7121022939682007,
      "learning_rate": 3.839130170870133e-05,
      "loss": 1.4668,
      "step": 76500
    },
    {
      "epoch": 0.7010834926704907,
      "grad_norm": 1.5933057069778442,
      "learning_rate": 3.831542687183223e-05,
      "loss": 1.4394,
      "step": 77000
    },
    {
      "epoch": 0.7056359828826368,
      "grad_norm": 2.3423147201538086,
      "learning_rate": 3.823955203496313e-05,
      "loss": 1.419,
      "step": 77500
    },
    {
      "epoch": 0.7101884730947828,
      "grad_norm": 2.3738741874694824,
      "learning_rate": 3.816367719809403e-05,
      "loss": 1.432,
      "step": 78000
    },
    {
      "epoch": 0.7147409633069289,
      "grad_norm": 2.617544412612915,
      "learning_rate": 3.808780236122493e-05,
      "loss": 1.4025,
      "step": 78500
    },
    {
      "epoch": 0.7192934535190749,
      "grad_norm": 1.015317678451538,
      "learning_rate": 3.801192752435582e-05,
      "loss": 1.3974,
      "step": 79000
    },
    {
      "epoch": 0.723845943731221,
      "grad_norm": 1.7523547410964966,
      "learning_rate": 3.793605268748672e-05,
      "loss": 1.4314,
      "step": 79500
    },
    {
      "epoch": 0.728398433943367,
      "grad_norm": 1.621612787246704,
      "learning_rate": 3.786017785061762e-05,
      "loss": 1.4117,
      "step": 80000
    },
    {
      "epoch": 0.7329509241555131,
      "grad_norm": 1.675252079963684,
      "learning_rate": 3.778430301374852e-05,
      "loss": 1.4071,
      "step": 80500
    },
    {
      "epoch": 0.7375034143676591,
      "grad_norm": 2.218848705291748,
      "learning_rate": 3.770842817687942e-05,
      "loss": 1.3938,
      "step": 81000
    },
    {
      "epoch": 0.7420559045798052,
      "grad_norm": 1.0135998725891113,
      "learning_rate": 3.7632553340010315e-05,
      "loss": 1.4161,
      "step": 81500
    },
    {
      "epoch": 0.7466083947919512,
      "grad_norm": 2.363743305206299,
      "learning_rate": 3.7556678503141215e-05,
      "loss": 1.4065,
      "step": 82000
    },
    {
      "epoch": 0.7511608850040973,
      "grad_norm": 1.4600533246994019,
      "learning_rate": 3.7480803666272115e-05,
      "loss": 1.4127,
      "step": 82500
    },
    {
      "epoch": 0.7557133752162433,
      "grad_norm": 1.851562261581421,
      "learning_rate": 3.740492882940302e-05,
      "loss": 1.4511,
      "step": 83000
    },
    {
      "epoch": 0.7602658654283894,
      "grad_norm": 1.828016757965088,
      "learning_rate": 3.732905399253392e-05,
      "loss": 1.4015,
      "step": 83500
    },
    {
      "epoch": 0.7648183556405354,
      "grad_norm": 1.836269497871399,
      "learning_rate": 3.725317915566482e-05,
      "loss": 1.4201,
      "step": 84000
    },
    {
      "epoch": 0.7693708458526815,
      "grad_norm": 1.6285814046859741,
      "learning_rate": 3.7177304318795715e-05,
      "loss": 1.4117,
      "step": 84500
    },
    {
      "epoch": 0.7739233360648274,
      "grad_norm": 1.1409574747085571,
      "learning_rate": 3.7101429481926615e-05,
      "loss": 1.3832,
      "step": 85000
    },
    {
      "epoch": 0.7784758262769735,
      "grad_norm": 1.3484891653060913,
      "learning_rate": 3.7025554645057515e-05,
      "loss": 1.4233,
      "step": 85500
    },
    {
      "epoch": 0.7830283164891195,
      "grad_norm": 2.121706485748291,
      "learning_rate": 3.6949679808188415e-05,
      "loss": 1.402,
      "step": 86000
    },
    {
      "epoch": 0.7875808067012656,
      "grad_norm": 2.5278890132904053,
      "learning_rate": 3.6873804971319315e-05,
      "loss": 1.4242,
      "step": 86500
    },
    {
      "epoch": 0.7921332969134116,
      "grad_norm": 1.5969198942184448,
      "learning_rate": 3.6797930134450215e-05,
      "loss": 1.4048,
      "step": 87000
    },
    {
      "epoch": 0.7966857871255577,
      "grad_norm": 1.7309540510177612,
      "learning_rate": 3.672205529758111e-05,
      "loss": 1.4184,
      "step": 87500
    },
    {
      "epoch": 0.8012382773377037,
      "grad_norm": 3.2090744972229004,
      "learning_rate": 3.664618046071201e-05,
      "loss": 1.4265,
      "step": 88000
    },
    {
      "epoch": 0.8057907675498498,
      "grad_norm": 1.9063198566436768,
      "learning_rate": 3.657030562384291e-05,
      "loss": 1.4297,
      "step": 88500
    },
    {
      "epoch": 0.8103432577619958,
      "grad_norm": 1.5400705337524414,
      "learning_rate": 3.649443078697381e-05,
      "loss": 1.4528,
      "step": 89000
    },
    {
      "epoch": 0.8148957479741419,
      "grad_norm": 1.7730275392532349,
      "learning_rate": 3.641855595010471e-05,
      "loss": 1.4142,
      "step": 89500
    },
    {
      "epoch": 0.8194482381862879,
      "grad_norm": 2.5325891971588135,
      "learning_rate": 3.634268111323561e-05,
      "loss": 1.4327,
      "step": 90000
    },
    {
      "epoch": 0.824000728398434,
      "grad_norm": 2.34662127494812,
      "learning_rate": 3.626680627636651e-05,
      "loss": 1.4232,
      "step": 90500
    },
    {
      "epoch": 0.82855321861058,
      "grad_norm": 1.2610607147216797,
      "learning_rate": 3.619093143949741e-05,
      "loss": 1.4128,
      "step": 91000
    },
    {
      "epoch": 0.833105708822726,
      "grad_norm": 1.5490508079528809,
      "learning_rate": 3.611505660262831e-05,
      "loss": 1.3912,
      "step": 91500
    },
    {
      "epoch": 0.837658199034872,
      "grad_norm": 1.4846287965774536,
      "learning_rate": 3.603918176575921e-05,
      "loss": 1.4253,
      "step": 92000
    },
    {
      "epoch": 0.8422106892470181,
      "grad_norm": 1.5123846530914307,
      "learning_rate": 3.596330692889011e-05,
      "loss": 1.4288,
      "step": 92500
    },
    {
      "epoch": 0.8467631794591641,
      "grad_norm": 2.869833469390869,
      "learning_rate": 3.588743209202101e-05,
      "loss": 1.4053,
      "step": 93000
    },
    {
      "epoch": 0.8513156696713102,
      "grad_norm": 1.3906861543655396,
      "learning_rate": 3.58115572551519e-05,
      "loss": 1.433,
      "step": 93500
    },
    {
      "epoch": 0.8558681598834562,
      "grad_norm": 1.0686962604522705,
      "learning_rate": 3.57356824182828e-05,
      "loss": 1.3779,
      "step": 94000
    },
    {
      "epoch": 0.8604206500956023,
      "grad_norm": 1.898055911064148,
      "learning_rate": 3.56598075814137e-05,
      "loss": 1.3888,
      "step": 94500
    },
    {
      "epoch": 0.8649731403077483,
      "grad_norm": 3.5353546142578125,
      "learning_rate": 3.55839327445446e-05,
      "loss": 1.3903,
      "step": 95000
    },
    {
      "epoch": 0.8695256305198944,
      "grad_norm": 1.470703125,
      "learning_rate": 3.55080579076755e-05,
      "loss": 1.3758,
      "step": 95500
    },
    {
      "epoch": 0.8740781207320404,
      "grad_norm": 1.4897403717041016,
      "learning_rate": 3.54321830708064e-05,
      "loss": 1.4264,
      "step": 96000
    },
    {
      "epoch": 0.8786306109441865,
      "grad_norm": 1.631150245666504,
      "learning_rate": 3.5356308233937294e-05,
      "loss": 1.3956,
      "step": 96500
    },
    {
      "epoch": 0.8831831011563325,
      "grad_norm": 1.4428099393844604,
      "learning_rate": 3.5280433397068194e-05,
      "loss": 1.4073,
      "step": 97000
    },
    {
      "epoch": 0.8877355913684786,
      "grad_norm": 1.879005789756775,
      "learning_rate": 3.5204558560199094e-05,
      "loss": 1.4133,
      "step": 97500
    },
    {
      "epoch": 0.8922880815806246,
      "grad_norm": 1.7584060430526733,
      "learning_rate": 3.5128683723329994e-05,
      "loss": 1.4188,
      "step": 98000
    },
    {
      "epoch": 0.8968405717927707,
      "grad_norm": 1.5144892930984497,
      "learning_rate": 3.50528088864609e-05,
      "loss": 1.4072,
      "step": 98500
    },
    {
      "epoch": 0.9013930620049166,
      "grad_norm": 1.5361531972885132,
      "learning_rate": 3.49769340495918e-05,
      "loss": 1.421,
      "step": 99000
    },
    {
      "epoch": 0.9059455522170627,
      "grad_norm": 1.3133127689361572,
      "learning_rate": 3.4901059212722694e-05,
      "loss": 1.4169,
      "step": 99500
    },
    {
      "epoch": 0.9104980424292087,
      "grad_norm": 1.8255308866500854,
      "learning_rate": 3.4825184375853594e-05,
      "loss": 1.3832,
      "step": 100000
    },
    {
      "epoch": 0.9150505326413548,
      "grad_norm": 1.765775203704834,
      "learning_rate": 3.4749309538984494e-05,
      "loss": 1.3827,
      "step": 100500
    },
    {
      "epoch": 0.9196030228535008,
      "grad_norm": 3.2547006607055664,
      "learning_rate": 3.4673434702115394e-05,
      "loss": 1.4011,
      "step": 101000
    },
    {
      "epoch": 0.9241555130656469,
      "grad_norm": 2.5694596767425537,
      "learning_rate": 3.4597559865246294e-05,
      "loss": 1.3868,
      "step": 101500
    },
    {
      "epoch": 0.9287080032777929,
      "grad_norm": 1.8394837379455566,
      "learning_rate": 3.4521685028377194e-05,
      "loss": 1.4079,
      "step": 102000
    },
    {
      "epoch": 0.933260493489939,
      "grad_norm": 1.544235348701477,
      "learning_rate": 3.444581019150809e-05,
      "loss": 1.4154,
      "step": 102500
    },
    {
      "epoch": 0.937812983702085,
      "grad_norm": 2.3242204189300537,
      "learning_rate": 3.436993535463899e-05,
      "loss": 1.4145,
      "step": 103000
    },
    {
      "epoch": 0.9423654739142311,
      "grad_norm": 1.4065098762512207,
      "learning_rate": 3.429406051776989e-05,
      "loss": 1.3852,
      "step": 103500
    },
    {
      "epoch": 0.9469179641263771,
      "grad_norm": 1.7728382349014282,
      "learning_rate": 3.421818568090079e-05,
      "loss": 1.4082,
      "step": 104000
    },
    {
      "epoch": 0.9514704543385232,
      "grad_norm": 1.1972371339797974,
      "learning_rate": 3.414231084403169e-05,
      "loss": 1.3863,
      "step": 104500
    },
    {
      "epoch": 0.9560229445506692,
      "grad_norm": 1.4880298376083374,
      "learning_rate": 3.406643600716259e-05,
      "loss": 1.3803,
      "step": 105000
    },
    {
      "epoch": 0.9605754347628153,
      "grad_norm": 1.973327875137329,
      "learning_rate": 3.3990561170293487e-05,
      "loss": 1.3844,
      "step": 105500
    },
    {
      "epoch": 0.9651279249749612,
      "grad_norm": 3.1504766941070557,
      "learning_rate": 3.3914686333424387e-05,
      "loss": 1.3887,
      "step": 106000
    },
    {
      "epoch": 0.9696804151871073,
      "grad_norm": 1.6976909637451172,
      "learning_rate": 3.3838811496555287e-05,
      "loss": 1.4344,
      "step": 106500
    },
    {
      "epoch": 0.9742329053992534,
      "grad_norm": 1.5262069702148438,
      "learning_rate": 3.3762936659686186e-05,
      "loss": 1.4188,
      "step": 107000
    },
    {
      "epoch": 0.9787853956113994,
      "grad_norm": 1.7988815307617188,
      "learning_rate": 3.3687061822817086e-05,
      "loss": 1.401,
      "step": 107500
    },
    {
      "epoch": 0.9833378858235455,
      "grad_norm": 2.4446630477905273,
      "learning_rate": 3.3611186985947986e-05,
      "loss": 1.3752,
      "step": 108000
    },
    {
      "epoch": 0.9878903760356915,
      "grad_norm": 1.919823169708252,
      "learning_rate": 3.353531214907888e-05,
      "loss": 1.3984,
      "step": 108500
    },
    {
      "epoch": 0.9924428662478376,
      "grad_norm": 1.7250622510910034,
      "learning_rate": 3.345943731220978e-05,
      "loss": 1.4187,
      "step": 109000
    },
    {
      "epoch": 0.9969953564599836,
      "grad_norm": 1.3520405292510986,
      "learning_rate": 3.338356247534068e-05,
      "loss": 1.3928,
      "step": 109500
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.3585346937179565,
      "eval_runtime": 8752.7613,
      "eval_samples_per_second": 6.274,
      "eval_steps_per_second": 0.784,
      "step": 109830
    },
    {
      "epoch": 1.0015478466721297,
      "grad_norm": 1.0346589088439941,
      "learning_rate": 3.330768763847158e-05,
      "loss": 1.3872,
      "step": 110000
    },
    {
      "epoch": 1.0061003368842758,
      "grad_norm": 1.7906439304351807,
      "learning_rate": 3.323181280160248e-05,
      "loss": 1.3401,
      "step": 110500
    },
    {
      "epoch": 1.0106528270964217,
      "grad_norm": 1.88340163230896,
      "learning_rate": 3.315593796473338e-05,
      "loss": 1.3424,
      "step": 111000
    },
    {
      "epoch": 1.0152053173085678,
      "grad_norm": 1.7193864583969116,
      "learning_rate": 3.308006312786427e-05,
      "loss": 1.3526,
      "step": 111500
    },
    {
      "epoch": 1.0197578075207139,
      "grad_norm": 2.7961347103118896,
      "learning_rate": 3.300418829099517e-05,
      "loss": 1.3315,
      "step": 112000
    },
    {
      "epoch": 1.02431029773286,
      "grad_norm": 1.736942172050476,
      "learning_rate": 3.292831345412607e-05,
      "loss": 1.3785,
      "step": 112500
    },
    {
      "epoch": 1.0288627879450059,
      "grad_norm": 1.5586538314819336,
      "learning_rate": 3.285243861725697e-05,
      "loss": 1.3552,
      "step": 113000
    },
    {
      "epoch": 1.033415278157152,
      "grad_norm": 2.3295154571533203,
      "learning_rate": 3.277656378038787e-05,
      "loss": 1.3586,
      "step": 113500
    },
    {
      "epoch": 1.037967768369298,
      "grad_norm": 2.3068604469299316,
      "learning_rate": 3.270068894351877e-05,
      "loss": 1.372,
      "step": 114000
    },
    {
      "epoch": 1.0425202585814441,
      "grad_norm": 1.605578064918518,
      "learning_rate": 3.262481410664967e-05,
      "loss": 1.374,
      "step": 114500
    },
    {
      "epoch": 1.04707274879359,
      "grad_norm": 1.8648048639297485,
      "learning_rate": 3.254893926978057e-05,
      "loss": 1.3476,
      "step": 115000
    },
    {
      "epoch": 1.0516252390057361,
      "grad_norm": 1.8500488996505737,
      "learning_rate": 3.247306443291147e-05,
      "loss": 1.2911,
      "step": 115500
    },
    {
      "epoch": 1.0561777292178822,
      "grad_norm": 1.9887126684188843,
      "learning_rate": 3.239718959604237e-05,
      "loss": 1.3213,
      "step": 116000
    },
    {
      "epoch": 1.0607302194300283,
      "grad_norm": 1.6582130193710327,
      "learning_rate": 3.232131475917327e-05,
      "loss": 1.3585,
      "step": 116500
    },
    {
      "epoch": 1.0652827096421742,
      "grad_norm": 1.7362922430038452,
      "learning_rate": 3.2245439922304166e-05,
      "loss": 1.3461,
      "step": 117000
    },
    {
      "epoch": 1.0698351998543203,
      "grad_norm": 1.6501516103744507,
      "learning_rate": 3.2169565085435065e-05,
      "loss": 1.3466,
      "step": 117500
    },
    {
      "epoch": 1.0743876900664664,
      "grad_norm": 1.5018956661224365,
      "learning_rate": 3.2093690248565965e-05,
      "loss": 1.3124,
      "step": 118000
    },
    {
      "epoch": 1.0789401802786125,
      "grad_norm": 1.4581304788589478,
      "learning_rate": 3.2017815411696865e-05,
      "loss": 1.3426,
      "step": 118500
    },
    {
      "epoch": 1.0834926704907584,
      "grad_norm": 1.377450942993164,
      "learning_rate": 3.1941940574827765e-05,
      "loss": 1.3432,
      "step": 119000
    },
    {
      "epoch": 1.0880451607029045,
      "grad_norm": 2.512152671813965,
      "learning_rate": 3.1866065737958665e-05,
      "loss": 1.3467,
      "step": 119500
    },
    {
      "epoch": 1.0925976509150506,
      "grad_norm": 1.1767557859420776,
      "learning_rate": 3.179019090108956e-05,
      "loss": 1.3165,
      "step": 120000
    },
    {
      "epoch": 1.0971501411271967,
      "grad_norm": 1.808351993560791,
      "learning_rate": 3.171431606422046e-05,
      "loss": 1.3397,
      "step": 120500
    },
    {
      "epoch": 1.1017026313393425,
      "grad_norm": 2.1824779510498047,
      "learning_rate": 3.1638441227351365e-05,
      "loss": 1.3901,
      "step": 121000
    },
    {
      "epoch": 1.1062551215514886,
      "grad_norm": 1.33114755153656,
      "learning_rate": 3.1562566390482265e-05,
      "loss": 1.3587,
      "step": 121500
    },
    {
      "epoch": 1.1108076117636347,
      "grad_norm": 1.6965357065200806,
      "learning_rate": 3.1486691553613165e-05,
      "loss": 1.3608,
      "step": 122000
    },
    {
      "epoch": 1.1153601019757808,
      "grad_norm": 1.692177653312683,
      "learning_rate": 3.1410816716744065e-05,
      "loss": 1.3454,
      "step": 122500
    },
    {
      "epoch": 1.1199125921879267,
      "grad_norm": 1.4591429233551025,
      "learning_rate": 3.133494187987496e-05,
      "loss": 1.3373,
      "step": 123000
    },
    {
      "epoch": 1.1244650824000728,
      "grad_norm": 1.2995587587356567,
      "learning_rate": 3.125906704300586e-05,
      "loss": 1.3244,
      "step": 123500
    },
    {
      "epoch": 1.129017572612219,
      "grad_norm": 1.6212031841278076,
      "learning_rate": 3.118319220613676e-05,
      "loss": 1.3158,
      "step": 124000
    },
    {
      "epoch": 1.133570062824365,
      "grad_norm": 2.005744457244873,
      "learning_rate": 3.110731736926766e-05,
      "loss": 1.3359,
      "step": 124500
    },
    {
      "epoch": 1.1381225530365109,
      "grad_norm": 3.4786970615386963,
      "learning_rate": 3.103144253239856e-05,
      "loss": 1.3102,
      "step": 125000
    },
    {
      "epoch": 1.142675043248657,
      "grad_norm": 1.561305284500122,
      "learning_rate": 3.095556769552946e-05,
      "loss": 1.3697,
      "step": 125500
    },
    {
      "epoch": 1.147227533460803,
      "grad_norm": 4.323517799377441,
      "learning_rate": 3.087969285866035e-05,
      "loss": 1.3174,
      "step": 126000
    },
    {
      "epoch": 1.1517800236729492,
      "grad_norm": 0.8158659338951111,
      "learning_rate": 3.080381802179125e-05,
      "loss": 1.3347,
      "step": 126500
    },
    {
      "epoch": 1.156332513885095,
      "grad_norm": 0.8384687304496765,
      "learning_rate": 3.072794318492215e-05,
      "loss": 1.3553,
      "step": 127000
    },
    {
      "epoch": 1.1608850040972412,
      "grad_norm": 1.6851309537887573,
      "learning_rate": 3.065206834805305e-05,
      "loss": 1.3169,
      "step": 127500
    },
    {
      "epoch": 1.1654374943093873,
      "grad_norm": 1.4459443092346191,
      "learning_rate": 3.057619351118395e-05,
      "loss": 1.3523,
      "step": 128000
    },
    {
      "epoch": 1.1699899845215334,
      "grad_norm": 1.0672913789749146,
      "learning_rate": 3.0500318674314855e-05,
      "loss": 1.333,
      "step": 128500
    },
    {
      "epoch": 1.1745424747336792,
      "grad_norm": 1.4032045602798462,
      "learning_rate": 3.0424443837445748e-05,
      "loss": 1.3433,
      "step": 129000
    },
    {
      "epoch": 1.1790949649458253,
      "grad_norm": 1.9707062244415283,
      "learning_rate": 3.0348569000576648e-05,
      "loss": 1.3191,
      "step": 129500
    },
    {
      "epoch": 1.1836474551579714,
      "grad_norm": 1.6448880434036255,
      "learning_rate": 3.0272694163707548e-05,
      "loss": 1.3245,
      "step": 130000
    },
    {
      "epoch": 1.1881999453701175,
      "grad_norm": 1.9429068565368652,
      "learning_rate": 3.0196819326838448e-05,
      "loss": 1.3444,
      "step": 130500
    },
    {
      "epoch": 1.1927524355822634,
      "grad_norm": 3.149672031402588,
      "learning_rate": 3.0120944489969348e-05,
      "loss": 1.3019,
      "step": 131000
    },
    {
      "epoch": 1.1973049257944095,
      "grad_norm": 1.0704978704452515,
      "learning_rate": 3.004506965310025e-05,
      "loss": 1.3416,
      "step": 131500
    },
    {
      "epoch": 1.2018574160065556,
      "grad_norm": 1.811996579170227,
      "learning_rate": 2.9969194816231144e-05,
      "loss": 1.335,
      "step": 132000
    },
    {
      "epoch": 1.2064099062187017,
      "grad_norm": 1.251356601715088,
      "learning_rate": 2.9893319979362044e-05,
      "loss": 1.3276,
      "step": 132500
    },
    {
      "epoch": 1.2109623964308476,
      "grad_norm": 4.401016712188721,
      "learning_rate": 2.9817445142492944e-05,
      "loss": 1.3117,
      "step": 133000
    },
    {
      "epoch": 1.2155148866429937,
      "grad_norm": 0.967278242111206,
      "learning_rate": 2.9741570305623844e-05,
      "loss": 1.3817,
      "step": 133500
    },
    {
      "epoch": 1.2200673768551398,
      "grad_norm": 2.813791036605835,
      "learning_rate": 2.9665695468754744e-05,
      "loss": 1.3525,
      "step": 134000
    },
    {
      "epoch": 1.2246198670672859,
      "grad_norm": 1.3094284534454346,
      "learning_rate": 2.9589820631885644e-05,
      "loss": 1.3295,
      "step": 134500
    },
    {
      "epoch": 1.229172357279432,
      "grad_norm": 1.0387040376663208,
      "learning_rate": 2.951394579501654e-05,
      "loss": 1.3469,
      "step": 135000
    },
    {
      "epoch": 1.2337248474915778,
      "grad_norm": 2.019998788833618,
      "learning_rate": 2.943807095814744e-05,
      "loss": 1.3247,
      "step": 135500
    },
    {
      "epoch": 1.238277337703724,
      "grad_norm": 1.3990575075149536,
      "learning_rate": 2.936219612127834e-05,
      "loss": 1.3394,
      "step": 136000
    },
    {
      "epoch": 1.24282982791587,
      "grad_norm": 2.048640489578247,
      "learning_rate": 2.928632128440924e-05,
      "loss": 1.3361,
      "step": 136500
    },
    {
      "epoch": 1.247382318128016,
      "grad_norm": 3.0081748962402344,
      "learning_rate": 2.921044644754014e-05,
      "loss": 1.2802,
      "step": 137000
    },
    {
      "epoch": 1.251934808340162,
      "grad_norm": 1.4741703271865845,
      "learning_rate": 2.913457161067104e-05,
      "loss": 1.3483,
      "step": 137500
    },
    {
      "epoch": 1.2564872985523081,
      "grad_norm": 3.313781261444092,
      "learning_rate": 2.9058696773801937e-05,
      "loss": 1.3261,
      "step": 138000
    },
    {
      "epoch": 1.2610397887644542,
      "grad_norm": 1.610719919204712,
      "learning_rate": 2.8982821936932837e-05,
      "loss": 1.3462,
      "step": 138500
    },
    {
      "epoch": 1.2655922789766003,
      "grad_norm": 2.2081501483917236,
      "learning_rate": 2.8906947100063737e-05,
      "loss": 1.3244,
      "step": 139000
    },
    {
      "epoch": 1.2701447691887462,
      "grad_norm": 1.1500531435012817,
      "learning_rate": 2.8831072263194637e-05,
      "loss": 1.2967,
      "step": 139500
    },
    {
      "epoch": 1.2746972594008923,
      "grad_norm": 0.9683411121368408,
      "learning_rate": 2.8755197426325537e-05,
      "loss": 1.3492,
      "step": 140000
    },
    {
      "epoch": 1.2792497496130384,
      "grad_norm": 1.5783758163452148,
      "learning_rate": 2.8679322589456437e-05,
      "loss": 1.3197,
      "step": 140500
    },
    {
      "epoch": 1.2838022398251843,
      "grad_norm": 1.7220871448516846,
      "learning_rate": 2.860344775258733e-05,
      "loss": 1.3092,
      "step": 141000
    },
    {
      "epoch": 1.2883547300373304,
      "grad_norm": 2.9035158157348633,
      "learning_rate": 2.852757291571823e-05,
      "loss": 1.3101,
      "step": 141500
    },
    {
      "epoch": 1.2929072202494765,
      "grad_norm": 1.4323313236236572,
      "learning_rate": 2.845169807884913e-05,
      "loss": 1.3553,
      "step": 142000
    },
    {
      "epoch": 1.2974597104616226,
      "grad_norm": 1.4018563032150269,
      "learning_rate": 2.8375823241980033e-05,
      "loss": 1.3343,
      "step": 142500
    },
    {
      "epoch": 1.3020122006737687,
      "grad_norm": 1.583981990814209,
      "learning_rate": 2.8299948405110933e-05,
      "loss": 1.3031,
      "step": 143000
    },
    {
      "epoch": 1.3065646908859145,
      "grad_norm": 1.5928168296813965,
      "learning_rate": 2.8224073568241833e-05,
      "loss": 1.3133,
      "step": 143500
    },
    {
      "epoch": 1.3111171810980606,
      "grad_norm": 2.5757739543914795,
      "learning_rate": 2.8148198731372727e-05,
      "loss": 1.3236,
      "step": 144000
    },
    {
      "epoch": 1.3156696713102067,
      "grad_norm": 1.3519634008407593,
      "learning_rate": 2.8072323894503627e-05,
      "loss": 1.3087,
      "step": 144500
    },
    {
      "epoch": 1.3202221615223526,
      "grad_norm": 1.6798925399780273,
      "learning_rate": 2.7996449057634526e-05,
      "loss": 1.307,
      "step": 145000
    },
    {
      "epoch": 1.3247746517344987,
      "grad_norm": 1.3874562978744507,
      "learning_rate": 2.7920574220765426e-05,
      "loss": 1.3315,
      "step": 145500
    },
    {
      "epoch": 1.3293271419466448,
      "grad_norm": 1.9649837017059326,
      "learning_rate": 2.7844699383896326e-05,
      "loss": 1.321,
      "step": 146000
    },
    {
      "epoch": 1.333879632158791,
      "grad_norm": 1.595662236213684,
      "learning_rate": 2.7768824547027226e-05,
      "loss": 1.3124,
      "step": 146500
    },
    {
      "epoch": 1.338432122370937,
      "grad_norm": 1.8520607948303223,
      "learning_rate": 2.7692949710158123e-05,
      "loss": 1.33,
      "step": 147000
    },
    {
      "epoch": 1.3429846125830829,
      "grad_norm": 1.8915382623672485,
      "learning_rate": 2.7617074873289023e-05,
      "loss": 1.3545,
      "step": 147500
    },
    {
      "epoch": 1.347537102795229,
      "grad_norm": 2.4892516136169434,
      "learning_rate": 2.7541200036419923e-05,
      "loss": 1.3264,
      "step": 148000
    },
    {
      "epoch": 1.352089593007375,
      "grad_norm": 1.8140419721603394,
      "learning_rate": 2.7465325199550823e-05,
      "loss": 1.3473,
      "step": 148500
    },
    {
      "epoch": 1.356642083219521,
      "grad_norm": 0.9238882660865784,
      "learning_rate": 2.7389450362681723e-05,
      "loss": 1.3201,
      "step": 149000
    },
    {
      "epoch": 1.361194573431667,
      "grad_norm": 2.3246827125549316,
      "learning_rate": 2.7313575525812623e-05,
      "loss": 1.3092,
      "step": 149500
    },
    {
      "epoch": 1.3657470636438132,
      "grad_norm": 1.8816039562225342,
      "learning_rate": 2.723770068894352e-05,
      "loss": 1.3589,
      "step": 150000
    },
    {
      "epoch": 1.3702995538559593,
      "grad_norm": 2.0951240062713623,
      "learning_rate": 2.716182585207442e-05,
      "loss": 1.2984,
      "step": 150500
    },
    {
      "epoch": 1.3748520440681054,
      "grad_norm": 2.205416202545166,
      "learning_rate": 2.708595101520532e-05,
      "loss": 1.3293,
      "step": 151000
    },
    {
      "epoch": 1.3794045342802512,
      "grad_norm": 2.8396096229553223,
      "learning_rate": 2.701007617833622e-05,
      "loss": 1.305,
      "step": 151500
    },
    {
      "epoch": 1.3839570244923973,
      "grad_norm": 1.847890853881836,
      "learning_rate": 2.693420134146712e-05,
      "loss": 1.3311,
      "step": 152000
    },
    {
      "epoch": 1.3885095147045434,
      "grad_norm": 1.740880012512207,
      "learning_rate": 2.6858326504598012e-05,
      "loss": 1.3071,
      "step": 152500
    },
    {
      "epoch": 1.3930620049166893,
      "grad_norm": 2.251570463180542,
      "learning_rate": 2.6782451667728912e-05,
      "loss": 1.3359,
      "step": 153000
    },
    {
      "epoch": 1.3976144951288354,
      "grad_norm": 2.7269747257232666,
      "learning_rate": 2.6706576830859816e-05,
      "loss": 1.3043,
      "step": 153500
    },
    {
      "epoch": 1.4021669853409815,
      "grad_norm": 1.8080662488937378,
      "learning_rate": 2.6630701993990716e-05,
      "loss": 1.3412,
      "step": 154000
    },
    {
      "epoch": 1.4067194755531276,
      "grad_norm": 1.7722899913787842,
      "learning_rate": 2.6554827157121616e-05,
      "loss": 1.3801,
      "step": 154500
    },
    {
      "epoch": 1.4112719657652737,
      "grad_norm": 2.2214584350585938,
      "learning_rate": 2.6478952320252516e-05,
      "loss": 1.3179,
      "step": 155000
    },
    {
      "epoch": 1.4158244559774196,
      "grad_norm": 1.6643017530441284,
      "learning_rate": 2.640307748338341e-05,
      "loss": 1.357,
      "step": 155500
    },
    {
      "epoch": 1.4203769461895657,
      "grad_norm": 1.2427759170532227,
      "learning_rate": 2.632720264651431e-05,
      "loss": 1.3045,
      "step": 156000
    },
    {
      "epoch": 1.4249294364017118,
      "grad_norm": 0.9536493420600891,
      "learning_rate": 2.625132780964521e-05,
      "loss": 1.3277,
      "step": 156500
    },
    {
      "epoch": 1.4294819266138576,
      "grad_norm": 2.002270221710205,
      "learning_rate": 2.617545297277611e-05,
      "loss": 1.317,
      "step": 157000
    },
    {
      "epoch": 1.4340344168260037,
      "grad_norm": 0.4921259582042694,
      "learning_rate": 2.609957813590701e-05,
      "loss": 1.3152,
      "step": 157500
    },
    {
      "epoch": 1.4385869070381498,
      "grad_norm": 1.8741861581802368,
      "learning_rate": 2.602370329903791e-05,
      "loss": 1.3133,
      "step": 158000
    },
    {
      "epoch": 1.443139397250296,
      "grad_norm": 1.2460625171661377,
      "learning_rate": 2.5947828462168805e-05,
      "loss": 1.282,
      "step": 158500
    },
    {
      "epoch": 1.447691887462442,
      "grad_norm": 3.2104973793029785,
      "learning_rate": 2.5871953625299705e-05,
      "loss": 1.3427,
      "step": 159000
    },
    {
      "epoch": 1.452244377674588,
      "grad_norm": 3.2712881565093994,
      "learning_rate": 2.5796078788430605e-05,
      "loss": 1.3266,
      "step": 159500
    },
    {
      "epoch": 1.456796867886734,
      "grad_norm": 1.8932713270187378,
      "learning_rate": 2.5720203951561505e-05,
      "loss": 1.3143,
      "step": 160000
    },
    {
      "epoch": 1.4613493580988801,
      "grad_norm": 1.288489580154419,
      "learning_rate": 2.5644329114692405e-05,
      "loss": 1.3728,
      "step": 160500
    },
    {
      "epoch": 1.4659018483110262,
      "grad_norm": 1.4560388326644897,
      "learning_rate": 2.5568454277823305e-05,
      "loss": 1.3375,
      "step": 161000
    },
    {
      "epoch": 1.4704543385231723,
      "grad_norm": 1.2329695224761963,
      "learning_rate": 2.5492579440954202e-05,
      "loss": 1.3237,
      "step": 161500
    },
    {
      "epoch": 1.4750068287353182,
      "grad_norm": 4.188831329345703,
      "learning_rate": 2.54167046040851e-05,
      "loss": 1.3132,
      "step": 162000
    },
    {
      "epoch": 1.4795593189474643,
      "grad_norm": 1.5511448383331299,
      "learning_rate": 2.5340829767216e-05,
      "loss": 1.29,
      "step": 162500
    },
    {
      "epoch": 1.4841118091596104,
      "grad_norm": 4.672113418579102,
      "learning_rate": 2.52649549303469e-05,
      "loss": 1.3199,
      "step": 163000
    },
    {
      "epoch": 1.4886642993717563,
      "grad_norm": 1.2039318084716797,
      "learning_rate": 2.51890800934778e-05,
      "loss": 1.2841,
      "step": 163500
    },
    {
      "epoch": 1.4932167895839024,
      "grad_norm": 1.847794532775879,
      "learning_rate": 2.51132052566087e-05,
      "loss": 1.3288,
      "step": 164000
    },
    {
      "epoch": 1.4977692797960485,
      "grad_norm": 1.4057514667510986,
      "learning_rate": 2.5037330419739595e-05,
      "loss": 1.3236,
      "step": 164500
    },
    {
      "epoch": 1.5023217700081943,
      "grad_norm": 2.012225866317749,
      "learning_rate": 2.4961455582870498e-05,
      "loss": 1.3072,
      "step": 165000
    },
    {
      "epoch": 1.5068742602203407,
      "grad_norm": 1.6773337125778198,
      "learning_rate": 2.4885580746001398e-05,
      "loss": 1.3042,
      "step": 165500
    },
    {
      "epoch": 1.5114267504324865,
      "grad_norm": 2.117347240447998,
      "learning_rate": 2.4809705909132298e-05,
      "loss": 1.2905,
      "step": 166000
    },
    {
      "epoch": 1.5159792406446326,
      "grad_norm": 1.8705825805664062,
      "learning_rate": 2.4733831072263198e-05,
      "loss": 1.3386,
      "step": 166500
    },
    {
      "epoch": 1.5205317308567787,
      "grad_norm": 1.494754433631897,
      "learning_rate": 2.4657956235394095e-05,
      "loss": 1.3434,
      "step": 167000
    },
    {
      "epoch": 1.5250842210689246,
      "grad_norm": 1.9470508098602295,
      "learning_rate": 2.4582081398524995e-05,
      "loss": 1.332,
      "step": 167500
    },
    {
      "epoch": 1.5296367112810707,
      "grad_norm": 1.1308850049972534,
      "learning_rate": 2.450620656165589e-05,
      "loss": 1.3116,
      "step": 168000
    },
    {
      "epoch": 1.5341892014932168,
      "grad_norm": 1.1885138750076294,
      "learning_rate": 2.443033172478679e-05,
      "loss": 1.2934,
      "step": 168500
    },
    {
      "epoch": 1.5387416917053627,
      "grad_norm": 1.2506706714630127,
      "learning_rate": 2.435445688791769e-05,
      "loss": 1.3238,
      "step": 169000
    },
    {
      "epoch": 1.543294181917509,
      "grad_norm": 1.812448501586914,
      "learning_rate": 2.427858205104859e-05,
      "loss": 1.3089,
      "step": 169500
    },
    {
      "epoch": 1.5478466721296549,
      "grad_norm": 1.8032556772232056,
      "learning_rate": 2.420270721417949e-05,
      "loss": 1.3079,
      "step": 170000
    },
    {
      "epoch": 1.552399162341801,
      "grad_norm": 1.5847158432006836,
      "learning_rate": 2.412683237731039e-05,
      "loss": 1.3092,
      "step": 170500
    },
    {
      "epoch": 1.556951652553947,
      "grad_norm": 1.7021327018737793,
      "learning_rate": 2.4050957540441288e-05,
      "loss": 1.346,
      "step": 171000
    },
    {
      "epoch": 1.561504142766093,
      "grad_norm": 1.5729948282241821,
      "learning_rate": 2.3975082703572188e-05,
      "loss": 1.32,
      "step": 171500
    },
    {
      "epoch": 1.566056632978239,
      "grad_norm": 1.2317551374435425,
      "learning_rate": 2.3899207866703088e-05,
      "loss": 1.3129,
      "step": 172000
    },
    {
      "epoch": 1.5706091231903851,
      "grad_norm": 1.8153263330459595,
      "learning_rate": 2.3823333029833984e-05,
      "loss": 1.3026,
      "step": 172500
    },
    {
      "epoch": 1.575161613402531,
      "grad_norm": 1.255297303199768,
      "learning_rate": 2.3747458192964887e-05,
      "loss": 1.3328,
      "step": 173000
    },
    {
      "epoch": 1.5797141036146773,
      "grad_norm": 1.3833106756210327,
      "learning_rate": 2.3671583356095787e-05,
      "loss": 1.3335,
      "step": 173500
    },
    {
      "epoch": 1.5842665938268232,
      "grad_norm": 1.813408613204956,
      "learning_rate": 2.3595708519226684e-05,
      "loss": 1.3541,
      "step": 174000
    },
    {
      "epoch": 1.5888190840389693,
      "grad_norm": 2.2668211460113525,
      "learning_rate": 2.3519833682357584e-05,
      "loss": 1.3046,
      "step": 174500
    },
    {
      "epoch": 1.5933715742511154,
      "grad_norm": 1.6130638122558594,
      "learning_rate": 2.3443958845488484e-05,
      "loss": 1.3281,
      "step": 175000
    },
    {
      "epoch": 1.5979240644632613,
      "grad_norm": 1.5277936458587646,
      "learning_rate": 2.336808400861938e-05,
      "loss": 1.3404,
      "step": 175500
    },
    {
      "epoch": 1.6024765546754074,
      "grad_norm": 3.2314531803131104,
      "learning_rate": 2.329220917175028e-05,
      "loss": 1.3108,
      "step": 176000
    },
    {
      "epoch": 1.6070290448875535,
      "grad_norm": 1.3392505645751953,
      "learning_rate": 2.321633433488118e-05,
      "loss": 1.3382,
      "step": 176500
    },
    {
      "epoch": 1.6115815350996994,
      "grad_norm": 0.9741905331611633,
      "learning_rate": 2.314045949801208e-05,
      "loss": 1.3465,
      "step": 177000
    },
    {
      "epoch": 1.6161340253118457,
      "grad_norm": 1.4180400371551514,
      "learning_rate": 2.306458466114298e-05,
      "loss": 1.3116,
      "step": 177500
    },
    {
      "epoch": 1.6206865155239916,
      "grad_norm": 1.7524619102478027,
      "learning_rate": 2.298870982427388e-05,
      "loss": 1.2997,
      "step": 178000
    },
    {
      "epoch": 1.6252390057361377,
      "grad_norm": 1.5839983224868774,
      "learning_rate": 2.2912834987404777e-05,
      "loss": 1.3241,
      "step": 178500
    },
    {
      "epoch": 1.6297914959482838,
      "grad_norm": 1.83384108543396,
      "learning_rate": 2.2836960150535677e-05,
      "loss": 1.3176,
      "step": 179000
    },
    {
      "epoch": 1.6343439861604296,
      "grad_norm": 1.496987223625183,
      "learning_rate": 2.2761085313666577e-05,
      "loss": 1.3032,
      "step": 179500
    },
    {
      "epoch": 1.638896476372576,
      "grad_norm": 0.7502235770225525,
      "learning_rate": 2.2685210476797473e-05,
      "loss": 1.3123,
      "step": 180000
    },
    {
      "epoch": 1.6434489665847218,
      "grad_norm": 2.814448356628418,
      "learning_rate": 2.2609335639928377e-05,
      "loss": 1.3191,
      "step": 180500
    },
    {
      "epoch": 1.648001456796868,
      "grad_norm": 1.026883840560913,
      "learning_rate": 2.2533460803059277e-05,
      "loss": 1.2933,
      "step": 181000
    },
    {
      "epoch": 1.652553947009014,
      "grad_norm": 1.6958979368209839,
      "learning_rate": 2.2457585966190173e-05,
      "loss": 1.3494,
      "step": 181500
    },
    {
      "epoch": 1.65710643722116,
      "grad_norm": 1.4546979665756226,
      "learning_rate": 2.2381711129321073e-05,
      "loss": 1.2992,
      "step": 182000
    },
    {
      "epoch": 1.661658927433306,
      "grad_norm": 3.7702746391296387,
      "learning_rate": 2.2305836292451973e-05,
      "loss": 1.3085,
      "step": 182500
    },
    {
      "epoch": 1.666211417645452,
      "grad_norm": 2.0571863651275635,
      "learning_rate": 2.222996145558287e-05,
      "loss": 1.3442,
      "step": 183000
    },
    {
      "epoch": 1.670763907857598,
      "grad_norm": 2.0650198459625244,
      "learning_rate": 2.215408661871377e-05,
      "loss": 1.2939,
      "step": 183500
    },
    {
      "epoch": 1.6753163980697443,
      "grad_norm": 2.050914764404297,
      "learning_rate": 2.207821178184467e-05,
      "loss": 1.3427,
      "step": 184000
    },
    {
      "epoch": 1.6798688882818902,
      "grad_norm": 1.648142695426941,
      "learning_rate": 2.200233694497557e-05,
      "loss": 1.3085,
      "step": 184500
    },
    {
      "epoch": 1.6844213784940363,
      "grad_norm": 4.423246383666992,
      "learning_rate": 2.192646210810647e-05,
      "loss": 1.3024,
      "step": 185000
    },
    {
      "epoch": 1.6889738687061824,
      "grad_norm": 1.0575041770935059,
      "learning_rate": 2.185058727123737e-05,
      "loss": 1.3384,
      "step": 185500
    },
    {
      "epoch": 1.6935263589183283,
      "grad_norm": 1.0151809453964233,
      "learning_rate": 2.1774712434368266e-05,
      "loss": 1.3211,
      "step": 186000
    },
    {
      "epoch": 1.6980788491304744,
      "grad_norm": 1.007027506828308,
      "learning_rate": 2.1698837597499166e-05,
      "loss": 1.3149,
      "step": 186500
    },
    {
      "epoch": 1.7026313393426205,
      "grad_norm": 1.3097753524780273,
      "learning_rate": 2.1622962760630066e-05,
      "loss": 1.2853,
      "step": 187000
    },
    {
      "epoch": 1.7071838295547663,
      "grad_norm": 2.06523060798645,
      "learning_rate": 2.1547087923760963e-05,
      "loss": 1.3136,
      "step": 187500
    },
    {
      "epoch": 1.7117363197669127,
      "grad_norm": 1.774385929107666,
      "learning_rate": 2.1471213086891863e-05,
      "loss": 1.3119,
      "step": 188000
    },
    {
      "epoch": 1.7162888099790585,
      "grad_norm": 0.9065178632736206,
      "learning_rate": 2.1395338250022766e-05,
      "loss": 1.3135,
      "step": 188500
    },
    {
      "epoch": 1.7208413001912046,
      "grad_norm": 1.649781584739685,
      "learning_rate": 2.1319463413153663e-05,
      "loss": 1.3534,
      "step": 189000
    },
    {
      "epoch": 1.7253937904033507,
      "grad_norm": 2.406254291534424,
      "learning_rate": 2.1243588576284563e-05,
      "loss": 1.275,
      "step": 189500
    },
    {
      "epoch": 1.7299462806154966,
      "grad_norm": 1.4433355331420898,
      "learning_rate": 2.1167713739415463e-05,
      "loss": 1.2855,
      "step": 190000
    },
    {
      "epoch": 1.7344987708276427,
      "grad_norm": 1.7602217197418213,
      "learning_rate": 2.109183890254636e-05,
      "loss": 1.3095,
      "step": 190500
    },
    {
      "epoch": 1.7390512610397888,
      "grad_norm": 1.9585429430007935,
      "learning_rate": 2.101596406567726e-05,
      "loss": 1.3236,
      "step": 191000
    },
    {
      "epoch": 1.7436037512519347,
      "grad_norm": 2.0951359272003174,
      "learning_rate": 2.094008922880816e-05,
      "loss": 1.3083,
      "step": 191500
    },
    {
      "epoch": 1.748156241464081,
      "grad_norm": 1.0139554738998413,
      "learning_rate": 2.086421439193906e-05,
      "loss": 1.2771,
      "step": 192000
    },
    {
      "epoch": 1.7527087316762269,
      "grad_norm": 3.249112606048584,
      "learning_rate": 2.078833955506996e-05,
      "loss": 1.2997,
      "step": 192500
    },
    {
      "epoch": 1.757261221888373,
      "grad_norm": 1.35708487033844,
      "learning_rate": 2.071246471820086e-05,
      "loss": 1.3014,
      "step": 193000
    },
    {
      "epoch": 1.761813712100519,
      "grad_norm": 2.0398905277252197,
      "learning_rate": 2.0636589881331756e-05,
      "loss": 1.304,
      "step": 193500
    },
    {
      "epoch": 1.766366202312665,
      "grad_norm": 1.4281554222106934,
      "learning_rate": 2.0560715044462656e-05,
      "loss": 1.2999,
      "step": 194000
    },
    {
      "epoch": 1.770918692524811,
      "grad_norm": 1.7434582710266113,
      "learning_rate": 2.0484840207593556e-05,
      "loss": 1.2873,
      "step": 194500
    },
    {
      "epoch": 1.7754711827369571,
      "grad_norm": 1.8078045845031738,
      "learning_rate": 2.0408965370724452e-05,
      "loss": 1.3079,
      "step": 195000
    },
    {
      "epoch": 1.780023672949103,
      "grad_norm": 1.8207480907440186,
      "learning_rate": 2.0333090533855352e-05,
      "loss": 1.3148,
      "step": 195500
    },
    {
      "epoch": 1.7845761631612493,
      "grad_norm": 2.051539897918701,
      "learning_rate": 2.0257215696986252e-05,
      "loss": 1.313,
      "step": 196000
    },
    {
      "epoch": 1.7891286533733952,
      "grad_norm": 1.9800575971603394,
      "learning_rate": 2.0181340860117152e-05,
      "loss": 1.292,
      "step": 196500
    },
    {
      "epoch": 1.7936811435855413,
      "grad_norm": 1.3687883615493774,
      "learning_rate": 2.0105466023248052e-05,
      "loss": 1.2946,
      "step": 197000
    },
    {
      "epoch": 1.7982336337976874,
      "grad_norm": 1.606689691543579,
      "learning_rate": 2.0029591186378952e-05,
      "loss": 1.2835,
      "step": 197500
    },
    {
      "epoch": 1.8027861240098333,
      "grad_norm": 1.668310284614563,
      "learning_rate": 1.995371634950985e-05,
      "loss": 1.311,
      "step": 198000
    },
    {
      "epoch": 1.8073386142219794,
      "grad_norm": 1.5143674612045288,
      "learning_rate": 1.987784151264075e-05,
      "loss": 1.328,
      "step": 198500
    },
    {
      "epoch": 1.8118911044341255,
      "grad_norm": 1.2308881282806396,
      "learning_rate": 1.980196667577165e-05,
      "loss": 1.2966,
      "step": 199000
    },
    {
      "epoch": 1.8164435946462714,
      "grad_norm": 1.9216700792312622,
      "learning_rate": 1.9726091838902545e-05,
      "loss": 1.3152,
      "step": 199500
    },
    {
      "epoch": 1.8209960848584177,
      "grad_norm": 1.6158149242401123,
      "learning_rate": 1.965021700203345e-05,
      "loss": 1.2793,
      "step": 200000
    },
    {
      "epoch": 1.8255485750705636,
      "grad_norm": 1.1222878694534302,
      "learning_rate": 1.957434216516435e-05,
      "loss": 1.3161,
      "step": 200500
    },
    {
      "epoch": 1.8301010652827097,
      "grad_norm": 1.535786509513855,
      "learning_rate": 1.9498467328295245e-05,
      "loss": 1.29,
      "step": 201000
    },
    {
      "epoch": 1.8346535554948558,
      "grad_norm": 1.2622369527816772,
      "learning_rate": 1.9422592491426145e-05,
      "loss": 1.2841,
      "step": 201500
    },
    {
      "epoch": 1.8392060457070016,
      "grad_norm": 3.5427122116088867,
      "learning_rate": 1.9346717654557045e-05,
      "loss": 1.3074,
      "step": 202000
    },
    {
      "epoch": 1.8437585359191477,
      "grad_norm": 1.4522877931594849,
      "learning_rate": 1.927084281768794e-05,
      "loss": 1.2934,
      "step": 202500
    },
    {
      "epoch": 1.8483110261312938,
      "grad_norm": 2.219264030456543,
      "learning_rate": 1.919496798081884e-05,
      "loss": 1.3173,
      "step": 203000
    },
    {
      "epoch": 1.8528635163434397,
      "grad_norm": 1.9081783294677734,
      "learning_rate": 1.911909314394974e-05,
      "loss": 1.3229,
      "step": 203500
    },
    {
      "epoch": 1.857416006555586,
      "grad_norm": 1.4428763389587402,
      "learning_rate": 1.904321830708064e-05,
      "loss": 1.3367,
      "step": 204000
    },
    {
      "epoch": 1.861968496767732,
      "grad_norm": 1.497144103050232,
      "learning_rate": 1.896734347021154e-05,
      "loss": 1.2971,
      "step": 204500
    },
    {
      "epoch": 1.866520986979878,
      "grad_norm": 1.484753966331482,
      "learning_rate": 1.8891468633342438e-05,
      "loss": 1.2827,
      "step": 205000
    },
    {
      "epoch": 1.871073477192024,
      "grad_norm": 2.1920154094696045,
      "learning_rate": 1.8815593796473338e-05,
      "loss": 1.3254,
      "step": 205500
    },
    {
      "epoch": 1.87562596740417,
      "grad_norm": 2.901679039001465,
      "learning_rate": 1.8739718959604238e-05,
      "loss": 1.321,
      "step": 206000
    },
    {
      "epoch": 1.880178457616316,
      "grad_norm": 1.5530126094818115,
      "learning_rate": 1.8663844122735134e-05,
      "loss": 1.313,
      "step": 206500
    },
    {
      "epoch": 1.8847309478284622,
      "grad_norm": 2.012547254562378,
      "learning_rate": 1.8587969285866034e-05,
      "loss": 1.3118,
      "step": 207000
    },
    {
      "epoch": 1.889283438040608,
      "grad_norm": 2.0960752964019775,
      "learning_rate": 1.8512094448996938e-05,
      "loss": 1.3075,
      "step": 207500
    },
    {
      "epoch": 1.8938359282527544,
      "grad_norm": 1.3937523365020752,
      "learning_rate": 1.8436219612127834e-05,
      "loss": 1.3033,
      "step": 208000
    },
    {
      "epoch": 1.8983884184649003,
      "grad_norm": 1.2954022884368896,
      "learning_rate": 1.8360344775258734e-05,
      "loss": 1.2981,
      "step": 208500
    },
    {
      "epoch": 1.9029409086770463,
      "grad_norm": 1.542432188987732,
      "learning_rate": 1.8284469938389634e-05,
      "loss": 1.321,
      "step": 209000
    },
    {
      "epoch": 1.9074933988891924,
      "grad_norm": 1.6144071817398071,
      "learning_rate": 1.820859510152053e-05,
      "loss": 1.3179,
      "step": 209500
    },
    {
      "epoch": 1.9120458891013383,
      "grad_norm": 1.8272895812988281,
      "learning_rate": 1.813272026465143e-05,
      "loss": 1.2954,
      "step": 210000
    },
    {
      "epoch": 1.9165983793134844,
      "grad_norm": 1.7835822105407715,
      "learning_rate": 1.805684542778233e-05,
      "loss": 1.2911,
      "step": 210500
    },
    {
      "epoch": 1.9211508695256305,
      "grad_norm": 3.8876395225524902,
      "learning_rate": 1.798097059091323e-05,
      "loss": 1.307,
      "step": 211000
    },
    {
      "epoch": 1.9257033597377764,
      "grad_norm": 2.9842615127563477,
      "learning_rate": 1.790509575404413e-05,
      "loss": 1.2784,
      "step": 211500
    },
    {
      "epoch": 1.9302558499499227,
      "grad_norm": 1.462430715560913,
      "learning_rate": 1.782922091717503e-05,
      "loss": 1.2642,
      "step": 212000
    },
    {
      "epoch": 1.9348083401620686,
      "grad_norm": 2.187493324279785,
      "learning_rate": 1.7753346080305927e-05,
      "loss": 1.3199,
      "step": 212500
    },
    {
      "epoch": 1.9393608303742147,
      "grad_norm": 1.5249699354171753,
      "learning_rate": 1.7677471243436827e-05,
      "loss": 1.2764,
      "step": 213000
    },
    {
      "epoch": 1.9439133205863608,
      "grad_norm": 1.7772417068481445,
      "learning_rate": 1.7601596406567727e-05,
      "loss": 1.312,
      "step": 213500
    },
    {
      "epoch": 1.9484658107985067,
      "grad_norm": 1.7825020551681519,
      "learning_rate": 1.7525721569698624e-05,
      "loss": 1.2559,
      "step": 214000
    },
    {
      "epoch": 1.953018301010653,
      "grad_norm": 1.6443028450012207,
      "learning_rate": 1.7449846732829524e-05,
      "loss": 1.2861,
      "step": 214500
    },
    {
      "epoch": 1.9575707912227989,
      "grad_norm": 1.441727638244629,
      "learning_rate": 1.7373971895960424e-05,
      "loss": 1.3176,
      "step": 215000
    },
    {
      "epoch": 1.962123281434945,
      "grad_norm": 1.3216092586517334,
      "learning_rate": 1.7298097059091324e-05,
      "loss": 1.2969,
      "step": 215500
    },
    {
      "epoch": 1.966675771647091,
      "grad_norm": 0.9373908638954163,
      "learning_rate": 1.7222222222222224e-05,
      "loss": 1.2939,
      "step": 216000
    },
    {
      "epoch": 1.971228261859237,
      "grad_norm": 1.6834022998809814,
      "learning_rate": 1.7146347385353124e-05,
      "loss": 1.2785,
      "step": 216500
    },
    {
      "epoch": 1.975780752071383,
      "grad_norm": 1.567661166191101,
      "learning_rate": 1.707047254848402e-05,
      "loss": 1.3165,
      "step": 217000
    },
    {
      "epoch": 1.9803332422835291,
      "grad_norm": 2.280330181121826,
      "learning_rate": 1.699459771161492e-05,
      "loss": 1.3291,
      "step": 217500
    },
    {
      "epoch": 1.984885732495675,
      "grad_norm": 2.101574659347534,
      "learning_rate": 1.691872287474582e-05,
      "loss": 1.3091,
      "step": 218000
    },
    {
      "epoch": 1.9894382227078213,
      "grad_norm": 2.850477933883667,
      "learning_rate": 1.6842848037876717e-05,
      "loss": 1.2925,
      "step": 218500
    },
    {
      "epoch": 1.9939907129199672,
      "grad_norm": 2.522669553756714,
      "learning_rate": 1.676697320100762e-05,
      "loss": 1.2956,
      "step": 219000
    },
    {
      "epoch": 1.9985432031321133,
      "grad_norm": 2.072850465774536,
      "learning_rate": 1.669109836413852e-05,
      "loss": 1.2559,
      "step": 219500
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.2900017499923706,
      "eval_runtime": 7938.0982,
      "eval_samples_per_second": 6.918,
      "eval_steps_per_second": 0.865,
      "step": 219660
    },
    {
      "epoch": 2.0030956933442594,
      "grad_norm": 1.650156855583191,
      "learning_rate": 1.6615223527269417e-05,
      "loss": 1.2671,
      "step": 220000
    },
    {
      "epoch": 2.0076481835564053,
      "grad_norm": 3.722804307937622,
      "learning_rate": 1.6539348690400317e-05,
      "loss": 1.2589,
      "step": 220500
    },
    {
      "epoch": 2.0122006737685516,
      "grad_norm": 1.3536323308944702,
      "learning_rate": 1.6463473853531217e-05,
      "loss": 1.2704,
      "step": 221000
    },
    {
      "epoch": 2.0167531639806975,
      "grad_norm": 1.6350234746932983,
      "learning_rate": 1.6387599016662113e-05,
      "loss": 1.2737,
      "step": 221500
    },
    {
      "epoch": 2.0213056541928434,
      "grad_norm": 3.431196451187134,
      "learning_rate": 1.6311724179793013e-05,
      "loss": 1.247,
      "step": 222000
    },
    {
      "epoch": 2.0258581444049897,
      "grad_norm": 2.2560105323791504,
      "learning_rate": 1.6235849342923913e-05,
      "loss": 1.2373,
      "step": 222500
    },
    {
      "epoch": 2.0304106346171356,
      "grad_norm": 1.785232663154602,
      "learning_rate": 1.6159974506054813e-05,
      "loss": 1.2314,
      "step": 223000
    },
    {
      "epoch": 2.0349631248292814,
      "grad_norm": 1.5430951118469238,
      "learning_rate": 1.6084099669185713e-05,
      "loss": 1.2575,
      "step": 223500
    },
    {
      "epoch": 2.0395156150414278,
      "grad_norm": 1.7156634330749512,
      "learning_rate": 1.6008224832316613e-05,
      "loss": 1.2554,
      "step": 224000
    },
    {
      "epoch": 2.0440681052535736,
      "grad_norm": 2.926072120666504,
      "learning_rate": 1.593234999544751e-05,
      "loss": 1.2357,
      "step": 224500
    },
    {
      "epoch": 2.04862059546572,
      "grad_norm": 2.888049602508545,
      "learning_rate": 1.585647515857841e-05,
      "loss": 1.2273,
      "step": 225000
    },
    {
      "epoch": 2.053173085677866,
      "grad_norm": 1.8383628129959106,
      "learning_rate": 1.578060032170931e-05,
      "loss": 1.277,
      "step": 225500
    },
    {
      "epoch": 2.0577255758900117,
      "grad_norm": 2.303338050842285,
      "learning_rate": 1.5704725484840206e-05,
      "loss": 1.2667,
      "step": 226000
    },
    {
      "epoch": 2.062278066102158,
      "grad_norm": 1.8259650468826294,
      "learning_rate": 1.5628850647971106e-05,
      "loss": 1.23,
      "step": 226500
    },
    {
      "epoch": 2.066830556314304,
      "grad_norm": 1.9282273054122925,
      "learning_rate": 1.555297581110201e-05,
      "loss": 1.2598,
      "step": 227000
    },
    {
      "epoch": 2.07138304652645,
      "grad_norm": 2.146799325942993,
      "learning_rate": 1.5477100974232906e-05,
      "loss": 1.2289,
      "step": 227500
    },
    {
      "epoch": 2.075935536738596,
      "grad_norm": 2.4346697330474854,
      "learning_rate": 1.5401226137363806e-05,
      "loss": 1.2212,
      "step": 228000
    },
    {
      "epoch": 2.080488026950742,
      "grad_norm": 2.284818172454834,
      "learning_rate": 1.5325351300494706e-05,
      "loss": 1.2434,
      "step": 228500
    },
    {
      "epoch": 2.0850405171628883,
      "grad_norm": 1.5248558521270752,
      "learning_rate": 1.5249476463625603e-05,
      "loss": 1.2152,
      "step": 229000
    },
    {
      "epoch": 2.089593007375034,
      "grad_norm": 1.526095986366272,
      "learning_rate": 1.5173601626756504e-05,
      "loss": 1.2482,
      "step": 229500
    },
    {
      "epoch": 2.09414549758718,
      "grad_norm": 2.1469204425811768,
      "learning_rate": 1.5097726789887404e-05,
      "loss": 1.235,
      "step": 230000
    },
    {
      "epoch": 2.0986979877993264,
      "grad_norm": 1.893846035003662,
      "learning_rate": 1.50218519530183e-05,
      "loss": 1.2313,
      "step": 230500
    },
    {
      "epoch": 2.1032504780114722,
      "grad_norm": 1.7460681200027466,
      "learning_rate": 1.49459771161492e-05,
      "loss": 1.2339,
      "step": 231000
    },
    {
      "epoch": 2.107802968223618,
      "grad_norm": 1.2238456010818481,
      "learning_rate": 1.48701022792801e-05,
      "loss": 1.2592,
      "step": 231500
    },
    {
      "epoch": 2.1123554584357644,
      "grad_norm": 2.472846508026123,
      "learning_rate": 1.4794227442410999e-05,
      "loss": 1.2561,
      "step": 232000
    },
    {
      "epoch": 2.1169079486479103,
      "grad_norm": 2.0690176486968994,
      "learning_rate": 1.4718352605541899e-05,
      "loss": 1.2508,
      "step": 232500
    },
    {
      "epoch": 2.1214604388600566,
      "grad_norm": 1.5738964080810547,
      "learning_rate": 1.4642477768672799e-05,
      "loss": 1.2568,
      "step": 233000
    },
    {
      "epoch": 2.1260129290722025,
      "grad_norm": 1.4435229301452637,
      "learning_rate": 1.4566602931803697e-05,
      "loss": 1.2623,
      "step": 233500
    },
    {
      "epoch": 2.1305654192843484,
      "grad_norm": 2.025221824645996,
      "learning_rate": 1.4490728094934597e-05,
      "loss": 1.2772,
      "step": 234000
    },
    {
      "epoch": 2.1351179094964947,
      "grad_norm": 2.413177013397217,
      "learning_rate": 1.4414853258065497e-05,
      "loss": 1.2471,
      "step": 234500
    },
    {
      "epoch": 2.1396703997086406,
      "grad_norm": 1.7952046394348145,
      "learning_rate": 1.4338978421196394e-05,
      "loss": 1.2536,
      "step": 235000
    },
    {
      "epoch": 2.1442228899207865,
      "grad_norm": 1.6738560199737549,
      "learning_rate": 1.4263103584327295e-05,
      "loss": 1.2648,
      "step": 235500
    },
    {
      "epoch": 2.148775380132933,
      "grad_norm": 2.020228862762451,
      "learning_rate": 1.4187228747458195e-05,
      "loss": 1.251,
      "step": 236000
    },
    {
      "epoch": 2.1533278703450787,
      "grad_norm": 2.4796743392944336,
      "learning_rate": 1.4111353910589092e-05,
      "loss": 1.2383,
      "step": 236500
    },
    {
      "epoch": 2.157880360557225,
      "grad_norm": 1.7156853675842285,
      "learning_rate": 1.4035479073719992e-05,
      "loss": 1.2441,
      "step": 237000
    },
    {
      "epoch": 2.162432850769371,
      "grad_norm": 1.840744137763977,
      "learning_rate": 1.3959604236850894e-05,
      "loss": 1.2378,
      "step": 237500
    },
    {
      "epoch": 2.1669853409815167,
      "grad_norm": 1.2780590057373047,
      "learning_rate": 1.388372939998179e-05,
      "loss": 1.2589,
      "step": 238000
    },
    {
      "epoch": 2.171537831193663,
      "grad_norm": 3.4635093212127686,
      "learning_rate": 1.380785456311269e-05,
      "loss": 1.2479,
      "step": 238500
    },
    {
      "epoch": 2.176090321405809,
      "grad_norm": 1.1118619441986084,
      "learning_rate": 1.3731979726243588e-05,
      "loss": 1.253,
      "step": 239000
    },
    {
      "epoch": 2.180642811617955,
      "grad_norm": 2.535376787185669,
      "learning_rate": 1.3656104889374488e-05,
      "loss": 1.2181,
      "step": 239500
    },
    {
      "epoch": 2.185195301830101,
      "grad_norm": 2.4831526279449463,
      "learning_rate": 1.3580230052505388e-05,
      "loss": 1.2696,
      "step": 240000
    },
    {
      "epoch": 2.189747792042247,
      "grad_norm": 1.56392240524292,
      "learning_rate": 1.3504355215636285e-05,
      "loss": 1.2925,
      "step": 240500
    },
    {
      "epoch": 2.1943002822543933,
      "grad_norm": 1.2222754955291748,
      "learning_rate": 1.3428480378767187e-05,
      "loss": 1.2368,
      "step": 241000
    },
    {
      "epoch": 2.198852772466539,
      "grad_norm": 1.9213379621505737,
      "learning_rate": 1.3352605541898087e-05,
      "loss": 1.276,
      "step": 241500
    },
    {
      "epoch": 2.203405262678685,
      "grad_norm": 1.9495081901550293,
      "learning_rate": 1.3276730705028983e-05,
      "loss": 1.2725,
      "step": 242000
    },
    {
      "epoch": 2.2079577528908314,
      "grad_norm": 2.192897319793701,
      "learning_rate": 1.3200855868159883e-05,
      "loss": 1.2753,
      "step": 242500
    },
    {
      "epoch": 2.2125102431029773,
      "grad_norm": 1.7728641033172607,
      "learning_rate": 1.3124981031290785e-05,
      "loss": 1.2291,
      "step": 243000
    },
    {
      "epoch": 2.2170627333151236,
      "grad_norm": 1.8025158643722534,
      "learning_rate": 1.3049106194421681e-05,
      "loss": 1.2693,
      "step": 243500
    },
    {
      "epoch": 2.2216152235272695,
      "grad_norm": 2.1367082595825195,
      "learning_rate": 1.2973231357552581e-05,
      "loss": 1.2245,
      "step": 244000
    },
    {
      "epoch": 2.2261677137394154,
      "grad_norm": 1.8311567306518555,
      "learning_rate": 1.2897356520683481e-05,
      "loss": 1.2905,
      "step": 244500
    },
    {
      "epoch": 2.2307202039515617,
      "grad_norm": 4.589134693145752,
      "learning_rate": 1.282148168381438e-05,
      "loss": 1.2459,
      "step": 245000
    },
    {
      "epoch": 2.2352726941637076,
      "grad_norm": 1.0887116193771362,
      "learning_rate": 1.274560684694528e-05,
      "loss": 1.2825,
      "step": 245500
    },
    {
      "epoch": 2.2398251843758534,
      "grad_norm": 2.0692858695983887,
      "learning_rate": 1.266973201007618e-05,
      "loss": 1.235,
      "step": 246000
    },
    {
      "epoch": 2.2443776745879997,
      "grad_norm": 1.9352463483810425,
      "learning_rate": 1.2593857173207078e-05,
      "loss": 1.2279,
      "step": 246500
    },
    {
      "epoch": 2.2489301648001456,
      "grad_norm": 2.1613807678222656,
      "learning_rate": 1.2517982336337978e-05,
      "loss": 1.2264,
      "step": 247000
    },
    {
      "epoch": 2.2534826550122915,
      "grad_norm": 1.231446623802185,
      "learning_rate": 1.2442107499468876e-05,
      "loss": 1.2517,
      "step": 247500
    },
    {
      "epoch": 2.258035145224438,
      "grad_norm": 1.8540310859680176,
      "learning_rate": 1.2366232662599776e-05,
      "loss": 1.254,
      "step": 248000
    },
    {
      "epoch": 2.2625876354365837,
      "grad_norm": 1.866591453552246,
      "learning_rate": 1.2290357825730674e-05,
      "loss": 1.2437,
      "step": 248500
    },
    {
      "epoch": 2.26714012564873,
      "grad_norm": 1.6210315227508545,
      "learning_rate": 1.2214482988861574e-05,
      "loss": 1.2648,
      "step": 249000
    },
    {
      "epoch": 2.271692615860876,
      "grad_norm": 2.1640875339508057,
      "learning_rate": 1.2138608151992474e-05,
      "loss": 1.1969,
      "step": 249500
    },
    {
      "epoch": 2.2762451060730218,
      "grad_norm": 3.4526968002319336,
      "learning_rate": 1.2062733315123372e-05,
      "loss": 1.2727,
      "step": 250000
    },
    {
      "epoch": 2.280797596285168,
      "grad_norm": 2.1876511573791504,
      "learning_rate": 1.1986858478254272e-05,
      "loss": 1.2373,
      "step": 250500
    },
    {
      "epoch": 2.285350086497314,
      "grad_norm": 2.6014533042907715,
      "learning_rate": 1.1910983641385172e-05,
      "loss": 1.2611,
      "step": 251000
    },
    {
      "epoch": 2.2899025767094603,
      "grad_norm": 1.6169565916061401,
      "learning_rate": 1.183510880451607e-05,
      "loss": 1.242,
      "step": 251500
    },
    {
      "epoch": 2.294455066921606,
      "grad_norm": 1.3460602760314941,
      "learning_rate": 1.1759233967646969e-05,
      "loss": 1.2296,
      "step": 252000
    },
    {
      "epoch": 2.299007557133752,
      "grad_norm": 1.6315096616744995,
      "learning_rate": 1.168335913077787e-05,
      "loss": 1.2481,
      "step": 252500
    },
    {
      "epoch": 2.3035600473458984,
      "grad_norm": 2.008389711380005,
      "learning_rate": 1.1607484293908769e-05,
      "loss": 1.2594,
      "step": 253000
    },
    {
      "epoch": 2.3081125375580442,
      "grad_norm": 2.102761745452881,
      "learning_rate": 1.1531609457039667e-05,
      "loss": 1.2462,
      "step": 253500
    },
    {
      "epoch": 2.31266502777019,
      "grad_norm": 1.3784340620040894,
      "learning_rate": 1.1455734620170567e-05,
      "loss": 1.2435,
      "step": 254000
    },
    {
      "epoch": 2.3172175179823364,
      "grad_norm": 2.0040347576141357,
      "learning_rate": 1.1379859783301467e-05,
      "loss": 1.2752,
      "step": 254500
    },
    {
      "epoch": 2.3217700081944823,
      "grad_norm": 2.7261130809783936,
      "learning_rate": 1.1303984946432365e-05,
      "loss": 1.2464,
      "step": 255000
    },
    {
      "epoch": 2.326322498406628,
      "grad_norm": 1.8624333143234253,
      "learning_rate": 1.1228110109563265e-05,
      "loss": 1.2528,
      "step": 255500
    },
    {
      "epoch": 2.3308749886187745,
      "grad_norm": 1.787399172782898,
      "learning_rate": 1.1152235272694164e-05,
      "loss": 1.2501,
      "step": 256000
    },
    {
      "epoch": 2.3354274788309204,
      "grad_norm": 1.9694221019744873,
      "learning_rate": 1.1076360435825064e-05,
      "loss": 1.2694,
      "step": 256500
    },
    {
      "epoch": 2.3399799690430667,
      "grad_norm": 3.261828899383545,
      "learning_rate": 1.1000485598955964e-05,
      "loss": 1.2224,
      "step": 257000
    },
    {
      "epoch": 2.3445324592552126,
      "grad_norm": 1.951746940612793,
      "learning_rate": 1.0924610762086862e-05,
      "loss": 1.2294,
      "step": 257500
    },
    {
      "epoch": 2.3490849494673585,
      "grad_norm": 1.9700804948806763,
      "learning_rate": 1.084873592521776e-05,
      "loss": 1.2623,
      "step": 258000
    },
    {
      "epoch": 2.353637439679505,
      "grad_norm": 2.19085955619812,
      "learning_rate": 1.0772861088348662e-05,
      "loss": 1.2422,
      "step": 258500
    },
    {
      "epoch": 2.3581899298916507,
      "grad_norm": 2.3259971141815186,
      "learning_rate": 1.069698625147956e-05,
      "loss": 1.246,
      "step": 259000
    },
    {
      "epoch": 2.362742420103797,
      "grad_norm": 2.3726558685302734,
      "learning_rate": 1.0621111414610458e-05,
      "loss": 1.2236,
      "step": 259500
    },
    {
      "epoch": 2.367294910315943,
      "grad_norm": 4.542126655578613,
      "learning_rate": 1.0545236577741358e-05,
      "loss": 1.2468,
      "step": 260000
    },
    {
      "epoch": 2.3718474005280887,
      "grad_norm": 3.919049024581909,
      "learning_rate": 1.0469361740872258e-05,
      "loss": 1.2521,
      "step": 260500
    },
    {
      "epoch": 2.376399890740235,
      "grad_norm": 1.7188220024108887,
      "learning_rate": 1.0393486904003156e-05,
      "loss": 1.2318,
      "step": 261000
    },
    {
      "epoch": 2.380952380952381,
      "grad_norm": 2.510026693344116,
      "learning_rate": 1.0317612067134056e-05,
      "loss": 1.2275,
      "step": 261500
    },
    {
      "epoch": 2.385504871164527,
      "grad_norm": 1.938626766204834,
      "learning_rate": 1.0241737230264955e-05,
      "loss": 1.2437,
      "step": 262000
    },
    {
      "epoch": 2.390057361376673,
      "grad_norm": 1.9607993364334106,
      "learning_rate": 1.0165862393395855e-05,
      "loss": 1.2132,
      "step": 262500
    },
    {
      "epoch": 2.394609851588819,
      "grad_norm": 1.8084282875061035,
      "learning_rate": 1.0089987556526755e-05,
      "loss": 1.2412,
      "step": 263000
    },
    {
      "epoch": 2.399162341800965,
      "grad_norm": 1.5116305351257324,
      "learning_rate": 1.0014112719657653e-05,
      "loss": 1.2298,
      "step": 263500
    },
    {
      "epoch": 2.403714832013111,
      "grad_norm": 1.70967698097229,
      "learning_rate": 9.938237882788553e-06,
      "loss": 1.2568,
      "step": 264000
    },
    {
      "epoch": 2.408267322225257,
      "grad_norm": 3.2060441970825195,
      "learning_rate": 9.862363045919453e-06,
      "loss": 1.248,
      "step": 264500
    },
    {
      "epoch": 2.4128198124374034,
      "grad_norm": 2.8674259185791016,
      "learning_rate": 9.786488209050351e-06,
      "loss": 1.2174,
      "step": 265000
    },
    {
      "epoch": 2.4173723026495493,
      "grad_norm": 1.4307844638824463,
      "learning_rate": 9.71061337218125e-06,
      "loss": 1.2811,
      "step": 265500
    },
    {
      "epoch": 2.421924792861695,
      "grad_norm": 2.1733834743499756,
      "learning_rate": 9.63473853531215e-06,
      "loss": 1.2108,
      "step": 266000
    },
    {
      "epoch": 2.4264772830738415,
      "grad_norm": 1.7925256490707397,
      "learning_rate": 9.55886369844305e-06,
      "loss": 1.2241,
      "step": 266500
    },
    {
      "epoch": 2.4310297732859873,
      "grad_norm": 1.4829766750335693,
      "learning_rate": 9.482988861573948e-06,
      "loss": 1.2601,
      "step": 267000
    },
    {
      "epoch": 2.4355822634981337,
      "grad_norm": 4.542097091674805,
      "learning_rate": 9.407114024704846e-06,
      "loss": 1.254,
      "step": 267500
    },
    {
      "epoch": 2.4401347537102795,
      "grad_norm": 2.1861047744750977,
      "learning_rate": 9.331239187835748e-06,
      "loss": 1.2395,
      "step": 268000
    },
    {
      "epoch": 2.4446872439224254,
      "grad_norm": 1.3613041639328003,
      "learning_rate": 9.255364350966646e-06,
      "loss": 1.2504,
      "step": 268500
    },
    {
      "epoch": 2.4492397341345717,
      "grad_norm": 1.540963888168335,
      "learning_rate": 9.179489514097544e-06,
      "loss": 1.267,
      "step": 269000
    },
    {
      "epoch": 2.4537922243467176,
      "grad_norm": 1.9397189617156982,
      "learning_rate": 9.103614677228444e-06,
      "loss": 1.2757,
      "step": 269500
    },
    {
      "epoch": 2.458344714558864,
      "grad_norm": 1.5051771402359009,
      "learning_rate": 9.027739840359344e-06,
      "loss": 1.2372,
      "step": 270000
    },
    {
      "epoch": 2.46289720477101,
      "grad_norm": 1.9544081687927246,
      "learning_rate": 8.951865003490242e-06,
      "loss": 1.2342,
      "step": 270500
    },
    {
      "epoch": 2.4674496949831557,
      "grad_norm": 1.7431451082229614,
      "learning_rate": 8.875990166621142e-06,
      "loss": 1.2417,
      "step": 271000
    },
    {
      "epoch": 2.472002185195302,
      "grad_norm": 1.5809911489486694,
      "learning_rate": 8.80011532975204e-06,
      "loss": 1.2614,
      "step": 271500
    },
    {
      "epoch": 2.476554675407448,
      "grad_norm": 3.475827217102051,
      "learning_rate": 8.72424049288294e-06,
      "loss": 1.2203,
      "step": 272000
    },
    {
      "epoch": 2.4811071656195938,
      "grad_norm": 1.8686953783035278,
      "learning_rate": 8.64836565601384e-06,
      "loss": 1.2611,
      "step": 272500
    },
    {
      "epoch": 2.48565965583174,
      "grad_norm": 2.818176031112671,
      "learning_rate": 8.572490819144739e-06,
      "loss": 1.2529,
      "step": 273000
    },
    {
      "epoch": 2.490212146043886,
      "grad_norm": 2.2766733169555664,
      "learning_rate": 8.496615982275639e-06,
      "loss": 1.2649,
      "step": 273500
    },
    {
      "epoch": 2.494764636256032,
      "grad_norm": 1.527551293373108,
      "learning_rate": 8.420741145406539e-06,
      "loss": 1.2463,
      "step": 274000
    },
    {
      "epoch": 2.499317126468178,
      "grad_norm": 1.7998629808425903,
      "learning_rate": 8.344866308537437e-06,
      "loss": 1.2426,
      "step": 274500
    },
    {
      "epoch": 2.503869616680324,
      "grad_norm": 2.451552152633667,
      "learning_rate": 8.268991471668335e-06,
      "loss": 1.2514,
      "step": 275000
    },
    {
      "epoch": 2.5084221068924704,
      "grad_norm": 1.3440027236938477,
      "learning_rate": 8.193116634799235e-06,
      "loss": 1.234,
      "step": 275500
    },
    {
      "epoch": 2.5129745971046162,
      "grad_norm": 2.0951032638549805,
      "learning_rate": 8.117241797930135e-06,
      "loss": 1.2554,
      "step": 276000
    },
    {
      "epoch": 2.517527087316762,
      "grad_norm": 1.5132488012313843,
      "learning_rate": 8.041366961061033e-06,
      "loss": 1.2194,
      "step": 276500
    },
    {
      "epoch": 2.5220795775289084,
      "grad_norm": 1.8734163045883179,
      "learning_rate": 7.965492124191933e-06,
      "loss": 1.2245,
      "step": 277000
    },
    {
      "epoch": 2.5266320677410543,
      "grad_norm": 2.1283762454986572,
      "learning_rate": 7.889617287322833e-06,
      "loss": 1.2316,
      "step": 277500
    },
    {
      "epoch": 2.5311845579532006,
      "grad_norm": 2.31469464302063,
      "learning_rate": 7.813742450453732e-06,
      "loss": 1.2256,
      "step": 278000
    },
    {
      "epoch": 2.5357370481653465,
      "grad_norm": 3.0647470951080322,
      "learning_rate": 7.737867613584632e-06,
      "loss": 1.2415,
      "step": 278500
    },
    {
      "epoch": 2.5402895383774924,
      "grad_norm": 1.8252134323120117,
      "learning_rate": 7.66199277671553e-06,
      "loss": 1.2367,
      "step": 279000
    },
    {
      "epoch": 2.5448420285896383,
      "grad_norm": 1.7272511720657349,
      "learning_rate": 7.586117939846429e-06,
      "loss": 1.2201,
      "step": 279500
    },
    {
      "epoch": 2.5493945188017846,
      "grad_norm": 1.620742678642273,
      "learning_rate": 7.51024310297733e-06,
      "loss": 1.2504,
      "step": 280000
    },
    {
      "epoch": 2.5539470090139305,
      "grad_norm": 1.7763009071350098,
      "learning_rate": 7.434368266108228e-06,
      "loss": 1.2333,
      "step": 280500
    },
    {
      "epoch": 2.558499499226077,
      "grad_norm": 1.9576882123947144,
      "learning_rate": 7.358493429239127e-06,
      "loss": 1.2073,
      "step": 281000
    },
    {
      "epoch": 2.5630519894382227,
      "grad_norm": 2.318049669265747,
      "learning_rate": 7.282618592370027e-06,
      "loss": 1.2333,
      "step": 281500
    },
    {
      "epoch": 2.5676044796503685,
      "grad_norm": 0.853299617767334,
      "learning_rate": 7.206743755500926e-06,
      "loss": 1.2024,
      "step": 282000
    },
    {
      "epoch": 2.572156969862515,
      "grad_norm": 1.2270612716674805,
      "learning_rate": 7.130868918631825e-06,
      "loss": 1.2355,
      "step": 282500
    },
    {
      "epoch": 2.5767094600746607,
      "grad_norm": 1.7471885681152344,
      "learning_rate": 7.054994081762724e-06,
      "loss": 1.2283,
      "step": 283000
    },
    {
      "epoch": 2.581261950286807,
      "grad_norm": 2.231360912322998,
      "learning_rate": 6.979119244893624e-06,
      "loss": 1.2544,
      "step": 283500
    },
    {
      "epoch": 2.585814440498953,
      "grad_norm": 1.8557158708572388,
      "learning_rate": 6.903244408024523e-06,
      "loss": 1.2292,
      "step": 284000
    },
    {
      "epoch": 2.590366930711099,
      "grad_norm": 2.406499147415161,
      "learning_rate": 6.827369571155422e-06,
      "loss": 1.2373,
      "step": 284500
    },
    {
      "epoch": 2.594919420923245,
      "grad_norm": 1.8454474210739136,
      "learning_rate": 6.751494734286322e-06,
      "loss": 1.2319,
      "step": 285000
    },
    {
      "epoch": 2.599471911135391,
      "grad_norm": 2.0284483432769775,
      "learning_rate": 6.675619897417221e-06,
      "loss": 1.2522,
      "step": 285500
    },
    {
      "epoch": 2.6040244013475373,
      "grad_norm": 2.031736135482788,
      "learning_rate": 6.599745060548119e-06,
      "loss": 1.2367,
      "step": 286000
    },
    {
      "epoch": 2.608576891559683,
      "grad_norm": 1.8436288833618164,
      "learning_rate": 6.52387022367902e-06,
      "loss": 1.2764,
      "step": 286500
    },
    {
      "epoch": 2.613129381771829,
      "grad_norm": 3.0692331790924072,
      "learning_rate": 6.4479953868099184e-06,
      "loss": 1.2689,
      "step": 287000
    },
    {
      "epoch": 2.6176818719839754,
      "grad_norm": 1.8469138145446777,
      "learning_rate": 6.3721205499408175e-06,
      "loss": 1.2421,
      "step": 287500
    },
    {
      "epoch": 2.6222343621961213,
      "grad_norm": 1.8530853986740112,
      "learning_rate": 6.2962457130717175e-06,
      "loss": 1.213,
      "step": 288000
    },
    {
      "epoch": 2.6267868524082676,
      "grad_norm": 2.16693115234375,
      "learning_rate": 6.220370876202617e-06,
      "loss": 1.2374,
      "step": 288500
    },
    {
      "epoch": 2.6313393426204135,
      "grad_norm": 2.2841477394104004,
      "learning_rate": 6.144496039333516e-06,
      "loss": 1.2418,
      "step": 289000
    },
    {
      "epoch": 2.6358918328325593,
      "grad_norm": 2.433706045150757,
      "learning_rate": 6.068621202464415e-06,
      "loss": 1.2386,
      "step": 289500
    },
    {
      "epoch": 2.640444323044705,
      "grad_norm": 3.14551043510437,
      "learning_rate": 5.992746365595314e-06,
      "loss": 1.2346,
      "step": 290000
    },
    {
      "epoch": 2.6449968132568515,
      "grad_norm": 1.8869843482971191,
      "learning_rate": 5.916871528726214e-06,
      "loss": 1.2499,
      "step": 290500
    },
    {
      "epoch": 2.6495493034689974,
      "grad_norm": 2.150926113128662,
      "learning_rate": 5.840996691857112e-06,
      "loss": 1.2547,
      "step": 291000
    },
    {
      "epoch": 2.6541017936811437,
      "grad_norm": 1.9049314260482788,
      "learning_rate": 5.765121854988012e-06,
      "loss": 1.2346,
      "step": 291500
    },
    {
      "epoch": 2.6586542838932896,
      "grad_norm": 1.8521007299423218,
      "learning_rate": 5.689247018118911e-06,
      "loss": 1.2466,
      "step": 292000
    },
    {
      "epoch": 2.6632067741054355,
      "grad_norm": 2.3242249488830566,
      "learning_rate": 5.6133721812498105e-06,
      "loss": 1.2139,
      "step": 292500
    },
    {
      "epoch": 2.667759264317582,
      "grad_norm": 1.8505977392196655,
      "learning_rate": 5.5374973443807096e-06,
      "loss": 1.2275,
      "step": 293000
    },
    {
      "epoch": 2.6723117545297277,
      "grad_norm": 1.4799553155899048,
      "learning_rate": 5.4616225075116095e-06,
      "loss": 1.2148,
      "step": 293500
    },
    {
      "epoch": 2.676864244741874,
      "grad_norm": 1.7316018342971802,
      "learning_rate": 5.385747670642508e-06,
      "loss": 1.2414,
      "step": 294000
    },
    {
      "epoch": 2.68141673495402,
      "grad_norm": 2.3420491218566895,
      "learning_rate": 5.309872833773408e-06,
      "loss": 1.2556,
      "step": 294500
    },
    {
      "epoch": 2.6859692251661658,
      "grad_norm": 1.1340299844741821,
      "learning_rate": 5.233997996904307e-06,
      "loss": 1.2286,
      "step": 295000
    },
    {
      "epoch": 2.690521715378312,
      "grad_norm": 1.3825271129608154,
      "learning_rate": 5.158123160035206e-06,
      "loss": 1.2594,
      "step": 295500
    },
    {
      "epoch": 2.695074205590458,
      "grad_norm": 1.5515590906143188,
      "learning_rate": 5.082248323166105e-06,
      "loss": 1.2307,
      "step": 296000
    },
    {
      "epoch": 2.6996266958026043,
      "grad_norm": 1.2352699041366577,
      "learning_rate": 5.006373486297004e-06,
      "loss": 1.2649,
      "step": 296500
    },
    {
      "epoch": 2.70417918601475,
      "grad_norm": 3.7723472118377686,
      "learning_rate": 4.930498649427904e-06,
      "loss": 1.2281,
      "step": 297000
    },
    {
      "epoch": 2.708731676226896,
      "grad_norm": 3.3017752170562744,
      "learning_rate": 4.854623812558803e-06,
      "loss": 1.2246,
      "step": 297500
    },
    {
      "epoch": 2.713284166439042,
      "grad_norm": 1.8670380115509033,
      "learning_rate": 4.7787489756897025e-06,
      "loss": 1.2241,
      "step": 298000
    },
    {
      "epoch": 2.7178366566511882,
      "grad_norm": 2.034742832183838,
      "learning_rate": 4.702874138820602e-06,
      "loss": 1.2419,
      "step": 298500
    },
    {
      "epoch": 2.722389146863334,
      "grad_norm": 1.4708057641983032,
      "learning_rate": 4.6269993019515016e-06,
      "loss": 1.2403,
      "step": 299000
    },
    {
      "epoch": 2.7269416370754804,
      "grad_norm": 4.326996803283691,
      "learning_rate": 4.5511244650824e-06,
      "loss": 1.2464,
      "step": 299500
    },
    {
      "epoch": 2.7314941272876263,
      "grad_norm": 2.489254951477051,
      "learning_rate": 4.4752496282133e-06,
      "loss": 1.2303,
      "step": 300000
    },
    {
      "epoch": 2.736046617499772,
      "grad_norm": 1.7025161981582642,
      "learning_rate": 4.399374791344199e-06,
      "loss": 1.2234,
      "step": 300500
    },
    {
      "epoch": 2.7405991077119185,
      "grad_norm": 2.2579007148742676,
      "learning_rate": 4.323499954475098e-06,
      "loss": 1.2492,
      "step": 301000
    },
    {
      "epoch": 2.7451515979240644,
      "grad_norm": 3.5407161712646484,
      "learning_rate": 4.247625117605997e-06,
      "loss": 1.2355,
      "step": 301500
    },
    {
      "epoch": 2.7497040881362107,
      "grad_norm": 1.770940899848938,
      "learning_rate": 4.171750280736897e-06,
      "loss": 1.2595,
      "step": 302000
    },
    {
      "epoch": 2.7542565783483566,
      "grad_norm": 2.0011990070343018,
      "learning_rate": 4.095875443867795e-06,
      "loss": 1.2354,
      "step": 302500
    },
    {
      "epoch": 2.7588090685605025,
      "grad_norm": 1.237736701965332,
      "learning_rate": 4.020000606998695e-06,
      "loss": 1.2413,
      "step": 303000
    },
    {
      "epoch": 2.7633615587726488,
      "grad_norm": 2.3934404850006104,
      "learning_rate": 3.9441257701295945e-06,
      "loss": 1.2154,
      "step": 303500
    },
    {
      "epoch": 2.7679140489847947,
      "grad_norm": 1.899156093597412,
      "learning_rate": 3.868250933260494e-06,
      "loss": 1.2692,
      "step": 304000
    },
    {
      "epoch": 2.772466539196941,
      "grad_norm": 4.008796691894531,
      "learning_rate": 3.792376096391393e-06,
      "loss": 1.2206,
      "step": 304500
    },
    {
      "epoch": 2.777019029409087,
      "grad_norm": 1.7999159097671509,
      "learning_rate": 3.716501259522292e-06,
      "loss": 1.2428,
      "step": 305000
    },
    {
      "epoch": 2.7815715196212327,
      "grad_norm": 2.1091625690460205,
      "learning_rate": 3.6406264226531914e-06,
      "loss": 1.2276,
      "step": 305500
    },
    {
      "epoch": 2.7861240098333786,
      "grad_norm": 2.139573335647583,
      "learning_rate": 3.564751585784091e-06,
      "loss": 1.2479,
      "step": 306000
    },
    {
      "epoch": 2.790676500045525,
      "grad_norm": 1.1258076429367065,
      "learning_rate": 3.4888767489149897e-06,
      "loss": 1.2111,
      "step": 306500
    },
    {
      "epoch": 2.795228990257671,
      "grad_norm": 2.968679666519165,
      "learning_rate": 3.413001912045889e-06,
      "loss": 1.2084,
      "step": 307000
    },
    {
      "epoch": 2.799781480469817,
      "grad_norm": 1.2747098207473755,
      "learning_rate": 3.3371270751767887e-06,
      "loss": 1.2205,
      "step": 307500
    },
    {
      "epoch": 2.804333970681963,
      "grad_norm": 1.2769025564193726,
      "learning_rate": 3.2612522383076874e-06,
      "loss": 1.2283,
      "step": 308000
    },
    {
      "epoch": 2.808886460894109,
      "grad_norm": 1.605629324913025,
      "learning_rate": 3.185377401438587e-06,
      "loss": 1.2543,
      "step": 308500
    },
    {
      "epoch": 2.813438951106255,
      "grad_norm": 3.5867137908935547,
      "learning_rate": 3.109502564569486e-06,
      "loss": 1.2404,
      "step": 309000
    },
    {
      "epoch": 2.817991441318401,
      "grad_norm": 1.5547385215759277,
      "learning_rate": 3.0336277277003857e-06,
      "loss": 1.2306,
      "step": 309500
    },
    {
      "epoch": 2.8225439315305474,
      "grad_norm": 2.089463472366333,
      "learning_rate": 2.9577528908312848e-06,
      "loss": 1.2273,
      "step": 310000
    },
    {
      "epoch": 2.8270964217426933,
      "grad_norm": 1.562618613243103,
      "learning_rate": 2.881878053962184e-06,
      "loss": 1.2402,
      "step": 310500
    },
    {
      "epoch": 2.831648911954839,
      "grad_norm": 2.4745259284973145,
      "learning_rate": 2.8060032170930834e-06,
      "loss": 1.2155,
      "step": 311000
    },
    {
      "epoch": 2.8362014021669855,
      "grad_norm": 1.2664399147033691,
      "learning_rate": 2.7301283802239826e-06,
      "loss": 1.2023,
      "step": 311500
    },
    {
      "epoch": 2.8407538923791313,
      "grad_norm": 1.68686842918396,
      "learning_rate": 2.6542535433548817e-06,
      "loss": 1.2416,
      "step": 312000
    },
    {
      "epoch": 2.8453063825912777,
      "grad_norm": 2.0776705741882324,
      "learning_rate": 2.5783787064857812e-06,
      "loss": 1.2236,
      "step": 312500
    },
    {
      "epoch": 2.8498588728034235,
      "grad_norm": 1.6850932836532593,
      "learning_rate": 2.5025038696166804e-06,
      "loss": 1.2211,
      "step": 313000
    },
    {
      "epoch": 2.8544113630155694,
      "grad_norm": 1.514117956161499,
      "learning_rate": 2.42662903274758e-06,
      "loss": 1.2484,
      "step": 313500
    },
    {
      "epoch": 2.8589638532277153,
      "grad_norm": 1.8473858833312988,
      "learning_rate": 2.350754195878479e-06,
      "loss": 1.223,
      "step": 314000
    },
    {
      "epoch": 2.8635163434398616,
      "grad_norm": 2.037574291229248,
      "learning_rate": 2.2748793590093786e-06,
      "loss": 1.252,
      "step": 314500
    },
    {
      "epoch": 2.8680688336520075,
      "grad_norm": 1.6955630779266357,
      "learning_rate": 2.1990045221402777e-06,
      "loss": 1.2557,
      "step": 315000
    },
    {
      "epoch": 2.872621323864154,
      "grad_norm": 1.6658790111541748,
      "learning_rate": 2.123129685271177e-06,
      "loss": 1.2661,
      "step": 315500
    },
    {
      "epoch": 2.8771738140762997,
      "grad_norm": 2.223801612854004,
      "learning_rate": 2.047254848402076e-06,
      "loss": 1.2221,
      "step": 316000
    },
    {
      "epoch": 2.8817263042884456,
      "grad_norm": 2.2594833374023438,
      "learning_rate": 1.9713800115329755e-06,
      "loss": 1.2219,
      "step": 316500
    },
    {
      "epoch": 2.886278794500592,
      "grad_norm": 1.893192172050476,
      "learning_rate": 1.8955051746638746e-06,
      "loss": 1.2446,
      "step": 317000
    },
    {
      "epoch": 2.8908312847127378,
      "grad_norm": 1.854347586631775,
      "learning_rate": 1.8196303377947737e-06,
      "loss": 1.2544,
      "step": 317500
    },
    {
      "epoch": 2.895383774924884,
      "grad_norm": 0.895599365234375,
      "learning_rate": 1.7437555009256733e-06,
      "loss": 1.2318,
      "step": 318000
    },
    {
      "epoch": 2.89993626513703,
      "grad_norm": 3.593600034713745,
      "learning_rate": 1.6678806640565724e-06,
      "loss": 1.2583,
      "step": 318500
    },
    {
      "epoch": 2.904488755349176,
      "grad_norm": 2.038907527923584,
      "learning_rate": 1.5920058271874715e-06,
      "loss": 1.2414,
      "step": 319000
    },
    {
      "epoch": 2.909041245561322,
      "grad_norm": 1.994449257850647,
      "learning_rate": 1.5161309903183708e-06,
      "loss": 1.2366,
      "step": 319500
    },
    {
      "epoch": 2.913593735773468,
      "grad_norm": 1.761293649673462,
      "learning_rate": 1.4402561534492702e-06,
      "loss": 1.2182,
      "step": 320000
    },
    {
      "epoch": 2.9181462259856144,
      "grad_norm": 2.175781011581421,
      "learning_rate": 1.3643813165801693e-06,
      "loss": 1.196,
      "step": 320500
    },
    {
      "epoch": 2.9226987161977602,
      "grad_norm": 2.346984624862671,
      "learning_rate": 1.2885064797110686e-06,
      "loss": 1.2163,
      "step": 321000
    },
    {
      "epoch": 2.927251206409906,
      "grad_norm": 1.5194348096847534,
      "learning_rate": 1.212631642841968e-06,
      "loss": 1.2407,
      "step": 321500
    },
    {
      "epoch": 2.9318036966220524,
      "grad_norm": 3.2156708240509033,
      "learning_rate": 1.1367568059728673e-06,
      "loss": 1.2299,
      "step": 322000
    },
    {
      "epoch": 2.9363561868341983,
      "grad_norm": 1.1396629810333252,
      "learning_rate": 1.0608819691037666e-06,
      "loss": 1.2387,
      "step": 322500
    },
    {
      "epoch": 2.9409086770463446,
      "grad_norm": 1.9050639867782593,
      "learning_rate": 9.850071322346657e-07,
      "loss": 1.2556,
      "step": 323000
    },
    {
      "epoch": 2.9454611672584905,
      "grad_norm": 2.554082155227661,
      "learning_rate": 9.091322953655651e-07,
      "loss": 1.2159,
      "step": 323500
    },
    {
      "epoch": 2.9500136574706364,
      "grad_norm": 1.7566107511520386,
      "learning_rate": 8.332574584964642e-07,
      "loss": 1.1956,
      "step": 324000
    },
    {
      "epoch": 2.9545661476827823,
      "grad_norm": 1.8346569538116455,
      "learning_rate": 7.573826216273635e-07,
      "loss": 1.2246,
      "step": 324500
    },
    {
      "epoch": 2.9591186378949286,
      "grad_norm": 1.6764285564422607,
      "learning_rate": 6.815077847582628e-07,
      "loss": 1.2645,
      "step": 325000
    },
    {
      "epoch": 2.9636711281070744,
      "grad_norm": 1.886643409729004,
      "learning_rate": 6.05632947889162e-07,
      "loss": 1.2157,
      "step": 325500
    },
    {
      "epoch": 2.9682236183192208,
      "grad_norm": 1.586329698562622,
      "learning_rate": 5.297581110200613e-07,
      "loss": 1.2241,
      "step": 326000
    },
    {
      "epoch": 2.9727761085313666,
      "grad_norm": 2.14245343208313,
      "learning_rate": 4.538832741509606e-07,
      "loss": 1.2364,
      "step": 326500
    },
    {
      "epoch": 2.9773285987435125,
      "grad_norm": 1.6875765323638916,
      "learning_rate": 3.780084372818599e-07,
      "loss": 1.2455,
      "step": 327000
    },
    {
      "epoch": 2.981881088955659,
      "grad_norm": 2.1189067363739014,
      "learning_rate": 3.021336004127591e-07,
      "loss": 1.2198,
      "step": 327500
    },
    {
      "epoch": 2.9864335791678047,
      "grad_norm": 1.5863128900527954,
      "learning_rate": 2.262587635436584e-07,
      "loss": 1.2406,
      "step": 328000
    },
    {
      "epoch": 2.990986069379951,
      "grad_norm": 1.9158738851547241,
      "learning_rate": 1.5038392667455766e-07,
      "loss": 1.2179,
      "step": 328500
    },
    {
      "epoch": 2.995538559592097,
      "grad_norm": 1.6223478317260742,
      "learning_rate": 7.450908980545692e-08,
      "loss": 1.2477,
      "step": 329000
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.262010931968689,
      "eval_runtime": 8908.9533,
      "eval_samples_per_second": 6.164,
      "eval_steps_per_second": 0.771,
      "step": 329490
    }
  ],
  "logging_steps": 500,
  "max_steps": 329490,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.304655581184e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
