{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 219660,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004552490212146044,
      "grad_norm": 6.711992263793945,
      "learning_rate": 4.992427691280464e-05,
      "loss": 1.7673,
      "step": 500
    },
    {
      "epoch": 0.009104980424292088,
      "grad_norm": 3.201833963394165,
      "learning_rate": 4.984840207593554e-05,
      "loss": 1.604,
      "step": 1000
    },
    {
      "epoch": 0.013657470636438131,
      "grad_norm": 4.119305610656738,
      "learning_rate": 4.977252723906644e-05,
      "loss": 1.6249,
      "step": 1500
    },
    {
      "epoch": 0.018209960848584176,
      "grad_norm": 3.2692856788635254,
      "learning_rate": 4.9696652402197335e-05,
      "loss": 1.6417,
      "step": 2000
    },
    {
      "epoch": 0.02276245106073022,
      "grad_norm": 2.832024097442627,
      "learning_rate": 4.9620777565328235e-05,
      "loss": 1.6127,
      "step": 2500
    },
    {
      "epoch": 0.027314941272876262,
      "grad_norm": 2.507462501525879,
      "learning_rate": 4.9544902728459135e-05,
      "loss": 1.6134,
      "step": 3000
    },
    {
      "epoch": 0.03186743148502231,
      "grad_norm": 2.6528735160827637,
      "learning_rate": 4.9469027891590035e-05,
      "loss": 1.6213,
      "step": 3500
    },
    {
      "epoch": 0.03641992169716835,
      "grad_norm": 2.410271644592285,
      "learning_rate": 4.9393153054720935e-05,
      "loss": 1.5815,
      "step": 4000
    },
    {
      "epoch": 0.040972411909314395,
      "grad_norm": 2.4228503704071045,
      "learning_rate": 4.9317278217851835e-05,
      "loss": 1.6303,
      "step": 4500
    },
    {
      "epoch": 0.04552490212146044,
      "grad_norm": 2.2473554611206055,
      "learning_rate": 4.924140338098273e-05,
      "loss": 1.5858,
      "step": 5000
    },
    {
      "epoch": 0.05007739233360648,
      "grad_norm": 4.8948822021484375,
      "learning_rate": 4.916552854411363e-05,
      "loss": 1.6002,
      "step": 5500
    },
    {
      "epoch": 0.054629882545752524,
      "grad_norm": 1.545534372329712,
      "learning_rate": 4.908965370724453e-05,
      "loss": 1.5593,
      "step": 6000
    },
    {
      "epoch": 0.05918237275789857,
      "grad_norm": 2.9527928829193115,
      "learning_rate": 4.9013778870375435e-05,
      "loss": 1.5789,
      "step": 6500
    },
    {
      "epoch": 0.06373486297004462,
      "grad_norm": 2.5952768325805664,
      "learning_rate": 4.8937904033506335e-05,
      "loss": 1.6503,
      "step": 7000
    },
    {
      "epoch": 0.06828735318219066,
      "grad_norm": 2.5124309062957764,
      "learning_rate": 4.8862029196637235e-05,
      "loss": 1.5719,
      "step": 7500
    },
    {
      "epoch": 0.0728398433943367,
      "grad_norm": 2.3284144401550293,
      "learning_rate": 4.878615435976813e-05,
      "loss": 1.5241,
      "step": 8000
    },
    {
      "epoch": 0.07739233360648275,
      "grad_norm": 2.2942845821380615,
      "learning_rate": 4.871027952289903e-05,
      "loss": 1.6238,
      "step": 8500
    },
    {
      "epoch": 0.08194482381862879,
      "grad_norm": 1.7935348749160767,
      "learning_rate": 4.863440468602993e-05,
      "loss": 1.614,
      "step": 9000
    },
    {
      "epoch": 0.08649731403077483,
      "grad_norm": 2.6648783683776855,
      "learning_rate": 4.855852984916083e-05,
      "loss": 1.5704,
      "step": 9500
    },
    {
      "epoch": 0.09104980424292088,
      "grad_norm": 1.6302645206451416,
      "learning_rate": 4.848265501229173e-05,
      "loss": 1.5435,
      "step": 10000
    },
    {
      "epoch": 0.09560229445506692,
      "grad_norm": 1.8239576816558838,
      "learning_rate": 4.840678017542262e-05,
      "loss": 1.5374,
      "step": 10500
    },
    {
      "epoch": 0.10015478466721296,
      "grad_norm": 2.4171929359436035,
      "learning_rate": 4.833090533855352e-05,
      "loss": 1.5453,
      "step": 11000
    },
    {
      "epoch": 0.104707274879359,
      "grad_norm": 1.8650712966918945,
      "learning_rate": 4.825503050168442e-05,
      "loss": 1.5283,
      "step": 11500
    },
    {
      "epoch": 0.10925976509150505,
      "grad_norm": 2.2542166709899902,
      "learning_rate": 4.817915566481532e-05,
      "loss": 1.5883,
      "step": 12000
    },
    {
      "epoch": 0.11381225530365109,
      "grad_norm": 1.9423879384994507,
      "learning_rate": 4.810328082794622e-05,
      "loss": 1.5237,
      "step": 12500
    },
    {
      "epoch": 0.11836474551579713,
      "grad_norm": 4.801772117614746,
      "learning_rate": 4.802740599107712e-05,
      "loss": 1.5464,
      "step": 13000
    },
    {
      "epoch": 0.12291723572794319,
      "grad_norm": 1.9088784456253052,
      "learning_rate": 4.795153115420802e-05,
      "loss": 1.4789,
      "step": 13500
    },
    {
      "epoch": 0.12746972594008923,
      "grad_norm": 1.828966736793518,
      "learning_rate": 4.787565631733892e-05,
      "loss": 1.5253,
      "step": 14000
    },
    {
      "epoch": 0.13202221615223528,
      "grad_norm": 1.498440146446228,
      "learning_rate": 4.779978148046982e-05,
      "loss": 1.5235,
      "step": 14500
    },
    {
      "epoch": 0.13657470636438132,
      "grad_norm": 1.2931429147720337,
      "learning_rate": 4.772390664360072e-05,
      "loss": 1.5408,
      "step": 15000
    },
    {
      "epoch": 0.14112719657652736,
      "grad_norm": 1.5772604942321777,
      "learning_rate": 4.764803180673162e-05,
      "loss": 1.5701,
      "step": 15500
    },
    {
      "epoch": 0.1456796867886734,
      "grad_norm": 1.7862439155578613,
      "learning_rate": 4.757215696986252e-05,
      "loss": 1.5597,
      "step": 16000
    },
    {
      "epoch": 0.15023217700081945,
      "grad_norm": 1.632371187210083,
      "learning_rate": 4.7496282132993414e-05,
      "loss": 1.4911,
      "step": 16500
    },
    {
      "epoch": 0.1547846672129655,
      "grad_norm": 2.9482781887054443,
      "learning_rate": 4.7420407296124314e-05,
      "loss": 1.5559,
      "step": 17000
    },
    {
      "epoch": 0.15933715742511154,
      "grad_norm": 1.5640898942947388,
      "learning_rate": 4.7344532459255214e-05,
      "loss": 1.4804,
      "step": 17500
    },
    {
      "epoch": 0.16388964763725758,
      "grad_norm": 2.2412517070770264,
      "learning_rate": 4.7268657622386114e-05,
      "loss": 1.5526,
      "step": 18000
    },
    {
      "epoch": 0.16844213784940362,
      "grad_norm": 2.271923065185547,
      "learning_rate": 4.7192782785517014e-05,
      "loss": 1.5594,
      "step": 18500
    },
    {
      "epoch": 0.17299462806154967,
      "grad_norm": 1.2921818494796753,
      "learning_rate": 4.7116907948647914e-05,
      "loss": 1.5205,
      "step": 19000
    },
    {
      "epoch": 0.1775471182736957,
      "grad_norm": 1.9481315612792969,
      "learning_rate": 4.704103311177881e-05,
      "loss": 1.5361,
      "step": 19500
    },
    {
      "epoch": 0.18209960848584175,
      "grad_norm": 1.6352636814117432,
      "learning_rate": 4.696515827490971e-05,
      "loss": 1.5127,
      "step": 20000
    },
    {
      "epoch": 0.1866520986979878,
      "grad_norm": 1.2566120624542236,
      "learning_rate": 4.688928343804061e-05,
      "loss": 1.4809,
      "step": 20500
    },
    {
      "epoch": 0.19120458891013384,
      "grad_norm": 2.1223177909851074,
      "learning_rate": 4.681340860117151e-05,
      "loss": 1.5116,
      "step": 21000
    },
    {
      "epoch": 0.19575707912227988,
      "grad_norm": 1.8257746696472168,
      "learning_rate": 4.673753376430241e-05,
      "loss": 1.521,
      "step": 21500
    },
    {
      "epoch": 0.20030956933442592,
      "grad_norm": 1.5269874334335327,
      "learning_rate": 4.666165892743331e-05,
      "loss": 1.5207,
      "step": 22000
    },
    {
      "epoch": 0.20486205954657197,
      "grad_norm": 2.1110775470733643,
      "learning_rate": 4.658578409056421e-05,
      "loss": 1.5362,
      "step": 22500
    },
    {
      "epoch": 0.209414549758718,
      "grad_norm": 1.9805322885513306,
      "learning_rate": 4.650990925369511e-05,
      "loss": 1.5154,
      "step": 23000
    },
    {
      "epoch": 0.21396703997086405,
      "grad_norm": 2.283064126968384,
      "learning_rate": 4.643403441682601e-05,
      "loss": 1.4828,
      "step": 23500
    },
    {
      "epoch": 0.2185195301830101,
      "grad_norm": 1.9240810871124268,
      "learning_rate": 4.635815957995691e-05,
      "loss": 1.5956,
      "step": 24000
    },
    {
      "epoch": 0.22307202039515614,
      "grad_norm": 2.7188901901245117,
      "learning_rate": 4.628228474308781e-05,
      "loss": 1.4907,
      "step": 24500
    },
    {
      "epoch": 0.22762451060730218,
      "grad_norm": 1.6730681657791138,
      "learning_rate": 4.620640990621871e-05,
      "loss": 1.5159,
      "step": 25000
    },
    {
      "epoch": 0.23217700081944823,
      "grad_norm": 1.0829359292984009,
      "learning_rate": 4.61305350693496e-05,
      "loss": 1.5416,
      "step": 25500
    },
    {
      "epoch": 0.23672949103159427,
      "grad_norm": 1.9690616130828857,
      "learning_rate": 4.60546602324805e-05,
      "loss": 1.4973,
      "step": 26000
    },
    {
      "epoch": 0.2412819812437403,
      "grad_norm": 1.896836757659912,
      "learning_rate": 4.59787853956114e-05,
      "loss": 1.4919,
      "step": 26500
    },
    {
      "epoch": 0.24583447145588638,
      "grad_norm": 1.737241506576538,
      "learning_rate": 4.59029105587423e-05,
      "loss": 1.5036,
      "step": 27000
    },
    {
      "epoch": 0.2503869616680324,
      "grad_norm": 1.638809084892273,
      "learning_rate": 4.58270357218732e-05,
      "loss": 1.49,
      "step": 27500
    },
    {
      "epoch": 0.25493945188017847,
      "grad_norm": 2.3964622020721436,
      "learning_rate": 4.57511608850041e-05,
      "loss": 1.5257,
      "step": 28000
    },
    {
      "epoch": 0.2594919420923245,
      "grad_norm": 1.6429383754730225,
      "learning_rate": 4.567528604813499e-05,
      "loss": 1.5303,
      "step": 28500
    },
    {
      "epoch": 0.26404443230447056,
      "grad_norm": 1.478065848350525,
      "learning_rate": 4.55994112112659e-05,
      "loss": 1.53,
      "step": 29000
    },
    {
      "epoch": 0.2685969225166166,
      "grad_norm": 2.250810384750366,
      "learning_rate": 4.55235363743968e-05,
      "loss": 1.4999,
      "step": 29500
    },
    {
      "epoch": 0.27314941272876264,
      "grad_norm": 2.3155601024627686,
      "learning_rate": 4.54476615375277e-05,
      "loss": 1.5045,
      "step": 30000
    },
    {
      "epoch": 0.2777019029409087,
      "grad_norm": 1.9406174421310425,
      "learning_rate": 4.53717867006586e-05,
      "loss": 1.4882,
      "step": 30500
    },
    {
      "epoch": 0.28225439315305473,
      "grad_norm": 2.0826804637908936,
      "learning_rate": 4.52959118637895e-05,
      "loss": 1.5268,
      "step": 31000
    },
    {
      "epoch": 0.28680688336520077,
      "grad_norm": 1.9962064027786255,
      "learning_rate": 4.522003702692039e-05,
      "loss": 1.493,
      "step": 31500
    },
    {
      "epoch": 0.2913593735773468,
      "grad_norm": 1.8223729133605957,
      "learning_rate": 4.514416219005129e-05,
      "loss": 1.5031,
      "step": 32000
    },
    {
      "epoch": 0.29591186378949286,
      "grad_norm": 1.6665691137313843,
      "learning_rate": 4.506828735318219e-05,
      "loss": 1.4847,
      "step": 32500
    },
    {
      "epoch": 0.3004643540016389,
      "grad_norm": 1.9174461364746094,
      "learning_rate": 4.499241251631309e-05,
      "loss": 1.5001,
      "step": 33000
    },
    {
      "epoch": 0.30501684421378494,
      "grad_norm": 2.1745951175689697,
      "learning_rate": 4.491653767944399e-05,
      "loss": 1.4959,
      "step": 33500
    },
    {
      "epoch": 0.309569334425931,
      "grad_norm": 1.8195202350616455,
      "learning_rate": 4.484066284257489e-05,
      "loss": 1.4538,
      "step": 34000
    },
    {
      "epoch": 0.31412182463807703,
      "grad_norm": 3.1092007160186768,
      "learning_rate": 4.4764788005705786e-05,
      "loss": 1.4918,
      "step": 34500
    },
    {
      "epoch": 0.3186743148502231,
      "grad_norm": 1.930917739868164,
      "learning_rate": 4.4688913168836686e-05,
      "loss": 1.4934,
      "step": 35000
    },
    {
      "epoch": 0.3232268050623691,
      "grad_norm": 2.1174633502960205,
      "learning_rate": 4.4613038331967586e-05,
      "loss": 1.4888,
      "step": 35500
    },
    {
      "epoch": 0.32777929527451516,
      "grad_norm": 1.5831621885299683,
      "learning_rate": 4.4537163495098486e-05,
      "loss": 1.4966,
      "step": 36000
    },
    {
      "epoch": 0.3323317854866612,
      "grad_norm": 1.9858957529067993,
      "learning_rate": 4.4461288658229386e-05,
      "loss": 1.4325,
      "step": 36500
    },
    {
      "epoch": 0.33688427569880725,
      "grad_norm": 2.271111249923706,
      "learning_rate": 4.4385413821360286e-05,
      "loss": 1.4972,
      "step": 37000
    },
    {
      "epoch": 0.3414367659109533,
      "grad_norm": 1.579291820526123,
      "learning_rate": 4.4309538984491186e-05,
      "loss": 1.4988,
      "step": 37500
    },
    {
      "epoch": 0.34598925612309933,
      "grad_norm": 1.7411953210830688,
      "learning_rate": 4.4233664147622086e-05,
      "loss": 1.4576,
      "step": 38000
    },
    {
      "epoch": 0.3505417463352454,
      "grad_norm": 1.0848557949066162,
      "learning_rate": 4.4157789310752986e-05,
      "loss": 1.5001,
      "step": 38500
    },
    {
      "epoch": 0.3550942365473914,
      "grad_norm": 1.7193750143051147,
      "learning_rate": 4.4081914473883886e-05,
      "loss": 1.5135,
      "step": 39000
    },
    {
      "epoch": 0.35964672675953746,
      "grad_norm": 2.076620578765869,
      "learning_rate": 4.4006039637014786e-05,
      "loss": 1.4608,
      "step": 39500
    },
    {
      "epoch": 0.3641992169716835,
      "grad_norm": 1.7793735265731812,
      "learning_rate": 4.3930164800145686e-05,
      "loss": 1.4793,
      "step": 40000
    },
    {
      "epoch": 0.36875170718382955,
      "grad_norm": 1.4366289377212524,
      "learning_rate": 4.385428996327658e-05,
      "loss": 1.4905,
      "step": 40500
    },
    {
      "epoch": 0.3733041973959756,
      "grad_norm": 1.3538599014282227,
      "learning_rate": 4.377841512640748e-05,
      "loss": 1.4519,
      "step": 41000
    },
    {
      "epoch": 0.37785668760812163,
      "grad_norm": 2.698352575302124,
      "learning_rate": 4.370254028953838e-05,
      "loss": 1.5012,
      "step": 41500
    },
    {
      "epoch": 0.3824091778202677,
      "grad_norm": 1.4739645719528198,
      "learning_rate": 4.362666545266928e-05,
      "loss": 1.4796,
      "step": 42000
    },
    {
      "epoch": 0.3869616680324137,
      "grad_norm": 1.7406970262527466,
      "learning_rate": 4.355079061580018e-05,
      "loss": 1.4594,
      "step": 42500
    },
    {
      "epoch": 0.39151415824455976,
      "grad_norm": 1.3127927780151367,
      "learning_rate": 4.347491577893108e-05,
      "loss": 1.4767,
      "step": 43000
    },
    {
      "epoch": 0.3960666484567058,
      "grad_norm": 1.3746943473815918,
      "learning_rate": 4.339904094206197e-05,
      "loss": 1.4544,
      "step": 43500
    },
    {
      "epoch": 0.40061913866885185,
      "grad_norm": 2.2249526977539062,
      "learning_rate": 4.332316610519287e-05,
      "loss": 1.4708,
      "step": 44000
    },
    {
      "epoch": 0.4051716288809979,
      "grad_norm": 2.327796459197998,
      "learning_rate": 4.324729126832378e-05,
      "loss": 1.4801,
      "step": 44500
    },
    {
      "epoch": 0.40972411909314393,
      "grad_norm": 1.5917024612426758,
      "learning_rate": 4.317141643145468e-05,
      "loss": 1.443,
      "step": 45000
    },
    {
      "epoch": 0.41427660930529,
      "grad_norm": 1.9180450439453125,
      "learning_rate": 4.309554159458558e-05,
      "loss": 1.4992,
      "step": 45500
    },
    {
      "epoch": 0.418829099517436,
      "grad_norm": 1.876892328262329,
      "learning_rate": 4.301966675771647e-05,
      "loss": 1.4763,
      "step": 46000
    },
    {
      "epoch": 0.42338158972958206,
      "grad_norm": 2.57507586479187,
      "learning_rate": 4.294379192084737e-05,
      "loss": 1.4715,
      "step": 46500
    },
    {
      "epoch": 0.4279340799417281,
      "grad_norm": 1.6160082817077637,
      "learning_rate": 4.286791708397827e-05,
      "loss": 1.4571,
      "step": 47000
    },
    {
      "epoch": 0.43248657015387415,
      "grad_norm": 2.3625729084014893,
      "learning_rate": 4.279204224710917e-05,
      "loss": 1.472,
      "step": 47500
    },
    {
      "epoch": 0.4370390603660202,
      "grad_norm": 1.4745906591415405,
      "learning_rate": 4.271616741024007e-05,
      "loss": 1.4787,
      "step": 48000
    },
    {
      "epoch": 0.44159155057816624,
      "grad_norm": 1.8253474235534668,
      "learning_rate": 4.264029257337097e-05,
      "loss": 1.4408,
      "step": 48500
    },
    {
      "epoch": 0.4461440407903123,
      "grad_norm": 1.981594204902649,
      "learning_rate": 4.2564417736501865e-05,
      "loss": 1.4827,
      "step": 49000
    },
    {
      "epoch": 0.4506965310024583,
      "grad_norm": 2.4945731163024902,
      "learning_rate": 4.2488542899632765e-05,
      "loss": 1.4422,
      "step": 49500
    },
    {
      "epoch": 0.45524902121460437,
      "grad_norm": 1.399931788444519,
      "learning_rate": 4.2412668062763665e-05,
      "loss": 1.4517,
      "step": 50000
    },
    {
      "epoch": 0.4598015114267504,
      "grad_norm": 1.7381045818328857,
      "learning_rate": 4.2336793225894565e-05,
      "loss": 1.4792,
      "step": 50500
    },
    {
      "epoch": 0.46435400163889645,
      "grad_norm": 1.992401123046875,
      "learning_rate": 4.2260918389025465e-05,
      "loss": 1.4845,
      "step": 51000
    },
    {
      "epoch": 0.4689064918510425,
      "grad_norm": 2.4219253063201904,
      "learning_rate": 4.2185043552156365e-05,
      "loss": 1.4687,
      "step": 51500
    },
    {
      "epoch": 0.47345898206318854,
      "grad_norm": 1.3863270282745361,
      "learning_rate": 4.2109168715287265e-05,
      "loss": 1.4278,
      "step": 52000
    },
    {
      "epoch": 0.4780114722753346,
      "grad_norm": 2.184089422225952,
      "learning_rate": 4.2033293878418164e-05,
      "loss": 1.4607,
      "step": 52500
    },
    {
      "epoch": 0.4825639624874806,
      "grad_norm": 1.573323369026184,
      "learning_rate": 4.1957419041549064e-05,
      "loss": 1.4307,
      "step": 53000
    },
    {
      "epoch": 0.4871164526996267,
      "grad_norm": 1.3278053998947144,
      "learning_rate": 4.1881544204679964e-05,
      "loss": 1.4801,
      "step": 53500
    },
    {
      "epoch": 0.49166894291177277,
      "grad_norm": 1.918517827987671,
      "learning_rate": 4.1805669367810864e-05,
      "loss": 1.4652,
      "step": 54000
    },
    {
      "epoch": 0.4962214331239188,
      "grad_norm": 1.6713398694992065,
      "learning_rate": 4.1729794530941764e-05,
      "loss": 1.457,
      "step": 54500
    },
    {
      "epoch": 0.5007739233360649,
      "grad_norm": 1.9636454582214355,
      "learning_rate": 4.165391969407266e-05,
      "loss": 1.4802,
      "step": 55000
    },
    {
      "epoch": 0.5053264135482108,
      "grad_norm": 2.2944159507751465,
      "learning_rate": 4.157804485720356e-05,
      "loss": 1.4471,
      "step": 55500
    },
    {
      "epoch": 0.5098789037603569,
      "grad_norm": 1.6703966856002808,
      "learning_rate": 4.150217002033446e-05,
      "loss": 1.4511,
      "step": 56000
    },
    {
      "epoch": 0.5144313939725029,
      "grad_norm": 1.5112354755401611,
      "learning_rate": 4.142629518346536e-05,
      "loss": 1.4584,
      "step": 56500
    },
    {
      "epoch": 0.518983884184649,
      "grad_norm": 3.5930309295654297,
      "learning_rate": 4.135042034659626e-05,
      "loss": 1.4827,
      "step": 57000
    },
    {
      "epoch": 0.523536374396795,
      "grad_norm": 1.7980707883834839,
      "learning_rate": 4.127454550972716e-05,
      "loss": 1.4357,
      "step": 57500
    },
    {
      "epoch": 0.5280888646089411,
      "grad_norm": 1.4820793867111206,
      "learning_rate": 4.119867067285805e-05,
      "loss": 1.4636,
      "step": 58000
    },
    {
      "epoch": 0.5326413548210871,
      "grad_norm": 1.6004084348678589,
      "learning_rate": 4.112279583598895e-05,
      "loss": 1.4616,
      "step": 58500
    },
    {
      "epoch": 0.5371938450332332,
      "grad_norm": 1.527901530265808,
      "learning_rate": 4.104692099911985e-05,
      "loss": 1.4212,
      "step": 59000
    },
    {
      "epoch": 0.5417463352453792,
      "grad_norm": 2.45041561126709,
      "learning_rate": 4.097104616225075e-05,
      "loss": 1.4282,
      "step": 59500
    },
    {
      "epoch": 0.5462988254575253,
      "grad_norm": 1.7481502294540405,
      "learning_rate": 4.089517132538165e-05,
      "loss": 1.4231,
      "step": 60000
    },
    {
      "epoch": 0.5508513156696713,
      "grad_norm": 1.3719326257705688,
      "learning_rate": 4.081929648851256e-05,
      "loss": 1.4335,
      "step": 60500
    },
    {
      "epoch": 0.5554038058818174,
      "grad_norm": 1.3403795957565308,
      "learning_rate": 4.074342165164345e-05,
      "loss": 1.4605,
      "step": 61000
    },
    {
      "epoch": 0.5599562960939634,
      "grad_norm": 1.7062550783157349,
      "learning_rate": 4.066754681477435e-05,
      "loss": 1.4654,
      "step": 61500
    },
    {
      "epoch": 0.5645087863061095,
      "grad_norm": 3.0322699546813965,
      "learning_rate": 4.059167197790525e-05,
      "loss": 1.4563,
      "step": 62000
    },
    {
      "epoch": 0.5690612765182554,
      "grad_norm": 1.335919976234436,
      "learning_rate": 4.051579714103615e-05,
      "loss": 1.4979,
      "step": 62500
    },
    {
      "epoch": 0.5736137667304015,
      "grad_norm": 1.6601481437683105,
      "learning_rate": 4.043992230416705e-05,
      "loss": 1.4213,
      "step": 63000
    },
    {
      "epoch": 0.5781662569425475,
      "grad_norm": 1.551808476448059,
      "learning_rate": 4.036404746729795e-05,
      "loss": 1.4586,
      "step": 63500
    },
    {
      "epoch": 0.5827187471546936,
      "grad_norm": 0.9790830016136169,
      "learning_rate": 4.0288172630428843e-05,
      "loss": 1.4297,
      "step": 64000
    },
    {
      "epoch": 0.5872712373668396,
      "grad_norm": 1.507020115852356,
      "learning_rate": 4.0212297793559743e-05,
      "loss": 1.408,
      "step": 64500
    },
    {
      "epoch": 0.5918237275789857,
      "grad_norm": 0.5328876972198486,
      "learning_rate": 4.013642295669064e-05,
      "loss": 1.3959,
      "step": 65000
    },
    {
      "epoch": 0.5963762177911317,
      "grad_norm": 1.2166988849639893,
      "learning_rate": 4.006054811982154e-05,
      "loss": 1.4699,
      "step": 65500
    },
    {
      "epoch": 0.6009287080032778,
      "grad_norm": 2.0083260536193848,
      "learning_rate": 3.998467328295244e-05,
      "loss": 1.4231,
      "step": 66000
    },
    {
      "epoch": 0.6054811982154238,
      "grad_norm": 1.8845000267028809,
      "learning_rate": 3.990879844608334e-05,
      "loss": 1.4701,
      "step": 66500
    },
    {
      "epoch": 0.6100336884275699,
      "grad_norm": 1.0522596836090088,
      "learning_rate": 3.983292360921424e-05,
      "loss": 1.4649,
      "step": 67000
    },
    {
      "epoch": 0.6145861786397159,
      "grad_norm": 1.7644633054733276,
      "learning_rate": 3.975704877234514e-05,
      "loss": 1.4227,
      "step": 67500
    },
    {
      "epoch": 0.619138668851862,
      "grad_norm": 1.6478123664855957,
      "learning_rate": 3.968117393547604e-05,
      "loss": 1.4473,
      "step": 68000
    },
    {
      "epoch": 0.623691159064008,
      "grad_norm": 1.859995722770691,
      "learning_rate": 3.960529909860694e-05,
      "loss": 1.4432,
      "step": 68500
    },
    {
      "epoch": 0.6282436492761541,
      "grad_norm": 2.6457579135894775,
      "learning_rate": 3.952942426173784e-05,
      "loss": 1.4415,
      "step": 69000
    },
    {
      "epoch": 0.6327961394883,
      "grad_norm": 1.523418664932251,
      "learning_rate": 3.945354942486874e-05,
      "loss": 1.444,
      "step": 69500
    },
    {
      "epoch": 0.6373486297004461,
      "grad_norm": 1.3639545440673828,
      "learning_rate": 3.9377674587999636e-05,
      "loss": 1.461,
      "step": 70000
    },
    {
      "epoch": 0.6419011199125921,
      "grad_norm": 1.5062317848205566,
      "learning_rate": 3.9301799751130536e-05,
      "loss": 1.4501,
      "step": 70500
    },
    {
      "epoch": 0.6464536101247382,
      "grad_norm": 1.4950380325317383,
      "learning_rate": 3.9225924914261436e-05,
      "loss": 1.4345,
      "step": 71000
    },
    {
      "epoch": 0.6510061003368843,
      "grad_norm": 1.5224130153656006,
      "learning_rate": 3.9150050077392336e-05,
      "loss": 1.4422,
      "step": 71500
    },
    {
      "epoch": 0.6555585905490303,
      "grad_norm": 1.925525426864624,
      "learning_rate": 3.9074175240523236e-05,
      "loss": 1.427,
      "step": 72000
    },
    {
      "epoch": 0.6601110807611764,
      "grad_norm": 2.675509214401245,
      "learning_rate": 3.8998300403654136e-05,
      "loss": 1.4316,
      "step": 72500
    },
    {
      "epoch": 0.6646635709733224,
      "grad_norm": 1.532267451286316,
      "learning_rate": 3.892242556678503e-05,
      "loss": 1.4292,
      "step": 73000
    },
    {
      "epoch": 0.6692160611854685,
      "grad_norm": 7.720393180847168,
      "learning_rate": 3.884655072991593e-05,
      "loss": 1.4589,
      "step": 73500
    },
    {
      "epoch": 0.6737685513976145,
      "grad_norm": 1.5860896110534668,
      "learning_rate": 3.877067589304683e-05,
      "loss": 1.4481,
      "step": 74000
    },
    {
      "epoch": 0.6783210416097606,
      "grad_norm": 1.8452571630477905,
      "learning_rate": 3.869480105617773e-05,
      "loss": 1.4009,
      "step": 74500
    },
    {
      "epoch": 0.6828735318219066,
      "grad_norm": 1.3081570863723755,
      "learning_rate": 3.861892621930863e-05,
      "loss": 1.4423,
      "step": 75000
    },
    {
      "epoch": 0.6874260220340527,
      "grad_norm": 1.3529257774353027,
      "learning_rate": 3.854305138243953e-05,
      "loss": 1.4351,
      "step": 75500
    },
    {
      "epoch": 0.6919785122461987,
      "grad_norm": 2.912944793701172,
      "learning_rate": 3.846717654557043e-05,
      "loss": 1.4157,
      "step": 76000
    },
    {
      "epoch": 0.6965310024583448,
      "grad_norm": 1.7121022939682007,
      "learning_rate": 3.839130170870133e-05,
      "loss": 1.4668,
      "step": 76500
    },
    {
      "epoch": 0.7010834926704907,
      "grad_norm": 1.5933057069778442,
      "learning_rate": 3.831542687183223e-05,
      "loss": 1.4394,
      "step": 77000
    },
    {
      "epoch": 0.7056359828826368,
      "grad_norm": 2.3423147201538086,
      "learning_rate": 3.823955203496313e-05,
      "loss": 1.419,
      "step": 77500
    },
    {
      "epoch": 0.7101884730947828,
      "grad_norm": 2.3738741874694824,
      "learning_rate": 3.816367719809403e-05,
      "loss": 1.432,
      "step": 78000
    },
    {
      "epoch": 0.7147409633069289,
      "grad_norm": 2.617544412612915,
      "learning_rate": 3.808780236122493e-05,
      "loss": 1.4025,
      "step": 78500
    },
    {
      "epoch": 0.7192934535190749,
      "grad_norm": 1.015317678451538,
      "learning_rate": 3.801192752435582e-05,
      "loss": 1.3974,
      "step": 79000
    },
    {
      "epoch": 0.723845943731221,
      "grad_norm": 1.7523547410964966,
      "learning_rate": 3.793605268748672e-05,
      "loss": 1.4314,
      "step": 79500
    },
    {
      "epoch": 0.728398433943367,
      "grad_norm": 1.621612787246704,
      "learning_rate": 3.786017785061762e-05,
      "loss": 1.4117,
      "step": 80000
    },
    {
      "epoch": 0.7329509241555131,
      "grad_norm": 1.675252079963684,
      "learning_rate": 3.778430301374852e-05,
      "loss": 1.4071,
      "step": 80500
    },
    {
      "epoch": 0.7375034143676591,
      "grad_norm": 2.218848705291748,
      "learning_rate": 3.770842817687942e-05,
      "loss": 1.3938,
      "step": 81000
    },
    {
      "epoch": 0.7420559045798052,
      "grad_norm": 1.0135998725891113,
      "learning_rate": 3.7632553340010315e-05,
      "loss": 1.4161,
      "step": 81500
    },
    {
      "epoch": 0.7466083947919512,
      "grad_norm": 2.363743305206299,
      "learning_rate": 3.7556678503141215e-05,
      "loss": 1.4065,
      "step": 82000
    },
    {
      "epoch": 0.7511608850040973,
      "grad_norm": 1.4600533246994019,
      "learning_rate": 3.7480803666272115e-05,
      "loss": 1.4127,
      "step": 82500
    },
    {
      "epoch": 0.7557133752162433,
      "grad_norm": 1.851562261581421,
      "learning_rate": 3.740492882940302e-05,
      "loss": 1.4511,
      "step": 83000
    },
    {
      "epoch": 0.7602658654283894,
      "grad_norm": 1.828016757965088,
      "learning_rate": 3.732905399253392e-05,
      "loss": 1.4015,
      "step": 83500
    },
    {
      "epoch": 0.7648183556405354,
      "grad_norm": 1.836269497871399,
      "learning_rate": 3.725317915566482e-05,
      "loss": 1.4201,
      "step": 84000
    },
    {
      "epoch": 0.7693708458526815,
      "grad_norm": 1.6285814046859741,
      "learning_rate": 3.7177304318795715e-05,
      "loss": 1.4117,
      "step": 84500
    },
    {
      "epoch": 0.7739233360648274,
      "grad_norm": 1.1409574747085571,
      "learning_rate": 3.7101429481926615e-05,
      "loss": 1.3832,
      "step": 85000
    },
    {
      "epoch": 0.7784758262769735,
      "grad_norm": 1.3484891653060913,
      "learning_rate": 3.7025554645057515e-05,
      "loss": 1.4233,
      "step": 85500
    },
    {
      "epoch": 0.7830283164891195,
      "grad_norm": 2.121706485748291,
      "learning_rate": 3.6949679808188415e-05,
      "loss": 1.402,
      "step": 86000
    },
    {
      "epoch": 0.7875808067012656,
      "grad_norm": 2.5278890132904053,
      "learning_rate": 3.6873804971319315e-05,
      "loss": 1.4242,
      "step": 86500
    },
    {
      "epoch": 0.7921332969134116,
      "grad_norm": 1.5969198942184448,
      "learning_rate": 3.6797930134450215e-05,
      "loss": 1.4048,
      "step": 87000
    },
    {
      "epoch": 0.7966857871255577,
      "grad_norm": 1.7309540510177612,
      "learning_rate": 3.672205529758111e-05,
      "loss": 1.4184,
      "step": 87500
    },
    {
      "epoch": 0.8012382773377037,
      "grad_norm": 3.2090744972229004,
      "learning_rate": 3.664618046071201e-05,
      "loss": 1.4265,
      "step": 88000
    },
    {
      "epoch": 0.8057907675498498,
      "grad_norm": 1.9063198566436768,
      "learning_rate": 3.657030562384291e-05,
      "loss": 1.4297,
      "step": 88500
    },
    {
      "epoch": 0.8103432577619958,
      "grad_norm": 1.5400705337524414,
      "learning_rate": 3.649443078697381e-05,
      "loss": 1.4528,
      "step": 89000
    },
    {
      "epoch": 0.8148957479741419,
      "grad_norm": 1.7730275392532349,
      "learning_rate": 3.641855595010471e-05,
      "loss": 1.4142,
      "step": 89500
    },
    {
      "epoch": 0.8194482381862879,
      "grad_norm": 2.5325891971588135,
      "learning_rate": 3.634268111323561e-05,
      "loss": 1.4327,
      "step": 90000
    },
    {
      "epoch": 0.824000728398434,
      "grad_norm": 2.34662127494812,
      "learning_rate": 3.626680627636651e-05,
      "loss": 1.4232,
      "step": 90500
    },
    {
      "epoch": 0.82855321861058,
      "grad_norm": 1.2610607147216797,
      "learning_rate": 3.619093143949741e-05,
      "loss": 1.4128,
      "step": 91000
    },
    {
      "epoch": 0.833105708822726,
      "grad_norm": 1.5490508079528809,
      "learning_rate": 3.611505660262831e-05,
      "loss": 1.3912,
      "step": 91500
    },
    {
      "epoch": 0.837658199034872,
      "grad_norm": 1.4846287965774536,
      "learning_rate": 3.603918176575921e-05,
      "loss": 1.4253,
      "step": 92000
    },
    {
      "epoch": 0.8422106892470181,
      "grad_norm": 1.5123846530914307,
      "learning_rate": 3.596330692889011e-05,
      "loss": 1.4288,
      "step": 92500
    },
    {
      "epoch": 0.8467631794591641,
      "grad_norm": 2.869833469390869,
      "learning_rate": 3.588743209202101e-05,
      "loss": 1.4053,
      "step": 93000
    },
    {
      "epoch": 0.8513156696713102,
      "grad_norm": 1.3906861543655396,
      "learning_rate": 3.58115572551519e-05,
      "loss": 1.433,
      "step": 93500
    },
    {
      "epoch": 0.8558681598834562,
      "grad_norm": 1.0686962604522705,
      "learning_rate": 3.57356824182828e-05,
      "loss": 1.3779,
      "step": 94000
    },
    {
      "epoch": 0.8604206500956023,
      "grad_norm": 1.898055911064148,
      "learning_rate": 3.56598075814137e-05,
      "loss": 1.3888,
      "step": 94500
    },
    {
      "epoch": 0.8649731403077483,
      "grad_norm": 3.5353546142578125,
      "learning_rate": 3.55839327445446e-05,
      "loss": 1.3903,
      "step": 95000
    },
    {
      "epoch": 0.8695256305198944,
      "grad_norm": 1.470703125,
      "learning_rate": 3.55080579076755e-05,
      "loss": 1.3758,
      "step": 95500
    },
    {
      "epoch": 0.8740781207320404,
      "grad_norm": 1.4897403717041016,
      "learning_rate": 3.54321830708064e-05,
      "loss": 1.4264,
      "step": 96000
    },
    {
      "epoch": 0.8786306109441865,
      "grad_norm": 1.631150245666504,
      "learning_rate": 3.5356308233937294e-05,
      "loss": 1.3956,
      "step": 96500
    },
    {
      "epoch": 0.8831831011563325,
      "grad_norm": 1.4428099393844604,
      "learning_rate": 3.5280433397068194e-05,
      "loss": 1.4073,
      "step": 97000
    },
    {
      "epoch": 0.8877355913684786,
      "grad_norm": 1.879005789756775,
      "learning_rate": 3.5204558560199094e-05,
      "loss": 1.4133,
      "step": 97500
    },
    {
      "epoch": 0.8922880815806246,
      "grad_norm": 1.7584060430526733,
      "learning_rate": 3.5128683723329994e-05,
      "loss": 1.4188,
      "step": 98000
    },
    {
      "epoch": 0.8968405717927707,
      "grad_norm": 1.5144892930984497,
      "learning_rate": 3.50528088864609e-05,
      "loss": 1.4072,
      "step": 98500
    },
    {
      "epoch": 0.9013930620049166,
      "grad_norm": 1.5361531972885132,
      "learning_rate": 3.49769340495918e-05,
      "loss": 1.421,
      "step": 99000
    },
    {
      "epoch": 0.9059455522170627,
      "grad_norm": 1.3133127689361572,
      "learning_rate": 3.4901059212722694e-05,
      "loss": 1.4169,
      "step": 99500
    },
    {
      "epoch": 0.9104980424292087,
      "grad_norm": 1.8255308866500854,
      "learning_rate": 3.4825184375853594e-05,
      "loss": 1.3832,
      "step": 100000
    },
    {
      "epoch": 0.9150505326413548,
      "grad_norm": 1.765775203704834,
      "learning_rate": 3.4749309538984494e-05,
      "loss": 1.3827,
      "step": 100500
    },
    {
      "epoch": 0.9196030228535008,
      "grad_norm": 3.2547006607055664,
      "learning_rate": 3.4673434702115394e-05,
      "loss": 1.4011,
      "step": 101000
    },
    {
      "epoch": 0.9241555130656469,
      "grad_norm": 2.5694596767425537,
      "learning_rate": 3.4597559865246294e-05,
      "loss": 1.3868,
      "step": 101500
    },
    {
      "epoch": 0.9287080032777929,
      "grad_norm": 1.8394837379455566,
      "learning_rate": 3.4521685028377194e-05,
      "loss": 1.4079,
      "step": 102000
    },
    {
      "epoch": 0.933260493489939,
      "grad_norm": 1.544235348701477,
      "learning_rate": 3.444581019150809e-05,
      "loss": 1.4154,
      "step": 102500
    },
    {
      "epoch": 0.937812983702085,
      "grad_norm": 2.3242204189300537,
      "learning_rate": 3.436993535463899e-05,
      "loss": 1.4145,
      "step": 103000
    },
    {
      "epoch": 0.9423654739142311,
      "grad_norm": 1.4065098762512207,
      "learning_rate": 3.429406051776989e-05,
      "loss": 1.3852,
      "step": 103500
    },
    {
      "epoch": 0.9469179641263771,
      "grad_norm": 1.7728382349014282,
      "learning_rate": 3.421818568090079e-05,
      "loss": 1.4082,
      "step": 104000
    },
    {
      "epoch": 0.9514704543385232,
      "grad_norm": 1.1972371339797974,
      "learning_rate": 3.414231084403169e-05,
      "loss": 1.3863,
      "step": 104500
    },
    {
      "epoch": 0.9560229445506692,
      "grad_norm": 1.4880298376083374,
      "learning_rate": 3.406643600716259e-05,
      "loss": 1.3803,
      "step": 105000
    },
    {
      "epoch": 0.9605754347628153,
      "grad_norm": 1.973327875137329,
      "learning_rate": 3.3990561170293487e-05,
      "loss": 1.3844,
      "step": 105500
    },
    {
      "epoch": 0.9651279249749612,
      "grad_norm": 3.1504766941070557,
      "learning_rate": 3.3914686333424387e-05,
      "loss": 1.3887,
      "step": 106000
    },
    {
      "epoch": 0.9696804151871073,
      "grad_norm": 1.6976909637451172,
      "learning_rate": 3.3838811496555287e-05,
      "loss": 1.4344,
      "step": 106500
    },
    {
      "epoch": 0.9742329053992534,
      "grad_norm": 1.5262069702148438,
      "learning_rate": 3.3762936659686186e-05,
      "loss": 1.4188,
      "step": 107000
    },
    {
      "epoch": 0.9787853956113994,
      "grad_norm": 1.7988815307617188,
      "learning_rate": 3.3687061822817086e-05,
      "loss": 1.401,
      "step": 107500
    },
    {
      "epoch": 0.9833378858235455,
      "grad_norm": 2.4446630477905273,
      "learning_rate": 3.3611186985947986e-05,
      "loss": 1.3752,
      "step": 108000
    },
    {
      "epoch": 0.9878903760356915,
      "grad_norm": 1.919823169708252,
      "learning_rate": 3.353531214907888e-05,
      "loss": 1.3984,
      "step": 108500
    },
    {
      "epoch": 0.9924428662478376,
      "grad_norm": 1.7250622510910034,
      "learning_rate": 3.345943731220978e-05,
      "loss": 1.4187,
      "step": 109000
    },
    {
      "epoch": 0.9969953564599836,
      "grad_norm": 1.3520405292510986,
      "learning_rate": 3.338356247534068e-05,
      "loss": 1.3928,
      "step": 109500
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.3585346937179565,
      "eval_runtime": 8752.7613,
      "eval_samples_per_second": 6.274,
      "eval_steps_per_second": 0.784,
      "step": 109830
    },
    {
      "epoch": 1.0015478466721297,
      "grad_norm": 1.0346589088439941,
      "learning_rate": 3.330768763847158e-05,
      "loss": 1.3872,
      "step": 110000
    },
    {
      "epoch": 1.0061003368842758,
      "grad_norm": 1.7906439304351807,
      "learning_rate": 3.323181280160248e-05,
      "loss": 1.3401,
      "step": 110500
    },
    {
      "epoch": 1.0106528270964217,
      "grad_norm": 1.88340163230896,
      "learning_rate": 3.315593796473338e-05,
      "loss": 1.3424,
      "step": 111000
    },
    {
      "epoch": 1.0152053173085678,
      "grad_norm": 1.7193864583969116,
      "learning_rate": 3.308006312786427e-05,
      "loss": 1.3526,
      "step": 111500
    },
    {
      "epoch": 1.0197578075207139,
      "grad_norm": 2.7961347103118896,
      "learning_rate": 3.300418829099517e-05,
      "loss": 1.3315,
      "step": 112000
    },
    {
      "epoch": 1.02431029773286,
      "grad_norm": 1.736942172050476,
      "learning_rate": 3.292831345412607e-05,
      "loss": 1.3785,
      "step": 112500
    },
    {
      "epoch": 1.0288627879450059,
      "grad_norm": 1.5586538314819336,
      "learning_rate": 3.285243861725697e-05,
      "loss": 1.3552,
      "step": 113000
    },
    {
      "epoch": 1.033415278157152,
      "grad_norm": 2.3295154571533203,
      "learning_rate": 3.277656378038787e-05,
      "loss": 1.3586,
      "step": 113500
    },
    {
      "epoch": 1.037967768369298,
      "grad_norm": 2.3068604469299316,
      "learning_rate": 3.270068894351877e-05,
      "loss": 1.372,
      "step": 114000
    },
    {
      "epoch": 1.0425202585814441,
      "grad_norm": 1.605578064918518,
      "learning_rate": 3.262481410664967e-05,
      "loss": 1.374,
      "step": 114500
    },
    {
      "epoch": 1.04707274879359,
      "grad_norm": 1.8648048639297485,
      "learning_rate": 3.254893926978057e-05,
      "loss": 1.3476,
      "step": 115000
    },
    {
      "epoch": 1.0516252390057361,
      "grad_norm": 1.8500488996505737,
      "learning_rate": 3.247306443291147e-05,
      "loss": 1.2911,
      "step": 115500
    },
    {
      "epoch": 1.0561777292178822,
      "grad_norm": 1.9887126684188843,
      "learning_rate": 3.239718959604237e-05,
      "loss": 1.3213,
      "step": 116000
    },
    {
      "epoch": 1.0607302194300283,
      "grad_norm": 1.6582130193710327,
      "learning_rate": 3.232131475917327e-05,
      "loss": 1.3585,
      "step": 116500
    },
    {
      "epoch": 1.0652827096421742,
      "grad_norm": 1.7362922430038452,
      "learning_rate": 3.2245439922304166e-05,
      "loss": 1.3461,
      "step": 117000
    },
    {
      "epoch": 1.0698351998543203,
      "grad_norm": 1.6501516103744507,
      "learning_rate": 3.2169565085435065e-05,
      "loss": 1.3466,
      "step": 117500
    },
    {
      "epoch": 1.0743876900664664,
      "grad_norm": 1.5018956661224365,
      "learning_rate": 3.2093690248565965e-05,
      "loss": 1.3124,
      "step": 118000
    },
    {
      "epoch": 1.0789401802786125,
      "grad_norm": 1.4581304788589478,
      "learning_rate": 3.2017815411696865e-05,
      "loss": 1.3426,
      "step": 118500
    },
    {
      "epoch": 1.0834926704907584,
      "grad_norm": 1.377450942993164,
      "learning_rate": 3.1941940574827765e-05,
      "loss": 1.3432,
      "step": 119000
    },
    {
      "epoch": 1.0880451607029045,
      "grad_norm": 2.512152671813965,
      "learning_rate": 3.1866065737958665e-05,
      "loss": 1.3467,
      "step": 119500
    },
    {
      "epoch": 1.0925976509150506,
      "grad_norm": 1.1767557859420776,
      "learning_rate": 3.179019090108956e-05,
      "loss": 1.3165,
      "step": 120000
    },
    {
      "epoch": 1.0971501411271967,
      "grad_norm": 1.808351993560791,
      "learning_rate": 3.171431606422046e-05,
      "loss": 1.3397,
      "step": 120500
    },
    {
      "epoch": 1.1017026313393425,
      "grad_norm": 2.1824779510498047,
      "learning_rate": 3.1638441227351365e-05,
      "loss": 1.3901,
      "step": 121000
    },
    {
      "epoch": 1.1062551215514886,
      "grad_norm": 1.33114755153656,
      "learning_rate": 3.1562566390482265e-05,
      "loss": 1.3587,
      "step": 121500
    },
    {
      "epoch": 1.1108076117636347,
      "grad_norm": 1.6965357065200806,
      "learning_rate": 3.1486691553613165e-05,
      "loss": 1.3608,
      "step": 122000
    },
    {
      "epoch": 1.1153601019757808,
      "grad_norm": 1.692177653312683,
      "learning_rate": 3.1410816716744065e-05,
      "loss": 1.3454,
      "step": 122500
    },
    {
      "epoch": 1.1199125921879267,
      "grad_norm": 1.4591429233551025,
      "learning_rate": 3.133494187987496e-05,
      "loss": 1.3373,
      "step": 123000
    },
    {
      "epoch": 1.1244650824000728,
      "grad_norm": 1.2995587587356567,
      "learning_rate": 3.125906704300586e-05,
      "loss": 1.3244,
      "step": 123500
    },
    {
      "epoch": 1.129017572612219,
      "grad_norm": 1.6212031841278076,
      "learning_rate": 3.118319220613676e-05,
      "loss": 1.3158,
      "step": 124000
    },
    {
      "epoch": 1.133570062824365,
      "grad_norm": 2.005744457244873,
      "learning_rate": 3.110731736926766e-05,
      "loss": 1.3359,
      "step": 124500
    },
    {
      "epoch": 1.1381225530365109,
      "grad_norm": 3.4786970615386963,
      "learning_rate": 3.103144253239856e-05,
      "loss": 1.3102,
      "step": 125000
    },
    {
      "epoch": 1.142675043248657,
      "grad_norm": 1.561305284500122,
      "learning_rate": 3.095556769552946e-05,
      "loss": 1.3697,
      "step": 125500
    },
    {
      "epoch": 1.147227533460803,
      "grad_norm": 4.323517799377441,
      "learning_rate": 3.087969285866035e-05,
      "loss": 1.3174,
      "step": 126000
    },
    {
      "epoch": 1.1517800236729492,
      "grad_norm": 0.8158659338951111,
      "learning_rate": 3.080381802179125e-05,
      "loss": 1.3347,
      "step": 126500
    },
    {
      "epoch": 1.156332513885095,
      "grad_norm": 0.8384687304496765,
      "learning_rate": 3.072794318492215e-05,
      "loss": 1.3553,
      "step": 127000
    },
    {
      "epoch": 1.1608850040972412,
      "grad_norm": 1.6851309537887573,
      "learning_rate": 3.065206834805305e-05,
      "loss": 1.3169,
      "step": 127500
    },
    {
      "epoch": 1.1654374943093873,
      "grad_norm": 1.4459443092346191,
      "learning_rate": 3.057619351118395e-05,
      "loss": 1.3523,
      "step": 128000
    },
    {
      "epoch": 1.1699899845215334,
      "grad_norm": 1.0672913789749146,
      "learning_rate": 3.0500318674314855e-05,
      "loss": 1.333,
      "step": 128500
    },
    {
      "epoch": 1.1745424747336792,
      "grad_norm": 1.4032045602798462,
      "learning_rate": 3.0424443837445748e-05,
      "loss": 1.3433,
      "step": 129000
    },
    {
      "epoch": 1.1790949649458253,
      "grad_norm": 1.9707062244415283,
      "learning_rate": 3.0348569000576648e-05,
      "loss": 1.3191,
      "step": 129500
    },
    {
      "epoch": 1.1836474551579714,
      "grad_norm": 1.6448880434036255,
      "learning_rate": 3.0272694163707548e-05,
      "loss": 1.3245,
      "step": 130000
    },
    {
      "epoch": 1.1881999453701175,
      "grad_norm": 1.9429068565368652,
      "learning_rate": 3.0196819326838448e-05,
      "loss": 1.3444,
      "step": 130500
    },
    {
      "epoch": 1.1927524355822634,
      "grad_norm": 3.149672031402588,
      "learning_rate": 3.0120944489969348e-05,
      "loss": 1.3019,
      "step": 131000
    },
    {
      "epoch": 1.1973049257944095,
      "grad_norm": 1.0704978704452515,
      "learning_rate": 3.004506965310025e-05,
      "loss": 1.3416,
      "step": 131500
    },
    {
      "epoch": 1.2018574160065556,
      "grad_norm": 1.811996579170227,
      "learning_rate": 2.9969194816231144e-05,
      "loss": 1.335,
      "step": 132000
    },
    {
      "epoch": 1.2064099062187017,
      "grad_norm": 1.251356601715088,
      "learning_rate": 2.9893319979362044e-05,
      "loss": 1.3276,
      "step": 132500
    },
    {
      "epoch": 1.2109623964308476,
      "grad_norm": 4.401016712188721,
      "learning_rate": 2.9817445142492944e-05,
      "loss": 1.3117,
      "step": 133000
    },
    {
      "epoch": 1.2155148866429937,
      "grad_norm": 0.967278242111206,
      "learning_rate": 2.9741570305623844e-05,
      "loss": 1.3817,
      "step": 133500
    },
    {
      "epoch": 1.2200673768551398,
      "grad_norm": 2.813791036605835,
      "learning_rate": 2.9665695468754744e-05,
      "loss": 1.3525,
      "step": 134000
    },
    {
      "epoch": 1.2246198670672859,
      "grad_norm": 1.3094284534454346,
      "learning_rate": 2.9589820631885644e-05,
      "loss": 1.3295,
      "step": 134500
    },
    {
      "epoch": 1.229172357279432,
      "grad_norm": 1.0387040376663208,
      "learning_rate": 2.951394579501654e-05,
      "loss": 1.3469,
      "step": 135000
    },
    {
      "epoch": 1.2337248474915778,
      "grad_norm": 2.019998788833618,
      "learning_rate": 2.943807095814744e-05,
      "loss": 1.3247,
      "step": 135500
    },
    {
      "epoch": 1.238277337703724,
      "grad_norm": 1.3990575075149536,
      "learning_rate": 2.936219612127834e-05,
      "loss": 1.3394,
      "step": 136000
    },
    {
      "epoch": 1.24282982791587,
      "grad_norm": 2.048640489578247,
      "learning_rate": 2.928632128440924e-05,
      "loss": 1.3361,
      "step": 136500
    },
    {
      "epoch": 1.247382318128016,
      "grad_norm": 3.0081748962402344,
      "learning_rate": 2.921044644754014e-05,
      "loss": 1.2802,
      "step": 137000
    },
    {
      "epoch": 1.251934808340162,
      "grad_norm": 1.4741703271865845,
      "learning_rate": 2.913457161067104e-05,
      "loss": 1.3483,
      "step": 137500
    },
    {
      "epoch": 1.2564872985523081,
      "grad_norm": 3.313781261444092,
      "learning_rate": 2.9058696773801937e-05,
      "loss": 1.3261,
      "step": 138000
    },
    {
      "epoch": 1.2610397887644542,
      "grad_norm": 1.610719919204712,
      "learning_rate": 2.8982821936932837e-05,
      "loss": 1.3462,
      "step": 138500
    },
    {
      "epoch": 1.2655922789766003,
      "grad_norm": 2.2081501483917236,
      "learning_rate": 2.8906947100063737e-05,
      "loss": 1.3244,
      "step": 139000
    },
    {
      "epoch": 1.2701447691887462,
      "grad_norm": 1.1500531435012817,
      "learning_rate": 2.8831072263194637e-05,
      "loss": 1.2967,
      "step": 139500
    },
    {
      "epoch": 1.2746972594008923,
      "grad_norm": 0.9683411121368408,
      "learning_rate": 2.8755197426325537e-05,
      "loss": 1.3492,
      "step": 140000
    },
    {
      "epoch": 1.2792497496130384,
      "grad_norm": 1.5783758163452148,
      "learning_rate": 2.8679322589456437e-05,
      "loss": 1.3197,
      "step": 140500
    },
    {
      "epoch": 1.2838022398251843,
      "grad_norm": 1.7220871448516846,
      "learning_rate": 2.860344775258733e-05,
      "loss": 1.3092,
      "step": 141000
    },
    {
      "epoch": 1.2883547300373304,
      "grad_norm": 2.9035158157348633,
      "learning_rate": 2.852757291571823e-05,
      "loss": 1.3101,
      "step": 141500
    },
    {
      "epoch": 1.2929072202494765,
      "grad_norm": 1.4323313236236572,
      "learning_rate": 2.845169807884913e-05,
      "loss": 1.3553,
      "step": 142000
    },
    {
      "epoch": 1.2974597104616226,
      "grad_norm": 1.4018563032150269,
      "learning_rate": 2.8375823241980033e-05,
      "loss": 1.3343,
      "step": 142500
    },
    {
      "epoch": 1.3020122006737687,
      "grad_norm": 1.583981990814209,
      "learning_rate": 2.8299948405110933e-05,
      "loss": 1.3031,
      "step": 143000
    },
    {
      "epoch": 1.3065646908859145,
      "grad_norm": 1.5928168296813965,
      "learning_rate": 2.8224073568241833e-05,
      "loss": 1.3133,
      "step": 143500
    },
    {
      "epoch": 1.3111171810980606,
      "grad_norm": 2.5757739543914795,
      "learning_rate": 2.8148198731372727e-05,
      "loss": 1.3236,
      "step": 144000
    },
    {
      "epoch": 1.3156696713102067,
      "grad_norm": 1.3519634008407593,
      "learning_rate": 2.8072323894503627e-05,
      "loss": 1.3087,
      "step": 144500
    },
    {
      "epoch": 1.3202221615223526,
      "grad_norm": 1.6798925399780273,
      "learning_rate": 2.7996449057634526e-05,
      "loss": 1.307,
      "step": 145000
    },
    {
      "epoch": 1.3247746517344987,
      "grad_norm": 1.3874562978744507,
      "learning_rate": 2.7920574220765426e-05,
      "loss": 1.3315,
      "step": 145500
    },
    {
      "epoch": 1.3293271419466448,
      "grad_norm": 1.9649837017059326,
      "learning_rate": 2.7844699383896326e-05,
      "loss": 1.321,
      "step": 146000
    },
    {
      "epoch": 1.333879632158791,
      "grad_norm": 1.595662236213684,
      "learning_rate": 2.7768824547027226e-05,
      "loss": 1.3124,
      "step": 146500
    },
    {
      "epoch": 1.338432122370937,
      "grad_norm": 1.8520607948303223,
      "learning_rate": 2.7692949710158123e-05,
      "loss": 1.33,
      "step": 147000
    },
    {
      "epoch": 1.3429846125830829,
      "grad_norm": 1.8915382623672485,
      "learning_rate": 2.7617074873289023e-05,
      "loss": 1.3545,
      "step": 147500
    },
    {
      "epoch": 1.347537102795229,
      "grad_norm": 2.4892516136169434,
      "learning_rate": 2.7541200036419923e-05,
      "loss": 1.3264,
      "step": 148000
    },
    {
      "epoch": 1.352089593007375,
      "grad_norm": 1.8140419721603394,
      "learning_rate": 2.7465325199550823e-05,
      "loss": 1.3473,
      "step": 148500
    },
    {
      "epoch": 1.356642083219521,
      "grad_norm": 0.9238882660865784,
      "learning_rate": 2.7389450362681723e-05,
      "loss": 1.3201,
      "step": 149000
    },
    {
      "epoch": 1.361194573431667,
      "grad_norm": 2.3246827125549316,
      "learning_rate": 2.7313575525812623e-05,
      "loss": 1.3092,
      "step": 149500
    },
    {
      "epoch": 1.3657470636438132,
      "grad_norm": 1.8816039562225342,
      "learning_rate": 2.723770068894352e-05,
      "loss": 1.3589,
      "step": 150000
    },
    {
      "epoch": 1.3702995538559593,
      "grad_norm": 2.0951240062713623,
      "learning_rate": 2.716182585207442e-05,
      "loss": 1.2984,
      "step": 150500
    },
    {
      "epoch": 1.3748520440681054,
      "grad_norm": 2.205416202545166,
      "learning_rate": 2.708595101520532e-05,
      "loss": 1.3293,
      "step": 151000
    },
    {
      "epoch": 1.3794045342802512,
      "grad_norm": 2.8396096229553223,
      "learning_rate": 2.701007617833622e-05,
      "loss": 1.305,
      "step": 151500
    },
    {
      "epoch": 1.3839570244923973,
      "grad_norm": 1.847890853881836,
      "learning_rate": 2.693420134146712e-05,
      "loss": 1.3311,
      "step": 152000
    },
    {
      "epoch": 1.3885095147045434,
      "grad_norm": 1.740880012512207,
      "learning_rate": 2.6858326504598012e-05,
      "loss": 1.3071,
      "step": 152500
    },
    {
      "epoch": 1.3930620049166893,
      "grad_norm": 2.251570463180542,
      "learning_rate": 2.6782451667728912e-05,
      "loss": 1.3359,
      "step": 153000
    },
    {
      "epoch": 1.3976144951288354,
      "grad_norm": 2.7269747257232666,
      "learning_rate": 2.6706576830859816e-05,
      "loss": 1.3043,
      "step": 153500
    },
    {
      "epoch": 1.4021669853409815,
      "grad_norm": 1.8080662488937378,
      "learning_rate": 2.6630701993990716e-05,
      "loss": 1.3412,
      "step": 154000
    },
    {
      "epoch": 1.4067194755531276,
      "grad_norm": 1.7722899913787842,
      "learning_rate": 2.6554827157121616e-05,
      "loss": 1.3801,
      "step": 154500
    },
    {
      "epoch": 1.4112719657652737,
      "grad_norm": 2.2214584350585938,
      "learning_rate": 2.6478952320252516e-05,
      "loss": 1.3179,
      "step": 155000
    },
    {
      "epoch": 1.4158244559774196,
      "grad_norm": 1.6643017530441284,
      "learning_rate": 2.640307748338341e-05,
      "loss": 1.357,
      "step": 155500
    },
    {
      "epoch": 1.4203769461895657,
      "grad_norm": 1.2427759170532227,
      "learning_rate": 2.632720264651431e-05,
      "loss": 1.3045,
      "step": 156000
    },
    {
      "epoch": 1.4249294364017118,
      "grad_norm": 0.9536493420600891,
      "learning_rate": 2.625132780964521e-05,
      "loss": 1.3277,
      "step": 156500
    },
    {
      "epoch": 1.4294819266138576,
      "grad_norm": 2.002270221710205,
      "learning_rate": 2.617545297277611e-05,
      "loss": 1.317,
      "step": 157000
    },
    {
      "epoch": 1.4340344168260037,
      "grad_norm": 0.4921259582042694,
      "learning_rate": 2.609957813590701e-05,
      "loss": 1.3152,
      "step": 157500
    },
    {
      "epoch": 1.4385869070381498,
      "grad_norm": 1.8741861581802368,
      "learning_rate": 2.602370329903791e-05,
      "loss": 1.3133,
      "step": 158000
    },
    {
      "epoch": 1.443139397250296,
      "grad_norm": 1.2460625171661377,
      "learning_rate": 2.5947828462168805e-05,
      "loss": 1.282,
      "step": 158500
    },
    {
      "epoch": 1.447691887462442,
      "grad_norm": 3.2104973793029785,
      "learning_rate": 2.5871953625299705e-05,
      "loss": 1.3427,
      "step": 159000
    },
    {
      "epoch": 1.452244377674588,
      "grad_norm": 3.2712881565093994,
      "learning_rate": 2.5796078788430605e-05,
      "loss": 1.3266,
      "step": 159500
    },
    {
      "epoch": 1.456796867886734,
      "grad_norm": 1.8932713270187378,
      "learning_rate": 2.5720203951561505e-05,
      "loss": 1.3143,
      "step": 160000
    },
    {
      "epoch": 1.4613493580988801,
      "grad_norm": 1.288489580154419,
      "learning_rate": 2.5644329114692405e-05,
      "loss": 1.3728,
      "step": 160500
    },
    {
      "epoch": 1.4659018483110262,
      "grad_norm": 1.4560388326644897,
      "learning_rate": 2.5568454277823305e-05,
      "loss": 1.3375,
      "step": 161000
    },
    {
      "epoch": 1.4704543385231723,
      "grad_norm": 1.2329695224761963,
      "learning_rate": 2.5492579440954202e-05,
      "loss": 1.3237,
      "step": 161500
    },
    {
      "epoch": 1.4750068287353182,
      "grad_norm": 4.188831329345703,
      "learning_rate": 2.54167046040851e-05,
      "loss": 1.3132,
      "step": 162000
    },
    {
      "epoch": 1.4795593189474643,
      "grad_norm": 1.5511448383331299,
      "learning_rate": 2.5340829767216e-05,
      "loss": 1.29,
      "step": 162500
    },
    {
      "epoch": 1.4841118091596104,
      "grad_norm": 4.672113418579102,
      "learning_rate": 2.52649549303469e-05,
      "loss": 1.3199,
      "step": 163000
    },
    {
      "epoch": 1.4886642993717563,
      "grad_norm": 1.2039318084716797,
      "learning_rate": 2.51890800934778e-05,
      "loss": 1.2841,
      "step": 163500
    },
    {
      "epoch": 1.4932167895839024,
      "grad_norm": 1.847794532775879,
      "learning_rate": 2.51132052566087e-05,
      "loss": 1.3288,
      "step": 164000
    },
    {
      "epoch": 1.4977692797960485,
      "grad_norm": 1.4057514667510986,
      "learning_rate": 2.5037330419739595e-05,
      "loss": 1.3236,
      "step": 164500
    },
    {
      "epoch": 1.5023217700081943,
      "grad_norm": 2.012225866317749,
      "learning_rate": 2.4961455582870498e-05,
      "loss": 1.3072,
      "step": 165000
    },
    {
      "epoch": 1.5068742602203407,
      "grad_norm": 1.6773337125778198,
      "learning_rate": 2.4885580746001398e-05,
      "loss": 1.3042,
      "step": 165500
    },
    {
      "epoch": 1.5114267504324865,
      "grad_norm": 2.117347240447998,
      "learning_rate": 2.4809705909132298e-05,
      "loss": 1.2905,
      "step": 166000
    },
    {
      "epoch": 1.5159792406446326,
      "grad_norm": 1.8705825805664062,
      "learning_rate": 2.4733831072263198e-05,
      "loss": 1.3386,
      "step": 166500
    },
    {
      "epoch": 1.5205317308567787,
      "grad_norm": 1.494754433631897,
      "learning_rate": 2.4657956235394095e-05,
      "loss": 1.3434,
      "step": 167000
    },
    {
      "epoch": 1.5250842210689246,
      "grad_norm": 1.9470508098602295,
      "learning_rate": 2.4582081398524995e-05,
      "loss": 1.332,
      "step": 167500
    },
    {
      "epoch": 1.5296367112810707,
      "grad_norm": 1.1308850049972534,
      "learning_rate": 2.450620656165589e-05,
      "loss": 1.3116,
      "step": 168000
    },
    {
      "epoch": 1.5341892014932168,
      "grad_norm": 1.1885138750076294,
      "learning_rate": 2.443033172478679e-05,
      "loss": 1.2934,
      "step": 168500
    },
    {
      "epoch": 1.5387416917053627,
      "grad_norm": 1.2506706714630127,
      "learning_rate": 2.435445688791769e-05,
      "loss": 1.3238,
      "step": 169000
    },
    {
      "epoch": 1.543294181917509,
      "grad_norm": 1.812448501586914,
      "learning_rate": 2.427858205104859e-05,
      "loss": 1.3089,
      "step": 169500
    },
    {
      "epoch": 1.5478466721296549,
      "grad_norm": 1.8032556772232056,
      "learning_rate": 2.420270721417949e-05,
      "loss": 1.3079,
      "step": 170000
    },
    {
      "epoch": 1.552399162341801,
      "grad_norm": 1.5847158432006836,
      "learning_rate": 2.412683237731039e-05,
      "loss": 1.3092,
      "step": 170500
    },
    {
      "epoch": 1.556951652553947,
      "grad_norm": 1.7021327018737793,
      "learning_rate": 2.4050957540441288e-05,
      "loss": 1.346,
      "step": 171000
    },
    {
      "epoch": 1.561504142766093,
      "grad_norm": 1.5729948282241821,
      "learning_rate": 2.3975082703572188e-05,
      "loss": 1.32,
      "step": 171500
    },
    {
      "epoch": 1.566056632978239,
      "grad_norm": 1.2317551374435425,
      "learning_rate": 2.3899207866703088e-05,
      "loss": 1.3129,
      "step": 172000
    },
    {
      "epoch": 1.5706091231903851,
      "grad_norm": 1.8153263330459595,
      "learning_rate": 2.3823333029833984e-05,
      "loss": 1.3026,
      "step": 172500
    },
    {
      "epoch": 1.575161613402531,
      "grad_norm": 1.255297303199768,
      "learning_rate": 2.3747458192964887e-05,
      "loss": 1.3328,
      "step": 173000
    },
    {
      "epoch": 1.5797141036146773,
      "grad_norm": 1.3833106756210327,
      "learning_rate": 2.3671583356095787e-05,
      "loss": 1.3335,
      "step": 173500
    },
    {
      "epoch": 1.5842665938268232,
      "grad_norm": 1.813408613204956,
      "learning_rate": 2.3595708519226684e-05,
      "loss": 1.3541,
      "step": 174000
    },
    {
      "epoch": 1.5888190840389693,
      "grad_norm": 2.2668211460113525,
      "learning_rate": 2.3519833682357584e-05,
      "loss": 1.3046,
      "step": 174500
    },
    {
      "epoch": 1.5933715742511154,
      "grad_norm": 1.6130638122558594,
      "learning_rate": 2.3443958845488484e-05,
      "loss": 1.3281,
      "step": 175000
    },
    {
      "epoch": 1.5979240644632613,
      "grad_norm": 1.5277936458587646,
      "learning_rate": 2.336808400861938e-05,
      "loss": 1.3404,
      "step": 175500
    },
    {
      "epoch": 1.6024765546754074,
      "grad_norm": 3.2314531803131104,
      "learning_rate": 2.329220917175028e-05,
      "loss": 1.3108,
      "step": 176000
    },
    {
      "epoch": 1.6070290448875535,
      "grad_norm": 1.3392505645751953,
      "learning_rate": 2.321633433488118e-05,
      "loss": 1.3382,
      "step": 176500
    },
    {
      "epoch": 1.6115815350996994,
      "grad_norm": 0.9741905331611633,
      "learning_rate": 2.314045949801208e-05,
      "loss": 1.3465,
      "step": 177000
    },
    {
      "epoch": 1.6161340253118457,
      "grad_norm": 1.4180400371551514,
      "learning_rate": 2.306458466114298e-05,
      "loss": 1.3116,
      "step": 177500
    },
    {
      "epoch": 1.6206865155239916,
      "grad_norm": 1.7524619102478027,
      "learning_rate": 2.298870982427388e-05,
      "loss": 1.2997,
      "step": 178000
    },
    {
      "epoch": 1.6252390057361377,
      "grad_norm": 1.5839983224868774,
      "learning_rate": 2.2912834987404777e-05,
      "loss": 1.3241,
      "step": 178500
    },
    {
      "epoch": 1.6297914959482838,
      "grad_norm": 1.83384108543396,
      "learning_rate": 2.2836960150535677e-05,
      "loss": 1.3176,
      "step": 179000
    },
    {
      "epoch": 1.6343439861604296,
      "grad_norm": 1.496987223625183,
      "learning_rate": 2.2761085313666577e-05,
      "loss": 1.3032,
      "step": 179500
    },
    {
      "epoch": 1.638896476372576,
      "grad_norm": 0.7502235770225525,
      "learning_rate": 2.2685210476797473e-05,
      "loss": 1.3123,
      "step": 180000
    },
    {
      "epoch": 1.6434489665847218,
      "grad_norm": 2.814448356628418,
      "learning_rate": 2.2609335639928377e-05,
      "loss": 1.3191,
      "step": 180500
    },
    {
      "epoch": 1.648001456796868,
      "grad_norm": 1.026883840560913,
      "learning_rate": 2.2533460803059277e-05,
      "loss": 1.2933,
      "step": 181000
    },
    {
      "epoch": 1.652553947009014,
      "grad_norm": 1.6958979368209839,
      "learning_rate": 2.2457585966190173e-05,
      "loss": 1.3494,
      "step": 181500
    },
    {
      "epoch": 1.65710643722116,
      "grad_norm": 1.4546979665756226,
      "learning_rate": 2.2381711129321073e-05,
      "loss": 1.2992,
      "step": 182000
    },
    {
      "epoch": 1.661658927433306,
      "grad_norm": 3.7702746391296387,
      "learning_rate": 2.2305836292451973e-05,
      "loss": 1.3085,
      "step": 182500
    },
    {
      "epoch": 1.666211417645452,
      "grad_norm": 2.0571863651275635,
      "learning_rate": 2.222996145558287e-05,
      "loss": 1.3442,
      "step": 183000
    },
    {
      "epoch": 1.670763907857598,
      "grad_norm": 2.0650198459625244,
      "learning_rate": 2.215408661871377e-05,
      "loss": 1.2939,
      "step": 183500
    },
    {
      "epoch": 1.6753163980697443,
      "grad_norm": 2.050914764404297,
      "learning_rate": 2.207821178184467e-05,
      "loss": 1.3427,
      "step": 184000
    },
    {
      "epoch": 1.6798688882818902,
      "grad_norm": 1.648142695426941,
      "learning_rate": 2.200233694497557e-05,
      "loss": 1.3085,
      "step": 184500
    },
    {
      "epoch": 1.6844213784940363,
      "grad_norm": 4.423246383666992,
      "learning_rate": 2.192646210810647e-05,
      "loss": 1.3024,
      "step": 185000
    },
    {
      "epoch": 1.6889738687061824,
      "grad_norm": 1.0575041770935059,
      "learning_rate": 2.185058727123737e-05,
      "loss": 1.3384,
      "step": 185500
    },
    {
      "epoch": 1.6935263589183283,
      "grad_norm": 1.0151809453964233,
      "learning_rate": 2.1774712434368266e-05,
      "loss": 1.3211,
      "step": 186000
    },
    {
      "epoch": 1.6980788491304744,
      "grad_norm": 1.007027506828308,
      "learning_rate": 2.1698837597499166e-05,
      "loss": 1.3149,
      "step": 186500
    },
    {
      "epoch": 1.7026313393426205,
      "grad_norm": 1.3097753524780273,
      "learning_rate": 2.1622962760630066e-05,
      "loss": 1.2853,
      "step": 187000
    },
    {
      "epoch": 1.7071838295547663,
      "grad_norm": 2.06523060798645,
      "learning_rate": 2.1547087923760963e-05,
      "loss": 1.3136,
      "step": 187500
    },
    {
      "epoch": 1.7117363197669127,
      "grad_norm": 1.774385929107666,
      "learning_rate": 2.1471213086891863e-05,
      "loss": 1.3119,
      "step": 188000
    },
    {
      "epoch": 1.7162888099790585,
      "grad_norm": 0.9065178632736206,
      "learning_rate": 2.1395338250022766e-05,
      "loss": 1.3135,
      "step": 188500
    },
    {
      "epoch": 1.7208413001912046,
      "grad_norm": 1.649781584739685,
      "learning_rate": 2.1319463413153663e-05,
      "loss": 1.3534,
      "step": 189000
    },
    {
      "epoch": 1.7253937904033507,
      "grad_norm": 2.406254291534424,
      "learning_rate": 2.1243588576284563e-05,
      "loss": 1.275,
      "step": 189500
    },
    {
      "epoch": 1.7299462806154966,
      "grad_norm": 1.4433355331420898,
      "learning_rate": 2.1167713739415463e-05,
      "loss": 1.2855,
      "step": 190000
    },
    {
      "epoch": 1.7344987708276427,
      "grad_norm": 1.7602217197418213,
      "learning_rate": 2.109183890254636e-05,
      "loss": 1.3095,
      "step": 190500
    },
    {
      "epoch": 1.7390512610397888,
      "grad_norm": 1.9585429430007935,
      "learning_rate": 2.101596406567726e-05,
      "loss": 1.3236,
      "step": 191000
    },
    {
      "epoch": 1.7436037512519347,
      "grad_norm": 2.0951359272003174,
      "learning_rate": 2.094008922880816e-05,
      "loss": 1.3083,
      "step": 191500
    },
    {
      "epoch": 1.748156241464081,
      "grad_norm": 1.0139554738998413,
      "learning_rate": 2.086421439193906e-05,
      "loss": 1.2771,
      "step": 192000
    },
    {
      "epoch": 1.7527087316762269,
      "grad_norm": 3.249112606048584,
      "learning_rate": 2.078833955506996e-05,
      "loss": 1.2997,
      "step": 192500
    },
    {
      "epoch": 1.757261221888373,
      "grad_norm": 1.35708487033844,
      "learning_rate": 2.071246471820086e-05,
      "loss": 1.3014,
      "step": 193000
    },
    {
      "epoch": 1.761813712100519,
      "grad_norm": 2.0398905277252197,
      "learning_rate": 2.0636589881331756e-05,
      "loss": 1.304,
      "step": 193500
    },
    {
      "epoch": 1.766366202312665,
      "grad_norm": 1.4281554222106934,
      "learning_rate": 2.0560715044462656e-05,
      "loss": 1.2999,
      "step": 194000
    },
    {
      "epoch": 1.770918692524811,
      "grad_norm": 1.7434582710266113,
      "learning_rate": 2.0484840207593556e-05,
      "loss": 1.2873,
      "step": 194500
    },
    {
      "epoch": 1.7754711827369571,
      "grad_norm": 1.8078045845031738,
      "learning_rate": 2.0408965370724452e-05,
      "loss": 1.3079,
      "step": 195000
    },
    {
      "epoch": 1.780023672949103,
      "grad_norm": 1.8207480907440186,
      "learning_rate": 2.0333090533855352e-05,
      "loss": 1.3148,
      "step": 195500
    },
    {
      "epoch": 1.7845761631612493,
      "grad_norm": 2.051539897918701,
      "learning_rate": 2.0257215696986252e-05,
      "loss": 1.313,
      "step": 196000
    },
    {
      "epoch": 1.7891286533733952,
      "grad_norm": 1.9800575971603394,
      "learning_rate": 2.0181340860117152e-05,
      "loss": 1.292,
      "step": 196500
    },
    {
      "epoch": 1.7936811435855413,
      "grad_norm": 1.3687883615493774,
      "learning_rate": 2.0105466023248052e-05,
      "loss": 1.2946,
      "step": 197000
    },
    {
      "epoch": 1.7982336337976874,
      "grad_norm": 1.606689691543579,
      "learning_rate": 2.0029591186378952e-05,
      "loss": 1.2835,
      "step": 197500
    },
    {
      "epoch": 1.8027861240098333,
      "grad_norm": 1.668310284614563,
      "learning_rate": 1.995371634950985e-05,
      "loss": 1.311,
      "step": 198000
    },
    {
      "epoch": 1.8073386142219794,
      "grad_norm": 1.5143674612045288,
      "learning_rate": 1.987784151264075e-05,
      "loss": 1.328,
      "step": 198500
    },
    {
      "epoch": 1.8118911044341255,
      "grad_norm": 1.2308881282806396,
      "learning_rate": 1.980196667577165e-05,
      "loss": 1.2966,
      "step": 199000
    },
    {
      "epoch": 1.8164435946462714,
      "grad_norm": 1.9216700792312622,
      "learning_rate": 1.9726091838902545e-05,
      "loss": 1.3152,
      "step": 199500
    },
    {
      "epoch": 1.8209960848584177,
      "grad_norm": 1.6158149242401123,
      "learning_rate": 1.965021700203345e-05,
      "loss": 1.2793,
      "step": 200000
    },
    {
      "epoch": 1.8255485750705636,
      "grad_norm": 1.1222878694534302,
      "learning_rate": 1.957434216516435e-05,
      "loss": 1.3161,
      "step": 200500
    },
    {
      "epoch": 1.8301010652827097,
      "grad_norm": 1.535786509513855,
      "learning_rate": 1.9498467328295245e-05,
      "loss": 1.29,
      "step": 201000
    },
    {
      "epoch": 1.8346535554948558,
      "grad_norm": 1.2622369527816772,
      "learning_rate": 1.9422592491426145e-05,
      "loss": 1.2841,
      "step": 201500
    },
    {
      "epoch": 1.8392060457070016,
      "grad_norm": 3.5427122116088867,
      "learning_rate": 1.9346717654557045e-05,
      "loss": 1.3074,
      "step": 202000
    },
    {
      "epoch": 1.8437585359191477,
      "grad_norm": 1.4522877931594849,
      "learning_rate": 1.927084281768794e-05,
      "loss": 1.2934,
      "step": 202500
    },
    {
      "epoch": 1.8483110261312938,
      "grad_norm": 2.219264030456543,
      "learning_rate": 1.919496798081884e-05,
      "loss": 1.3173,
      "step": 203000
    },
    {
      "epoch": 1.8528635163434397,
      "grad_norm": 1.9081783294677734,
      "learning_rate": 1.911909314394974e-05,
      "loss": 1.3229,
      "step": 203500
    },
    {
      "epoch": 1.857416006555586,
      "grad_norm": 1.4428763389587402,
      "learning_rate": 1.904321830708064e-05,
      "loss": 1.3367,
      "step": 204000
    },
    {
      "epoch": 1.861968496767732,
      "grad_norm": 1.497144103050232,
      "learning_rate": 1.896734347021154e-05,
      "loss": 1.2971,
      "step": 204500
    },
    {
      "epoch": 1.866520986979878,
      "grad_norm": 1.484753966331482,
      "learning_rate": 1.8891468633342438e-05,
      "loss": 1.2827,
      "step": 205000
    },
    {
      "epoch": 1.871073477192024,
      "grad_norm": 2.1920154094696045,
      "learning_rate": 1.8815593796473338e-05,
      "loss": 1.3254,
      "step": 205500
    },
    {
      "epoch": 1.87562596740417,
      "grad_norm": 2.901679039001465,
      "learning_rate": 1.8739718959604238e-05,
      "loss": 1.321,
      "step": 206000
    },
    {
      "epoch": 1.880178457616316,
      "grad_norm": 1.5530126094818115,
      "learning_rate": 1.8663844122735134e-05,
      "loss": 1.313,
      "step": 206500
    },
    {
      "epoch": 1.8847309478284622,
      "grad_norm": 2.012547254562378,
      "learning_rate": 1.8587969285866034e-05,
      "loss": 1.3118,
      "step": 207000
    },
    {
      "epoch": 1.889283438040608,
      "grad_norm": 2.0960752964019775,
      "learning_rate": 1.8512094448996938e-05,
      "loss": 1.3075,
      "step": 207500
    },
    {
      "epoch": 1.8938359282527544,
      "grad_norm": 1.3937523365020752,
      "learning_rate": 1.8436219612127834e-05,
      "loss": 1.3033,
      "step": 208000
    },
    {
      "epoch": 1.8983884184649003,
      "grad_norm": 1.2954022884368896,
      "learning_rate": 1.8360344775258734e-05,
      "loss": 1.2981,
      "step": 208500
    },
    {
      "epoch": 1.9029409086770463,
      "grad_norm": 1.542432188987732,
      "learning_rate": 1.8284469938389634e-05,
      "loss": 1.321,
      "step": 209000
    },
    {
      "epoch": 1.9074933988891924,
      "grad_norm": 1.6144071817398071,
      "learning_rate": 1.820859510152053e-05,
      "loss": 1.3179,
      "step": 209500
    },
    {
      "epoch": 1.9120458891013383,
      "grad_norm": 1.8272895812988281,
      "learning_rate": 1.813272026465143e-05,
      "loss": 1.2954,
      "step": 210000
    },
    {
      "epoch": 1.9165983793134844,
      "grad_norm": 1.7835822105407715,
      "learning_rate": 1.805684542778233e-05,
      "loss": 1.2911,
      "step": 210500
    },
    {
      "epoch": 1.9211508695256305,
      "grad_norm": 3.8876395225524902,
      "learning_rate": 1.798097059091323e-05,
      "loss": 1.307,
      "step": 211000
    },
    {
      "epoch": 1.9257033597377764,
      "grad_norm": 2.9842615127563477,
      "learning_rate": 1.790509575404413e-05,
      "loss": 1.2784,
      "step": 211500
    },
    {
      "epoch": 1.9302558499499227,
      "grad_norm": 1.462430715560913,
      "learning_rate": 1.782922091717503e-05,
      "loss": 1.2642,
      "step": 212000
    },
    {
      "epoch": 1.9348083401620686,
      "grad_norm": 2.187493324279785,
      "learning_rate": 1.7753346080305927e-05,
      "loss": 1.3199,
      "step": 212500
    },
    {
      "epoch": 1.9393608303742147,
      "grad_norm": 1.5249699354171753,
      "learning_rate": 1.7677471243436827e-05,
      "loss": 1.2764,
      "step": 213000
    },
    {
      "epoch": 1.9439133205863608,
      "grad_norm": 1.7772417068481445,
      "learning_rate": 1.7601596406567727e-05,
      "loss": 1.312,
      "step": 213500
    },
    {
      "epoch": 1.9484658107985067,
      "grad_norm": 1.7825020551681519,
      "learning_rate": 1.7525721569698624e-05,
      "loss": 1.2559,
      "step": 214000
    },
    {
      "epoch": 1.953018301010653,
      "grad_norm": 1.6443028450012207,
      "learning_rate": 1.7449846732829524e-05,
      "loss": 1.2861,
      "step": 214500
    },
    {
      "epoch": 1.9575707912227989,
      "grad_norm": 1.441727638244629,
      "learning_rate": 1.7373971895960424e-05,
      "loss": 1.3176,
      "step": 215000
    },
    {
      "epoch": 1.962123281434945,
      "grad_norm": 1.3216092586517334,
      "learning_rate": 1.7298097059091324e-05,
      "loss": 1.2969,
      "step": 215500
    },
    {
      "epoch": 1.966675771647091,
      "grad_norm": 0.9373908638954163,
      "learning_rate": 1.7222222222222224e-05,
      "loss": 1.2939,
      "step": 216000
    },
    {
      "epoch": 1.971228261859237,
      "grad_norm": 1.6834022998809814,
      "learning_rate": 1.7146347385353124e-05,
      "loss": 1.2785,
      "step": 216500
    },
    {
      "epoch": 1.975780752071383,
      "grad_norm": 1.567661166191101,
      "learning_rate": 1.707047254848402e-05,
      "loss": 1.3165,
      "step": 217000
    },
    {
      "epoch": 1.9803332422835291,
      "grad_norm": 2.280330181121826,
      "learning_rate": 1.699459771161492e-05,
      "loss": 1.3291,
      "step": 217500
    },
    {
      "epoch": 1.984885732495675,
      "grad_norm": 2.101574659347534,
      "learning_rate": 1.691872287474582e-05,
      "loss": 1.3091,
      "step": 218000
    },
    {
      "epoch": 1.9894382227078213,
      "grad_norm": 2.850477933883667,
      "learning_rate": 1.6842848037876717e-05,
      "loss": 1.2925,
      "step": 218500
    },
    {
      "epoch": 1.9939907129199672,
      "grad_norm": 2.522669553756714,
      "learning_rate": 1.676697320100762e-05,
      "loss": 1.2956,
      "step": 219000
    },
    {
      "epoch": 1.9985432031321133,
      "grad_norm": 2.072850465774536,
      "learning_rate": 1.669109836413852e-05,
      "loss": 1.2559,
      "step": 219500
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.2900017499923706,
      "eval_runtime": 7938.0982,
      "eval_samples_per_second": 6.918,
      "eval_steps_per_second": 0.865,
      "step": 219660
    }
  ],
  "logging_steps": 500,
  "max_steps": 329490,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.869770387456e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
