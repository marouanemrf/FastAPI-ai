{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 109830,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004552490212146044,
      "grad_norm": 6.711992263793945,
      "learning_rate": 4.992427691280464e-05,
      "loss": 1.7673,
      "step": 500
    },
    {
      "epoch": 0.009104980424292088,
      "grad_norm": 3.201833963394165,
      "learning_rate": 4.984840207593554e-05,
      "loss": 1.604,
      "step": 1000
    },
    {
      "epoch": 0.013657470636438131,
      "grad_norm": 4.119305610656738,
      "learning_rate": 4.977252723906644e-05,
      "loss": 1.6249,
      "step": 1500
    },
    {
      "epoch": 0.018209960848584176,
      "grad_norm": 3.2692856788635254,
      "learning_rate": 4.9696652402197335e-05,
      "loss": 1.6417,
      "step": 2000
    },
    {
      "epoch": 0.02276245106073022,
      "grad_norm": 2.832024097442627,
      "learning_rate": 4.9620777565328235e-05,
      "loss": 1.6127,
      "step": 2500
    },
    {
      "epoch": 0.027314941272876262,
      "grad_norm": 2.507462501525879,
      "learning_rate": 4.9544902728459135e-05,
      "loss": 1.6134,
      "step": 3000
    },
    {
      "epoch": 0.03186743148502231,
      "grad_norm": 2.6528735160827637,
      "learning_rate": 4.9469027891590035e-05,
      "loss": 1.6213,
      "step": 3500
    },
    {
      "epoch": 0.03641992169716835,
      "grad_norm": 2.410271644592285,
      "learning_rate": 4.9393153054720935e-05,
      "loss": 1.5815,
      "step": 4000
    },
    {
      "epoch": 0.040972411909314395,
      "grad_norm": 2.4228503704071045,
      "learning_rate": 4.9317278217851835e-05,
      "loss": 1.6303,
      "step": 4500
    },
    {
      "epoch": 0.04552490212146044,
      "grad_norm": 2.2473554611206055,
      "learning_rate": 4.924140338098273e-05,
      "loss": 1.5858,
      "step": 5000
    },
    {
      "epoch": 0.05007739233360648,
      "grad_norm": 4.8948822021484375,
      "learning_rate": 4.916552854411363e-05,
      "loss": 1.6002,
      "step": 5500
    },
    {
      "epoch": 0.054629882545752524,
      "grad_norm": 1.545534372329712,
      "learning_rate": 4.908965370724453e-05,
      "loss": 1.5593,
      "step": 6000
    },
    {
      "epoch": 0.05918237275789857,
      "grad_norm": 2.9527928829193115,
      "learning_rate": 4.9013778870375435e-05,
      "loss": 1.5789,
      "step": 6500
    },
    {
      "epoch": 0.06373486297004462,
      "grad_norm": 2.5952768325805664,
      "learning_rate": 4.8937904033506335e-05,
      "loss": 1.6503,
      "step": 7000
    },
    {
      "epoch": 0.06828735318219066,
      "grad_norm": 2.5124309062957764,
      "learning_rate": 4.8862029196637235e-05,
      "loss": 1.5719,
      "step": 7500
    },
    {
      "epoch": 0.0728398433943367,
      "grad_norm": 2.3284144401550293,
      "learning_rate": 4.878615435976813e-05,
      "loss": 1.5241,
      "step": 8000
    },
    {
      "epoch": 0.07739233360648275,
      "grad_norm": 2.2942845821380615,
      "learning_rate": 4.871027952289903e-05,
      "loss": 1.6238,
      "step": 8500
    },
    {
      "epoch": 0.08194482381862879,
      "grad_norm": 1.7935348749160767,
      "learning_rate": 4.863440468602993e-05,
      "loss": 1.614,
      "step": 9000
    },
    {
      "epoch": 0.08649731403077483,
      "grad_norm": 2.6648783683776855,
      "learning_rate": 4.855852984916083e-05,
      "loss": 1.5704,
      "step": 9500
    },
    {
      "epoch": 0.09104980424292088,
      "grad_norm": 1.6302645206451416,
      "learning_rate": 4.848265501229173e-05,
      "loss": 1.5435,
      "step": 10000
    },
    {
      "epoch": 0.09560229445506692,
      "grad_norm": 1.8239576816558838,
      "learning_rate": 4.840678017542262e-05,
      "loss": 1.5374,
      "step": 10500
    },
    {
      "epoch": 0.10015478466721296,
      "grad_norm": 2.4171929359436035,
      "learning_rate": 4.833090533855352e-05,
      "loss": 1.5453,
      "step": 11000
    },
    {
      "epoch": 0.104707274879359,
      "grad_norm": 1.8650712966918945,
      "learning_rate": 4.825503050168442e-05,
      "loss": 1.5283,
      "step": 11500
    },
    {
      "epoch": 0.10925976509150505,
      "grad_norm": 2.2542166709899902,
      "learning_rate": 4.817915566481532e-05,
      "loss": 1.5883,
      "step": 12000
    },
    {
      "epoch": 0.11381225530365109,
      "grad_norm": 1.9423879384994507,
      "learning_rate": 4.810328082794622e-05,
      "loss": 1.5237,
      "step": 12500
    },
    {
      "epoch": 0.11836474551579713,
      "grad_norm": 4.801772117614746,
      "learning_rate": 4.802740599107712e-05,
      "loss": 1.5464,
      "step": 13000
    },
    {
      "epoch": 0.12291723572794319,
      "grad_norm": 1.9088784456253052,
      "learning_rate": 4.795153115420802e-05,
      "loss": 1.4789,
      "step": 13500
    },
    {
      "epoch": 0.12746972594008923,
      "grad_norm": 1.828966736793518,
      "learning_rate": 4.787565631733892e-05,
      "loss": 1.5253,
      "step": 14000
    },
    {
      "epoch": 0.13202221615223528,
      "grad_norm": 1.498440146446228,
      "learning_rate": 4.779978148046982e-05,
      "loss": 1.5235,
      "step": 14500
    },
    {
      "epoch": 0.13657470636438132,
      "grad_norm": 1.2931429147720337,
      "learning_rate": 4.772390664360072e-05,
      "loss": 1.5408,
      "step": 15000
    },
    {
      "epoch": 0.14112719657652736,
      "grad_norm": 1.5772604942321777,
      "learning_rate": 4.764803180673162e-05,
      "loss": 1.5701,
      "step": 15500
    },
    {
      "epoch": 0.1456796867886734,
      "grad_norm": 1.7862439155578613,
      "learning_rate": 4.757215696986252e-05,
      "loss": 1.5597,
      "step": 16000
    },
    {
      "epoch": 0.15023217700081945,
      "grad_norm": 1.632371187210083,
      "learning_rate": 4.7496282132993414e-05,
      "loss": 1.4911,
      "step": 16500
    },
    {
      "epoch": 0.1547846672129655,
      "grad_norm": 2.9482781887054443,
      "learning_rate": 4.7420407296124314e-05,
      "loss": 1.5559,
      "step": 17000
    },
    {
      "epoch": 0.15933715742511154,
      "grad_norm": 1.5640898942947388,
      "learning_rate": 4.7344532459255214e-05,
      "loss": 1.4804,
      "step": 17500
    },
    {
      "epoch": 0.16388964763725758,
      "grad_norm": 2.2412517070770264,
      "learning_rate": 4.7268657622386114e-05,
      "loss": 1.5526,
      "step": 18000
    },
    {
      "epoch": 0.16844213784940362,
      "grad_norm": 2.271923065185547,
      "learning_rate": 4.7192782785517014e-05,
      "loss": 1.5594,
      "step": 18500
    },
    {
      "epoch": 0.17299462806154967,
      "grad_norm": 1.2921818494796753,
      "learning_rate": 4.7116907948647914e-05,
      "loss": 1.5205,
      "step": 19000
    },
    {
      "epoch": 0.1775471182736957,
      "grad_norm": 1.9481315612792969,
      "learning_rate": 4.704103311177881e-05,
      "loss": 1.5361,
      "step": 19500
    },
    {
      "epoch": 0.18209960848584175,
      "grad_norm": 1.6352636814117432,
      "learning_rate": 4.696515827490971e-05,
      "loss": 1.5127,
      "step": 20000
    },
    {
      "epoch": 0.1866520986979878,
      "grad_norm": 1.2566120624542236,
      "learning_rate": 4.688928343804061e-05,
      "loss": 1.4809,
      "step": 20500
    },
    {
      "epoch": 0.19120458891013384,
      "grad_norm": 2.1223177909851074,
      "learning_rate": 4.681340860117151e-05,
      "loss": 1.5116,
      "step": 21000
    },
    {
      "epoch": 0.19575707912227988,
      "grad_norm": 1.8257746696472168,
      "learning_rate": 4.673753376430241e-05,
      "loss": 1.521,
      "step": 21500
    },
    {
      "epoch": 0.20030956933442592,
      "grad_norm": 1.5269874334335327,
      "learning_rate": 4.666165892743331e-05,
      "loss": 1.5207,
      "step": 22000
    },
    {
      "epoch": 0.20486205954657197,
      "grad_norm": 2.1110775470733643,
      "learning_rate": 4.658578409056421e-05,
      "loss": 1.5362,
      "step": 22500
    },
    {
      "epoch": 0.209414549758718,
      "grad_norm": 1.9805322885513306,
      "learning_rate": 4.650990925369511e-05,
      "loss": 1.5154,
      "step": 23000
    },
    {
      "epoch": 0.21396703997086405,
      "grad_norm": 2.283064126968384,
      "learning_rate": 4.643403441682601e-05,
      "loss": 1.4828,
      "step": 23500
    },
    {
      "epoch": 0.2185195301830101,
      "grad_norm": 1.9240810871124268,
      "learning_rate": 4.635815957995691e-05,
      "loss": 1.5956,
      "step": 24000
    },
    {
      "epoch": 0.22307202039515614,
      "grad_norm": 2.7188901901245117,
      "learning_rate": 4.628228474308781e-05,
      "loss": 1.4907,
      "step": 24500
    },
    {
      "epoch": 0.22762451060730218,
      "grad_norm": 1.6730681657791138,
      "learning_rate": 4.620640990621871e-05,
      "loss": 1.5159,
      "step": 25000
    },
    {
      "epoch": 0.23217700081944823,
      "grad_norm": 1.0829359292984009,
      "learning_rate": 4.61305350693496e-05,
      "loss": 1.5416,
      "step": 25500
    },
    {
      "epoch": 0.23672949103159427,
      "grad_norm": 1.9690616130828857,
      "learning_rate": 4.60546602324805e-05,
      "loss": 1.4973,
      "step": 26000
    },
    {
      "epoch": 0.2412819812437403,
      "grad_norm": 1.896836757659912,
      "learning_rate": 4.59787853956114e-05,
      "loss": 1.4919,
      "step": 26500
    },
    {
      "epoch": 0.24583447145588638,
      "grad_norm": 1.737241506576538,
      "learning_rate": 4.59029105587423e-05,
      "loss": 1.5036,
      "step": 27000
    },
    {
      "epoch": 0.2503869616680324,
      "grad_norm": 1.638809084892273,
      "learning_rate": 4.58270357218732e-05,
      "loss": 1.49,
      "step": 27500
    },
    {
      "epoch": 0.25493945188017847,
      "grad_norm": 2.3964622020721436,
      "learning_rate": 4.57511608850041e-05,
      "loss": 1.5257,
      "step": 28000
    },
    {
      "epoch": 0.2594919420923245,
      "grad_norm": 1.6429383754730225,
      "learning_rate": 4.567528604813499e-05,
      "loss": 1.5303,
      "step": 28500
    },
    {
      "epoch": 0.26404443230447056,
      "grad_norm": 1.478065848350525,
      "learning_rate": 4.55994112112659e-05,
      "loss": 1.53,
      "step": 29000
    },
    {
      "epoch": 0.2685969225166166,
      "grad_norm": 2.250810384750366,
      "learning_rate": 4.55235363743968e-05,
      "loss": 1.4999,
      "step": 29500
    },
    {
      "epoch": 0.27314941272876264,
      "grad_norm": 2.3155601024627686,
      "learning_rate": 4.54476615375277e-05,
      "loss": 1.5045,
      "step": 30000
    },
    {
      "epoch": 0.2777019029409087,
      "grad_norm": 1.9406174421310425,
      "learning_rate": 4.53717867006586e-05,
      "loss": 1.4882,
      "step": 30500
    },
    {
      "epoch": 0.28225439315305473,
      "grad_norm": 2.0826804637908936,
      "learning_rate": 4.52959118637895e-05,
      "loss": 1.5268,
      "step": 31000
    },
    {
      "epoch": 0.28680688336520077,
      "grad_norm": 1.9962064027786255,
      "learning_rate": 4.522003702692039e-05,
      "loss": 1.493,
      "step": 31500
    },
    {
      "epoch": 0.2913593735773468,
      "grad_norm": 1.8223729133605957,
      "learning_rate": 4.514416219005129e-05,
      "loss": 1.5031,
      "step": 32000
    },
    {
      "epoch": 0.29591186378949286,
      "grad_norm": 1.6665691137313843,
      "learning_rate": 4.506828735318219e-05,
      "loss": 1.4847,
      "step": 32500
    },
    {
      "epoch": 0.3004643540016389,
      "grad_norm": 1.9174461364746094,
      "learning_rate": 4.499241251631309e-05,
      "loss": 1.5001,
      "step": 33000
    },
    {
      "epoch": 0.30501684421378494,
      "grad_norm": 2.1745951175689697,
      "learning_rate": 4.491653767944399e-05,
      "loss": 1.4959,
      "step": 33500
    },
    {
      "epoch": 0.309569334425931,
      "grad_norm": 1.8195202350616455,
      "learning_rate": 4.484066284257489e-05,
      "loss": 1.4538,
      "step": 34000
    },
    {
      "epoch": 0.31412182463807703,
      "grad_norm": 3.1092007160186768,
      "learning_rate": 4.4764788005705786e-05,
      "loss": 1.4918,
      "step": 34500
    },
    {
      "epoch": 0.3186743148502231,
      "grad_norm": 1.930917739868164,
      "learning_rate": 4.4688913168836686e-05,
      "loss": 1.4934,
      "step": 35000
    },
    {
      "epoch": 0.3232268050623691,
      "grad_norm": 2.1174633502960205,
      "learning_rate": 4.4613038331967586e-05,
      "loss": 1.4888,
      "step": 35500
    },
    {
      "epoch": 0.32777929527451516,
      "grad_norm": 1.5831621885299683,
      "learning_rate": 4.4537163495098486e-05,
      "loss": 1.4966,
      "step": 36000
    },
    {
      "epoch": 0.3323317854866612,
      "grad_norm": 1.9858957529067993,
      "learning_rate": 4.4461288658229386e-05,
      "loss": 1.4325,
      "step": 36500
    },
    {
      "epoch": 0.33688427569880725,
      "grad_norm": 2.271111249923706,
      "learning_rate": 4.4385413821360286e-05,
      "loss": 1.4972,
      "step": 37000
    },
    {
      "epoch": 0.3414367659109533,
      "grad_norm": 1.579291820526123,
      "learning_rate": 4.4309538984491186e-05,
      "loss": 1.4988,
      "step": 37500
    },
    {
      "epoch": 0.34598925612309933,
      "grad_norm": 1.7411953210830688,
      "learning_rate": 4.4233664147622086e-05,
      "loss": 1.4576,
      "step": 38000
    },
    {
      "epoch": 0.3505417463352454,
      "grad_norm": 1.0848557949066162,
      "learning_rate": 4.4157789310752986e-05,
      "loss": 1.5001,
      "step": 38500
    },
    {
      "epoch": 0.3550942365473914,
      "grad_norm": 1.7193750143051147,
      "learning_rate": 4.4081914473883886e-05,
      "loss": 1.5135,
      "step": 39000
    },
    {
      "epoch": 0.35964672675953746,
      "grad_norm": 2.076620578765869,
      "learning_rate": 4.4006039637014786e-05,
      "loss": 1.4608,
      "step": 39500
    },
    {
      "epoch": 0.3641992169716835,
      "grad_norm": 1.7793735265731812,
      "learning_rate": 4.3930164800145686e-05,
      "loss": 1.4793,
      "step": 40000
    },
    {
      "epoch": 0.36875170718382955,
      "grad_norm": 1.4366289377212524,
      "learning_rate": 4.385428996327658e-05,
      "loss": 1.4905,
      "step": 40500
    },
    {
      "epoch": 0.3733041973959756,
      "grad_norm": 1.3538599014282227,
      "learning_rate": 4.377841512640748e-05,
      "loss": 1.4519,
      "step": 41000
    },
    {
      "epoch": 0.37785668760812163,
      "grad_norm": 2.698352575302124,
      "learning_rate": 4.370254028953838e-05,
      "loss": 1.5012,
      "step": 41500
    },
    {
      "epoch": 0.3824091778202677,
      "grad_norm": 1.4739645719528198,
      "learning_rate": 4.362666545266928e-05,
      "loss": 1.4796,
      "step": 42000
    },
    {
      "epoch": 0.3869616680324137,
      "grad_norm": 1.7406970262527466,
      "learning_rate": 4.355079061580018e-05,
      "loss": 1.4594,
      "step": 42500
    },
    {
      "epoch": 0.39151415824455976,
      "grad_norm": 1.3127927780151367,
      "learning_rate": 4.347491577893108e-05,
      "loss": 1.4767,
      "step": 43000
    },
    {
      "epoch": 0.3960666484567058,
      "grad_norm": 1.3746943473815918,
      "learning_rate": 4.339904094206197e-05,
      "loss": 1.4544,
      "step": 43500
    },
    {
      "epoch": 0.40061913866885185,
      "grad_norm": 2.2249526977539062,
      "learning_rate": 4.332316610519287e-05,
      "loss": 1.4708,
      "step": 44000
    },
    {
      "epoch": 0.4051716288809979,
      "grad_norm": 2.327796459197998,
      "learning_rate": 4.324729126832378e-05,
      "loss": 1.4801,
      "step": 44500
    },
    {
      "epoch": 0.40972411909314393,
      "grad_norm": 1.5917024612426758,
      "learning_rate": 4.317141643145468e-05,
      "loss": 1.443,
      "step": 45000
    },
    {
      "epoch": 0.41427660930529,
      "grad_norm": 1.9180450439453125,
      "learning_rate": 4.309554159458558e-05,
      "loss": 1.4992,
      "step": 45500
    },
    {
      "epoch": 0.418829099517436,
      "grad_norm": 1.876892328262329,
      "learning_rate": 4.301966675771647e-05,
      "loss": 1.4763,
      "step": 46000
    },
    {
      "epoch": 0.42338158972958206,
      "grad_norm": 2.57507586479187,
      "learning_rate": 4.294379192084737e-05,
      "loss": 1.4715,
      "step": 46500
    },
    {
      "epoch": 0.4279340799417281,
      "grad_norm": 1.6160082817077637,
      "learning_rate": 4.286791708397827e-05,
      "loss": 1.4571,
      "step": 47000
    },
    {
      "epoch": 0.43248657015387415,
      "grad_norm": 2.3625729084014893,
      "learning_rate": 4.279204224710917e-05,
      "loss": 1.472,
      "step": 47500
    },
    {
      "epoch": 0.4370390603660202,
      "grad_norm": 1.4745906591415405,
      "learning_rate": 4.271616741024007e-05,
      "loss": 1.4787,
      "step": 48000
    },
    {
      "epoch": 0.44159155057816624,
      "grad_norm": 1.8253474235534668,
      "learning_rate": 4.264029257337097e-05,
      "loss": 1.4408,
      "step": 48500
    },
    {
      "epoch": 0.4461440407903123,
      "grad_norm": 1.981594204902649,
      "learning_rate": 4.2564417736501865e-05,
      "loss": 1.4827,
      "step": 49000
    },
    {
      "epoch": 0.4506965310024583,
      "grad_norm": 2.4945731163024902,
      "learning_rate": 4.2488542899632765e-05,
      "loss": 1.4422,
      "step": 49500
    },
    {
      "epoch": 0.45524902121460437,
      "grad_norm": 1.399931788444519,
      "learning_rate": 4.2412668062763665e-05,
      "loss": 1.4517,
      "step": 50000
    },
    {
      "epoch": 0.4598015114267504,
      "grad_norm": 1.7381045818328857,
      "learning_rate": 4.2336793225894565e-05,
      "loss": 1.4792,
      "step": 50500
    },
    {
      "epoch": 0.46435400163889645,
      "grad_norm": 1.992401123046875,
      "learning_rate": 4.2260918389025465e-05,
      "loss": 1.4845,
      "step": 51000
    },
    {
      "epoch": 0.4689064918510425,
      "grad_norm": 2.4219253063201904,
      "learning_rate": 4.2185043552156365e-05,
      "loss": 1.4687,
      "step": 51500
    },
    {
      "epoch": 0.47345898206318854,
      "grad_norm": 1.3863270282745361,
      "learning_rate": 4.2109168715287265e-05,
      "loss": 1.4278,
      "step": 52000
    },
    {
      "epoch": 0.4780114722753346,
      "grad_norm": 2.184089422225952,
      "learning_rate": 4.2033293878418164e-05,
      "loss": 1.4607,
      "step": 52500
    },
    {
      "epoch": 0.4825639624874806,
      "grad_norm": 1.573323369026184,
      "learning_rate": 4.1957419041549064e-05,
      "loss": 1.4307,
      "step": 53000
    },
    {
      "epoch": 0.4871164526996267,
      "grad_norm": 1.3278053998947144,
      "learning_rate": 4.1881544204679964e-05,
      "loss": 1.4801,
      "step": 53500
    },
    {
      "epoch": 0.49166894291177277,
      "grad_norm": 1.918517827987671,
      "learning_rate": 4.1805669367810864e-05,
      "loss": 1.4652,
      "step": 54000
    },
    {
      "epoch": 0.4962214331239188,
      "grad_norm": 1.6713398694992065,
      "learning_rate": 4.1729794530941764e-05,
      "loss": 1.457,
      "step": 54500
    },
    {
      "epoch": 0.5007739233360649,
      "grad_norm": 1.9636454582214355,
      "learning_rate": 4.165391969407266e-05,
      "loss": 1.4802,
      "step": 55000
    },
    {
      "epoch": 0.5053264135482108,
      "grad_norm": 2.2944159507751465,
      "learning_rate": 4.157804485720356e-05,
      "loss": 1.4471,
      "step": 55500
    },
    {
      "epoch": 0.5098789037603569,
      "grad_norm": 1.6703966856002808,
      "learning_rate": 4.150217002033446e-05,
      "loss": 1.4511,
      "step": 56000
    },
    {
      "epoch": 0.5144313939725029,
      "grad_norm": 1.5112354755401611,
      "learning_rate": 4.142629518346536e-05,
      "loss": 1.4584,
      "step": 56500
    },
    {
      "epoch": 0.518983884184649,
      "grad_norm": 3.5930309295654297,
      "learning_rate": 4.135042034659626e-05,
      "loss": 1.4827,
      "step": 57000
    },
    {
      "epoch": 0.523536374396795,
      "grad_norm": 1.7980707883834839,
      "learning_rate": 4.127454550972716e-05,
      "loss": 1.4357,
      "step": 57500
    },
    {
      "epoch": 0.5280888646089411,
      "grad_norm": 1.4820793867111206,
      "learning_rate": 4.119867067285805e-05,
      "loss": 1.4636,
      "step": 58000
    },
    {
      "epoch": 0.5326413548210871,
      "grad_norm": 1.6004084348678589,
      "learning_rate": 4.112279583598895e-05,
      "loss": 1.4616,
      "step": 58500
    },
    {
      "epoch": 0.5371938450332332,
      "grad_norm": 1.527901530265808,
      "learning_rate": 4.104692099911985e-05,
      "loss": 1.4212,
      "step": 59000
    },
    {
      "epoch": 0.5417463352453792,
      "grad_norm": 2.45041561126709,
      "learning_rate": 4.097104616225075e-05,
      "loss": 1.4282,
      "step": 59500
    },
    {
      "epoch": 0.5462988254575253,
      "grad_norm": 1.7481502294540405,
      "learning_rate": 4.089517132538165e-05,
      "loss": 1.4231,
      "step": 60000
    },
    {
      "epoch": 0.5508513156696713,
      "grad_norm": 1.3719326257705688,
      "learning_rate": 4.081929648851256e-05,
      "loss": 1.4335,
      "step": 60500
    },
    {
      "epoch": 0.5554038058818174,
      "grad_norm": 1.3403795957565308,
      "learning_rate": 4.074342165164345e-05,
      "loss": 1.4605,
      "step": 61000
    },
    {
      "epoch": 0.5599562960939634,
      "grad_norm": 1.7062550783157349,
      "learning_rate": 4.066754681477435e-05,
      "loss": 1.4654,
      "step": 61500
    },
    {
      "epoch": 0.5645087863061095,
      "grad_norm": 3.0322699546813965,
      "learning_rate": 4.059167197790525e-05,
      "loss": 1.4563,
      "step": 62000
    },
    {
      "epoch": 0.5690612765182554,
      "grad_norm": 1.335919976234436,
      "learning_rate": 4.051579714103615e-05,
      "loss": 1.4979,
      "step": 62500
    },
    {
      "epoch": 0.5736137667304015,
      "grad_norm": 1.6601481437683105,
      "learning_rate": 4.043992230416705e-05,
      "loss": 1.4213,
      "step": 63000
    },
    {
      "epoch": 0.5781662569425475,
      "grad_norm": 1.551808476448059,
      "learning_rate": 4.036404746729795e-05,
      "loss": 1.4586,
      "step": 63500
    },
    {
      "epoch": 0.5827187471546936,
      "grad_norm": 0.9790830016136169,
      "learning_rate": 4.0288172630428843e-05,
      "loss": 1.4297,
      "step": 64000
    },
    {
      "epoch": 0.5872712373668396,
      "grad_norm": 1.507020115852356,
      "learning_rate": 4.0212297793559743e-05,
      "loss": 1.408,
      "step": 64500
    },
    {
      "epoch": 0.5918237275789857,
      "grad_norm": 0.5328876972198486,
      "learning_rate": 4.013642295669064e-05,
      "loss": 1.3959,
      "step": 65000
    },
    {
      "epoch": 0.5963762177911317,
      "grad_norm": 1.2166988849639893,
      "learning_rate": 4.006054811982154e-05,
      "loss": 1.4699,
      "step": 65500
    },
    {
      "epoch": 0.6009287080032778,
      "grad_norm": 2.0083260536193848,
      "learning_rate": 3.998467328295244e-05,
      "loss": 1.4231,
      "step": 66000
    },
    {
      "epoch": 0.6054811982154238,
      "grad_norm": 1.8845000267028809,
      "learning_rate": 3.990879844608334e-05,
      "loss": 1.4701,
      "step": 66500
    },
    {
      "epoch": 0.6100336884275699,
      "grad_norm": 1.0522596836090088,
      "learning_rate": 3.983292360921424e-05,
      "loss": 1.4649,
      "step": 67000
    },
    {
      "epoch": 0.6145861786397159,
      "grad_norm": 1.7644633054733276,
      "learning_rate": 3.975704877234514e-05,
      "loss": 1.4227,
      "step": 67500
    },
    {
      "epoch": 0.619138668851862,
      "grad_norm": 1.6478123664855957,
      "learning_rate": 3.968117393547604e-05,
      "loss": 1.4473,
      "step": 68000
    },
    {
      "epoch": 0.623691159064008,
      "grad_norm": 1.859995722770691,
      "learning_rate": 3.960529909860694e-05,
      "loss": 1.4432,
      "step": 68500
    },
    {
      "epoch": 0.6282436492761541,
      "grad_norm": 2.6457579135894775,
      "learning_rate": 3.952942426173784e-05,
      "loss": 1.4415,
      "step": 69000
    },
    {
      "epoch": 0.6327961394883,
      "grad_norm": 1.523418664932251,
      "learning_rate": 3.945354942486874e-05,
      "loss": 1.444,
      "step": 69500
    },
    {
      "epoch": 0.6373486297004461,
      "grad_norm": 1.3639545440673828,
      "learning_rate": 3.9377674587999636e-05,
      "loss": 1.461,
      "step": 70000
    },
    {
      "epoch": 0.6419011199125921,
      "grad_norm": 1.5062317848205566,
      "learning_rate": 3.9301799751130536e-05,
      "loss": 1.4501,
      "step": 70500
    },
    {
      "epoch": 0.6464536101247382,
      "grad_norm": 1.4950380325317383,
      "learning_rate": 3.9225924914261436e-05,
      "loss": 1.4345,
      "step": 71000
    },
    {
      "epoch": 0.6510061003368843,
      "grad_norm": 1.5224130153656006,
      "learning_rate": 3.9150050077392336e-05,
      "loss": 1.4422,
      "step": 71500
    },
    {
      "epoch": 0.6555585905490303,
      "grad_norm": 1.925525426864624,
      "learning_rate": 3.9074175240523236e-05,
      "loss": 1.427,
      "step": 72000
    },
    {
      "epoch": 0.6601110807611764,
      "grad_norm": 2.675509214401245,
      "learning_rate": 3.8998300403654136e-05,
      "loss": 1.4316,
      "step": 72500
    },
    {
      "epoch": 0.6646635709733224,
      "grad_norm": 1.532267451286316,
      "learning_rate": 3.892242556678503e-05,
      "loss": 1.4292,
      "step": 73000
    },
    {
      "epoch": 0.6692160611854685,
      "grad_norm": 7.720393180847168,
      "learning_rate": 3.884655072991593e-05,
      "loss": 1.4589,
      "step": 73500
    },
    {
      "epoch": 0.6737685513976145,
      "grad_norm": 1.5860896110534668,
      "learning_rate": 3.877067589304683e-05,
      "loss": 1.4481,
      "step": 74000
    },
    {
      "epoch": 0.6783210416097606,
      "grad_norm": 1.8452571630477905,
      "learning_rate": 3.869480105617773e-05,
      "loss": 1.4009,
      "step": 74500
    },
    {
      "epoch": 0.6828735318219066,
      "grad_norm": 1.3081570863723755,
      "learning_rate": 3.861892621930863e-05,
      "loss": 1.4423,
      "step": 75000
    },
    {
      "epoch": 0.6874260220340527,
      "grad_norm": 1.3529257774353027,
      "learning_rate": 3.854305138243953e-05,
      "loss": 1.4351,
      "step": 75500
    },
    {
      "epoch": 0.6919785122461987,
      "grad_norm": 2.912944793701172,
      "learning_rate": 3.846717654557043e-05,
      "loss": 1.4157,
      "step": 76000
    },
    {
      "epoch": 0.6965310024583448,
      "grad_norm": 1.7121022939682007,
      "learning_rate": 3.839130170870133e-05,
      "loss": 1.4668,
      "step": 76500
    },
    {
      "epoch": 0.7010834926704907,
      "grad_norm": 1.5933057069778442,
      "learning_rate": 3.831542687183223e-05,
      "loss": 1.4394,
      "step": 77000
    },
    {
      "epoch": 0.7056359828826368,
      "grad_norm": 2.3423147201538086,
      "learning_rate": 3.823955203496313e-05,
      "loss": 1.419,
      "step": 77500
    },
    {
      "epoch": 0.7101884730947828,
      "grad_norm": 2.3738741874694824,
      "learning_rate": 3.816367719809403e-05,
      "loss": 1.432,
      "step": 78000
    },
    {
      "epoch": 0.7147409633069289,
      "grad_norm": 2.617544412612915,
      "learning_rate": 3.808780236122493e-05,
      "loss": 1.4025,
      "step": 78500
    },
    {
      "epoch": 0.7192934535190749,
      "grad_norm": 1.015317678451538,
      "learning_rate": 3.801192752435582e-05,
      "loss": 1.3974,
      "step": 79000
    },
    {
      "epoch": 0.723845943731221,
      "grad_norm": 1.7523547410964966,
      "learning_rate": 3.793605268748672e-05,
      "loss": 1.4314,
      "step": 79500
    },
    {
      "epoch": 0.728398433943367,
      "grad_norm": 1.621612787246704,
      "learning_rate": 3.786017785061762e-05,
      "loss": 1.4117,
      "step": 80000
    },
    {
      "epoch": 0.7329509241555131,
      "grad_norm": 1.675252079963684,
      "learning_rate": 3.778430301374852e-05,
      "loss": 1.4071,
      "step": 80500
    },
    {
      "epoch": 0.7375034143676591,
      "grad_norm": 2.218848705291748,
      "learning_rate": 3.770842817687942e-05,
      "loss": 1.3938,
      "step": 81000
    },
    {
      "epoch": 0.7420559045798052,
      "grad_norm": 1.0135998725891113,
      "learning_rate": 3.7632553340010315e-05,
      "loss": 1.4161,
      "step": 81500
    },
    {
      "epoch": 0.7466083947919512,
      "grad_norm": 2.363743305206299,
      "learning_rate": 3.7556678503141215e-05,
      "loss": 1.4065,
      "step": 82000
    },
    {
      "epoch": 0.7511608850040973,
      "grad_norm": 1.4600533246994019,
      "learning_rate": 3.7480803666272115e-05,
      "loss": 1.4127,
      "step": 82500
    },
    {
      "epoch": 0.7557133752162433,
      "grad_norm": 1.851562261581421,
      "learning_rate": 3.740492882940302e-05,
      "loss": 1.4511,
      "step": 83000
    },
    {
      "epoch": 0.7602658654283894,
      "grad_norm": 1.828016757965088,
      "learning_rate": 3.732905399253392e-05,
      "loss": 1.4015,
      "step": 83500
    },
    {
      "epoch": 0.7648183556405354,
      "grad_norm": 1.836269497871399,
      "learning_rate": 3.725317915566482e-05,
      "loss": 1.4201,
      "step": 84000
    },
    {
      "epoch": 0.7693708458526815,
      "grad_norm": 1.6285814046859741,
      "learning_rate": 3.7177304318795715e-05,
      "loss": 1.4117,
      "step": 84500
    },
    {
      "epoch": 0.7739233360648274,
      "grad_norm": 1.1409574747085571,
      "learning_rate": 3.7101429481926615e-05,
      "loss": 1.3832,
      "step": 85000
    },
    {
      "epoch": 0.7784758262769735,
      "grad_norm": 1.3484891653060913,
      "learning_rate": 3.7025554645057515e-05,
      "loss": 1.4233,
      "step": 85500
    },
    {
      "epoch": 0.7830283164891195,
      "grad_norm": 2.121706485748291,
      "learning_rate": 3.6949679808188415e-05,
      "loss": 1.402,
      "step": 86000
    },
    {
      "epoch": 0.7875808067012656,
      "grad_norm": 2.5278890132904053,
      "learning_rate": 3.6873804971319315e-05,
      "loss": 1.4242,
      "step": 86500
    },
    {
      "epoch": 0.7921332969134116,
      "grad_norm": 1.5969198942184448,
      "learning_rate": 3.6797930134450215e-05,
      "loss": 1.4048,
      "step": 87000
    },
    {
      "epoch": 0.7966857871255577,
      "grad_norm": 1.7309540510177612,
      "learning_rate": 3.672205529758111e-05,
      "loss": 1.4184,
      "step": 87500
    },
    {
      "epoch": 0.8012382773377037,
      "grad_norm": 3.2090744972229004,
      "learning_rate": 3.664618046071201e-05,
      "loss": 1.4265,
      "step": 88000
    },
    {
      "epoch": 0.8057907675498498,
      "grad_norm": 1.9063198566436768,
      "learning_rate": 3.657030562384291e-05,
      "loss": 1.4297,
      "step": 88500
    },
    {
      "epoch": 0.8103432577619958,
      "grad_norm": 1.5400705337524414,
      "learning_rate": 3.649443078697381e-05,
      "loss": 1.4528,
      "step": 89000
    },
    {
      "epoch": 0.8148957479741419,
      "grad_norm": 1.7730275392532349,
      "learning_rate": 3.641855595010471e-05,
      "loss": 1.4142,
      "step": 89500
    },
    {
      "epoch": 0.8194482381862879,
      "grad_norm": 2.5325891971588135,
      "learning_rate": 3.634268111323561e-05,
      "loss": 1.4327,
      "step": 90000
    },
    {
      "epoch": 0.824000728398434,
      "grad_norm": 2.34662127494812,
      "learning_rate": 3.626680627636651e-05,
      "loss": 1.4232,
      "step": 90500
    },
    {
      "epoch": 0.82855321861058,
      "grad_norm": 1.2610607147216797,
      "learning_rate": 3.619093143949741e-05,
      "loss": 1.4128,
      "step": 91000
    },
    {
      "epoch": 0.833105708822726,
      "grad_norm": 1.5490508079528809,
      "learning_rate": 3.611505660262831e-05,
      "loss": 1.3912,
      "step": 91500
    },
    {
      "epoch": 0.837658199034872,
      "grad_norm": 1.4846287965774536,
      "learning_rate": 3.603918176575921e-05,
      "loss": 1.4253,
      "step": 92000
    },
    {
      "epoch": 0.8422106892470181,
      "grad_norm": 1.5123846530914307,
      "learning_rate": 3.596330692889011e-05,
      "loss": 1.4288,
      "step": 92500
    },
    {
      "epoch": 0.8467631794591641,
      "grad_norm": 2.869833469390869,
      "learning_rate": 3.588743209202101e-05,
      "loss": 1.4053,
      "step": 93000
    },
    {
      "epoch": 0.8513156696713102,
      "grad_norm": 1.3906861543655396,
      "learning_rate": 3.58115572551519e-05,
      "loss": 1.433,
      "step": 93500
    },
    {
      "epoch": 0.8558681598834562,
      "grad_norm": 1.0686962604522705,
      "learning_rate": 3.57356824182828e-05,
      "loss": 1.3779,
      "step": 94000
    },
    {
      "epoch": 0.8604206500956023,
      "grad_norm": 1.898055911064148,
      "learning_rate": 3.56598075814137e-05,
      "loss": 1.3888,
      "step": 94500
    },
    {
      "epoch": 0.8649731403077483,
      "grad_norm": 3.5353546142578125,
      "learning_rate": 3.55839327445446e-05,
      "loss": 1.3903,
      "step": 95000
    },
    {
      "epoch": 0.8695256305198944,
      "grad_norm": 1.470703125,
      "learning_rate": 3.55080579076755e-05,
      "loss": 1.3758,
      "step": 95500
    },
    {
      "epoch": 0.8740781207320404,
      "grad_norm": 1.4897403717041016,
      "learning_rate": 3.54321830708064e-05,
      "loss": 1.4264,
      "step": 96000
    },
    {
      "epoch": 0.8786306109441865,
      "grad_norm": 1.631150245666504,
      "learning_rate": 3.5356308233937294e-05,
      "loss": 1.3956,
      "step": 96500
    },
    {
      "epoch": 0.8831831011563325,
      "grad_norm": 1.4428099393844604,
      "learning_rate": 3.5280433397068194e-05,
      "loss": 1.4073,
      "step": 97000
    },
    {
      "epoch": 0.8877355913684786,
      "grad_norm": 1.879005789756775,
      "learning_rate": 3.5204558560199094e-05,
      "loss": 1.4133,
      "step": 97500
    },
    {
      "epoch": 0.8922880815806246,
      "grad_norm": 1.7584060430526733,
      "learning_rate": 3.5128683723329994e-05,
      "loss": 1.4188,
      "step": 98000
    },
    {
      "epoch": 0.8968405717927707,
      "grad_norm": 1.5144892930984497,
      "learning_rate": 3.50528088864609e-05,
      "loss": 1.4072,
      "step": 98500
    },
    {
      "epoch": 0.9013930620049166,
      "grad_norm": 1.5361531972885132,
      "learning_rate": 3.49769340495918e-05,
      "loss": 1.421,
      "step": 99000
    },
    {
      "epoch": 0.9059455522170627,
      "grad_norm": 1.3133127689361572,
      "learning_rate": 3.4901059212722694e-05,
      "loss": 1.4169,
      "step": 99500
    },
    {
      "epoch": 0.9104980424292087,
      "grad_norm": 1.8255308866500854,
      "learning_rate": 3.4825184375853594e-05,
      "loss": 1.3832,
      "step": 100000
    },
    {
      "epoch": 0.9150505326413548,
      "grad_norm": 1.765775203704834,
      "learning_rate": 3.4749309538984494e-05,
      "loss": 1.3827,
      "step": 100500
    },
    {
      "epoch": 0.9196030228535008,
      "grad_norm": 3.2547006607055664,
      "learning_rate": 3.4673434702115394e-05,
      "loss": 1.4011,
      "step": 101000
    },
    {
      "epoch": 0.9241555130656469,
      "grad_norm": 2.5694596767425537,
      "learning_rate": 3.4597559865246294e-05,
      "loss": 1.3868,
      "step": 101500
    },
    {
      "epoch": 0.9287080032777929,
      "grad_norm": 1.8394837379455566,
      "learning_rate": 3.4521685028377194e-05,
      "loss": 1.4079,
      "step": 102000
    },
    {
      "epoch": 0.933260493489939,
      "grad_norm": 1.544235348701477,
      "learning_rate": 3.444581019150809e-05,
      "loss": 1.4154,
      "step": 102500
    },
    {
      "epoch": 0.937812983702085,
      "grad_norm": 2.3242204189300537,
      "learning_rate": 3.436993535463899e-05,
      "loss": 1.4145,
      "step": 103000
    },
    {
      "epoch": 0.9423654739142311,
      "grad_norm": 1.4065098762512207,
      "learning_rate": 3.429406051776989e-05,
      "loss": 1.3852,
      "step": 103500
    },
    {
      "epoch": 0.9469179641263771,
      "grad_norm": 1.7728382349014282,
      "learning_rate": 3.421818568090079e-05,
      "loss": 1.4082,
      "step": 104000
    },
    {
      "epoch": 0.9514704543385232,
      "grad_norm": 1.1972371339797974,
      "learning_rate": 3.414231084403169e-05,
      "loss": 1.3863,
      "step": 104500
    },
    {
      "epoch": 0.9560229445506692,
      "grad_norm": 1.4880298376083374,
      "learning_rate": 3.406643600716259e-05,
      "loss": 1.3803,
      "step": 105000
    },
    {
      "epoch": 0.9605754347628153,
      "grad_norm": 1.973327875137329,
      "learning_rate": 3.3990561170293487e-05,
      "loss": 1.3844,
      "step": 105500
    },
    {
      "epoch": 0.9651279249749612,
      "grad_norm": 3.1504766941070557,
      "learning_rate": 3.3914686333424387e-05,
      "loss": 1.3887,
      "step": 106000
    },
    {
      "epoch": 0.9696804151871073,
      "grad_norm": 1.6976909637451172,
      "learning_rate": 3.3838811496555287e-05,
      "loss": 1.4344,
      "step": 106500
    },
    {
      "epoch": 0.9742329053992534,
      "grad_norm": 1.5262069702148438,
      "learning_rate": 3.3762936659686186e-05,
      "loss": 1.4188,
      "step": 107000
    },
    {
      "epoch": 0.9787853956113994,
      "grad_norm": 1.7988815307617188,
      "learning_rate": 3.3687061822817086e-05,
      "loss": 1.401,
      "step": 107500
    },
    {
      "epoch": 0.9833378858235455,
      "grad_norm": 2.4446630477905273,
      "learning_rate": 3.3611186985947986e-05,
      "loss": 1.3752,
      "step": 108000
    },
    {
      "epoch": 0.9878903760356915,
      "grad_norm": 1.919823169708252,
      "learning_rate": 3.353531214907888e-05,
      "loss": 1.3984,
      "step": 108500
    },
    {
      "epoch": 0.9924428662478376,
      "grad_norm": 1.7250622510910034,
      "learning_rate": 3.345943731220978e-05,
      "loss": 1.4187,
      "step": 109000
    },
    {
      "epoch": 0.9969953564599836,
      "grad_norm": 1.3520405292510986,
      "learning_rate": 3.338356247534068e-05,
      "loss": 1.3928,
      "step": 109500
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.3585346937179565,
      "eval_runtime": 8752.7613,
      "eval_samples_per_second": 6.274,
      "eval_steps_per_second": 0.784,
      "step": 109830
    }
  ],
  "logging_steps": 500,
  "max_steps": 329490,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.434885193728e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
